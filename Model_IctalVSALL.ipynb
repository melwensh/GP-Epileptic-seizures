{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, \n",
    "    GlobalAveragePooling1D, Input, Activation, SpatialDropout1D, \n",
    "    LSTM, Bidirectional, Multiply, Reshape, LeakyReLU\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# ===================== DATA AUGMENTATION =====================\n",
    "def augment_signal(segment, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply random augmentation to EEG segment\n",
    "    \n",
    "    Args:\n",
    "        segment: Input EEG segment (1D array)\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented segment\n",
    "    \"\"\"\n",
    "    if np.random.random() > augmentation_prob:\n",
    "        return segment  # No augmentation\n",
    "    \n",
    "    # Choose augmentation type\n",
    "    aug_type = np.random.choice(['noise', 'scale', 'shift', 'time_shift'], p=[0.3, 0.3, 0.2, 0.2])\n",
    "    \n",
    "    if aug_type == 'noise':\n",
    "        # Add Gaussian noise\n",
    "        noise_level = np.random.uniform(0.01, 0.05)\n",
    "        noise = np.random.normal(0, noise_level, segment.shape)\n",
    "        return segment + noise\n",
    "    \n",
    "    elif aug_type == 'scale':\n",
    "        # Random amplitude scaling\n",
    "        scale = np.random.uniform(0.9, 1.1)\n",
    "        return segment * scale\n",
    "    \n",
    "    elif aug_type == 'shift':\n",
    "        # Random DC shift\n",
    "        shift = np.random.uniform(-0.1, 0.1)\n",
    "        return segment + shift\n",
    "    \n",
    "    elif aug_type == 'time_shift':\n",
    "        # Random time shift (circular shift)\n",
    "        shift_amount = np.random.randint(-20, 20)\n",
    "        return np.roll(segment, shift_amount)\n",
    "    \n",
    "    return segment\n",
    "\n",
    "\n",
    "def augment_batch(X_batch, y_batch, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply augmentation to a batch of data\n",
    "    \n",
    "    Args:\n",
    "        X_batch: Batch of EEG segments (batch_size, time_steps, channels)\n",
    "        y_batch: Batch of labels (1 = ICTAL, 0 = ALL)\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented batch\n",
    "    \"\"\"\n",
    "    X_augmented = np.zeros_like(X_batch)\n",
    "    \n",
    "    for i in range(len(X_batch)):\n",
    "        # Augment ICTAL class (minority) more aggressively\n",
    "        if y_batch[i] == 1:  # ICTAL (seizure)\n",
    "            prob = augmentation_prob * 1.5  # Higher probability for minority class\n",
    "        else:  # ALL (NORMAL + INTERICTAL)\n",
    "            prob = augmentation_prob\n",
    "        \n",
    "        X_augmented[i, :, 0] = augment_signal(X_batch[i, :, 0], prob)\n",
    "    \n",
    "    return X_augmented\n",
    "\n",
    "\n",
    "# ===================== CUSTOM DATA GENERATOR =====================\n",
    "class AugmentedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator with real-time augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, augmentation_prob=0.5, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.X))\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        \n",
    "        # Get batch data\n",
    "        X_batch = self.X[batch_indices].copy()\n",
    "        y_batch = self.y[batch_indices]\n",
    "        \n",
    "        # Apply augmentation\n",
    "        X_batch = augment_batch(X_batch, y_batch, self.augmentation_prob)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "# ===================== SE BLOCK IMPLEMENTATION =====================\n",
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block for channel attention\n",
    "    Paper: \"Interpretable classification of epileptic EEG signals...\"\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction=8, **kwargs):\n",
    "        super(SEBlock, self).__init__(**kwargs)\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.squeeze = GlobalAveragePooling1D()\n",
    "        \n",
    "        # Excitation network\n",
    "        self.fc1 = Dense(\n",
    "            channels // self.reduction, \n",
    "            activation='relu', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.fc2 = Dense(\n",
    "            channels, \n",
    "            activation='sigmoid', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        \n",
    "        super(SEBlock, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Squeeze: Global average pooling\n",
    "        squeeze = self.squeeze(inputs)\n",
    "        \n",
    "        # Excitation: Learn channel importance\n",
    "        excitation = self.fc1(squeeze)\n",
    "        excitation = self.fc2(excitation)\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        excitation = tf.reshape(excitation, [-1, 1, tf.shape(inputs)[-1]])\n",
    "        \n",
    "        # Scale: Multiply input with learned weights\n",
    "        return Multiply()([inputs, excitation])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SEBlock, self).get_config()\n",
    "        config.update({\"reduction\": self.reduction})\n",
    "        return config\n",
    "\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "data_dir = os.path.join(\"preprocessed\")\n",
    "X = np.load(os.path.join(data_dir, \"ALL_X.npy\"))\n",
    "y = np.load(os.path.join(data_dir, \"ALL_y.npy\"))\n",
    "file_ids = np.load(os.path.join(data_dir, \"ALL_file_ids.npy\"))  # âœ… Load file IDs\n",
    "\n",
    "# Convert to Binary Classification: ICTAL (1) vs ALL (0)\n",
    "# ICTAL = seizure (minority class)\n",
    "# ALL = NORMAL + INTERICTAL (majority class)\n",
    "y_encoded = np.where(y == 'ICTAL', 1, 0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BINARY CLASSIFICATION SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original classes: {np.unique(y)}\")\n",
    "print(f\"Binary encoding: 1 (ICTAL) vs 0 (ALL: NORMAL+INTERICTAL)\")\n",
    "print(f\"Binary labels distribution: {dict(zip(*np.unique(y_encoded, return_counts=True)[::-1]))}\")\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Total unique files: {len(np.unique(file_ids))}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare Data\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "\n",
    "# Compute Class Weights with stronger emphasis on minority class\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "# Boost minority class (ICTAL) weight even more\n",
    "class_weights[1] = class_weights[1] * 1.5  # 50% increase for ICTAL\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "print(f\"Class weights (adjusted): {class_weight_dict}\")\n",
    "\n",
    "# ===================== PREPARE CROSS VALIDATION (FILE-LEVEL) =====================\n",
    "random_state = np.random.randint(0, 10000)\n",
    "print(f\"ðŸŽ² Random state used for this run: {random_state}\")\n",
    "\n",
    "# âœ… KEY FIX: Split by unique file IDs, not by segments\n",
    "unique_file_ids = np.unique(file_ids)\n",
    "print(f\"Total unique files for splitting: {len(unique_file_ids)}\")\n",
    "\n",
    "# Create a mapping from file_id to its label (use majority vote if needed)\n",
    "file_id_to_label = {}\n",
    "for fid in unique_file_ids:\n",
    "    mask = file_ids == fid\n",
    "    labels_in_file = y_encoded[mask]\n",
    "    # Use the most common label (should be the same for all segments from one file)\n",
    "    file_id_to_label[fid] = np.bincount(labels_in_file).argmax()\n",
    "\n",
    "file_labels = np.array([file_id_to_label[fid] for fid in unique_file_ids])\n",
    "\n",
    "# âœ… Stratified K-Fold on FILE level\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "file_fold_indices = [(train_val_files, test_files) for train_val_files, test_files \n",
    "                     in kfold.split(unique_file_ids, file_labels)]\n",
    "\n",
    "# Convert file-level indices to segment-level indices\n",
    "fold_indices = []\n",
    "for train_val_files_idx, test_files_idx in file_fold_indices:\n",
    "    train_val_files = unique_file_ids[train_val_files_idx]\n",
    "    test_files = unique_file_ids[test_files_idx]\n",
    "    \n",
    "    # Get all segments from these files\n",
    "    train_val_mask = np.isin(file_ids, train_val_files)\n",
    "    test_mask = np.isin(file_ids, test_files)\n",
    "    \n",
    "    train_val_idx = np.where(train_val_mask)[0]\n",
    "    test_idx = np.where(test_mask)[0]\n",
    "    \n",
    "    fold_indices.append((train_val_idx, test_idx))\n",
    "    \n",
    "    print(f\"Fold: Train/Val files: {len(train_val_files)}, Test files: {len(test_files)}\")\n",
    "    print(f\"      Train/Val segments: {len(train_val_idx)}, Test segments: {len(test_idx)}\")\n",
    "\n",
    "# Save indices for reproducibility\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "np.save(os.path.join(\"results\", \"fold_indices.npy\"), np.array(fold_indices, dtype=object), allow_pickle=True)\n",
    "np.save(os.path.join(\"results\", \"file_fold_indices.npy\"), np.array(file_fold_indices, dtype=object), allow_pickle=True)\n",
    "\n",
    "\n",
    "# ===================== LOSS FUNCTIONS =====================\n",
    "def focal_loss(alpha=0.75, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        \n",
    "        return K.mean(alpha_t * focal_term * cross_entropy)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def hybrid_focal_loss(alpha=0.75, gamma=1.7, focal_weight=0.55):\n",
    "    \"\"\"Hybrid: Focal + BCE\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        bce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        \n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal = alpha_t * focal_term * bce\n",
    "        \n",
    "        combined = focal_weight * focal + (1 - focal_weight) * bce\n",
    "        return K.mean(combined)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# ===================== MODEL BUILDING =====================\n",
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    MODIFIED MODEL:\n",
    "    - Added 4th CNN block\n",
    "    - Changed Bidirectional LSTM to regular LSTM\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # ========== Block 1: Local patterns ==========\n",
    "    x = Conv1D(48, kernel_size=7, padding='same', kernel_regularizer=l2(0.002))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.28)(x)\n",
    "    \n",
    "    # ========== Block 2: Mid-level features ==========\n",
    "    x = Conv1D(96, kernel_size=5, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.32)(x)\n",
    "    \n",
    "    # ========== Block 3: High-level features ==========\n",
    "    x = Conv1D(128, kernel_size=3, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.38)(x)\n",
    "    \n",
    "    # ========== Block 4: Deep features (NEW LAYER) ==========\n",
    "    x = Conv1D(160, kernel_size=3, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.38)(x)\n",
    "    \n",
    "    # ========== LSTM Temporal Modeling (Changed from Bidirectional to regular LSTM) ==========\n",
    "    x = LSTM(64, return_sequences=False, kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # ========== Dense Classification ==========\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.48)(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.48)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ===================== TRAINING =====================\n",
    "USE_DATA_AUGMENTATION = True  \n",
    "\n",
    "acc_per_fold = []\n",
    "auc_per_fold = []\n",
    "conf_matrices = []\n",
    "class_names = ['ALL (NORMAL+INTERICTAL)', 'ICTAL (SEIZURE)']\n",
    "\n",
    "# Track metrics for best model selection\n",
    "fold_metrics = {\n",
    "    'fold_no': [],\n",
    "    'test_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_auc': [],\n",
    "    'val_acc': [],\n",
    "    'train_acc': [],\n",
    "    'f1_score': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "best_model = None\n",
    "best_fold = None\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "for fold_no, (train_val_idx, test_idx) in enumerate(fold_indices, start=1):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" FOLD {fold_no}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Split into train/val/test\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y_encoded[train_val_idx], y_encoded[test_idx]\n",
    "    file_ids_train_val = file_ids[train_val_idx]\n",
    "    \n",
    "    # âœ… KEY FIX: Split train/val by files, not segments\n",
    "    unique_train_val_files = np.unique(file_ids_train_val)\n",
    "    train_val_file_labels = np.array([file_id_to_label[fid] for fid in unique_train_val_files])\n",
    "    \n",
    "    train_files, val_files = train_test_split(\n",
    "        unique_train_val_files, \n",
    "        test_size=0.15, \n",
    "        stratify=train_val_file_labels, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_mask = np.isin(file_ids_train_val, train_files)\n",
    "    val_mask = np.isin(file_ids_train_val, val_files)\n",
    "    \n",
    "    X_train = X_train_val[train_mask]\n",
    "    y_train = y_train_val[train_mask]\n",
    "    X_val = X_train_val[val_mask]\n",
    "    y_val = y_train_val[val_mask]\n",
    "\n",
    "    print(f\"Train files: {len(train_files)}, Val files: {len(val_files)}, Test files: {len(np.unique(file_ids[test_idx]))}\")\n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    print(f\"Test set distribution: {dict(zip(*np.unique(y_test, return_counts=True)[::-1]))}\")\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(input_shape=(X.shape[1], 1))\n",
    "    \n",
    "    # Print model summary only for first fold\n",
    "    if fold_no == 1:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL ARCHITECTURE\")\n",
    "        print(\"=\"*60)\n",
    "        model.summary()\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Compile with adjusted hyperparameters\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),  # Increased learning rate\n",
    "        loss=hybrid_focal_loss(alpha=0.80, gamma=2.0, focal_weight=0.7),  # More aggressive focal loss\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=25, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1, \n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,  # More aggressive reduction\n",
    "        patience=5,  # Reduce faster\n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        os.path.join(\"results\", f\"model_fold{fold_no}.weights.h5\"),\n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True,\n",
    "        verbose=1,  # Changed to 1 for better monitoring\n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    callbacks = [early_stop, reduce_lr, checkpoint]\n",
    "    \n",
    "    # Train Model\n",
    "    print(f\"\\nðŸš€ Training Fold {fold_no}...\")\n",
    "    \n",
    "    if USE_DATA_AUGMENTATION:\n",
    "        print(\"ðŸ“Š Using data augmentation with real-time generator\")\n",
    "        \n",
    "        # Create data generators\n",
    "        train_generator = AugmentedDataGenerator(\n",
    "            X_train, y_train, \n",
    "            batch_size=12, \n",
    "            augmentation_prob=0.55,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Note: Validation data is NOT augmented\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=200,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        print(\"ðŸ“Š Training without data augmentation\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=150,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_weights(os.path.join(\"results\", f\"model_fold{fold_no}.weights.h5\"))\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # AUC Score\n",
    "    test_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    auc_per_fold.append(test_auc)\n",
    "    \n",
    "    print(f\"\\nâœ… Fold {fold_no} Results:\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "    \n",
    "    # Classification Metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store metrics\n",
    "    fold_metrics['fold_no'].append(fold_no)\n",
    "    fold_metrics['test_acc'].append(test_acc)\n",
    "    fold_metrics['test_loss'].append(test_loss)\n",
    "    fold_metrics['test_auc'].append(test_auc)\n",
    "    fold_metrics['val_acc'].append(max(history.history['val_accuracy']))\n",
    "    fold_metrics['train_acc'].append(max(history.history['accuracy']))\n",
    "    fold_metrics['f1_score'].append(f1)\n",
    "    fold_metrics['precision'].append(precision)\n",
    "    fold_metrics['recall'].append(recall)\n",
    "    \n",
    "    # Check if this is the best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_fold = fold_no\n",
    "        best_model = model\n",
    "        best_cm = cm\n",
    "        best_y_test = y_test\n",
    "        best_y_pred = y_pred\n",
    "        best_y_pred_prob = y_pred_prob\n",
    "        print(f\"ðŸŒŸ New best model! Fold {fold_no} with accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_no} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                annot_kws={'size': 14})\n",
    "    plt.title(f\"Fold {fold_no} Confusion Matrix\\nAcc: {test_acc:.4f} | AUC: {test_auc:.4f}\", \n",
    "              fontsize=14)\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"results\", f\"confusion_fold{fold_no}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Training History \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    plt.axhline(y=test_acc, color='r', linestyle='--', label=f'Test Acc: {test_acc:.4f}')\n",
    "    plt.title(f'Fold {fold_no} - Model Accuracy', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Accuracy', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    plt.title(f'Fold {fold_no} - Model Loss', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Loss', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"results\", f\"training_history_fold{fold_no}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Mean Test Accuracy across folds: {np.mean(acc_per_fold):.4f} (Â±{np.std(acc_per_fold):.4f})\")\n",
    "print(f\"ðŸ“Š Mean Test AUC across folds: {np.mean(auc_per_fold):.4f} (Â±{np.std(auc_per_fold):.4f})\")\n",
    "\n",
    "# Combine confusion matrices\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(total_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            annot_kws={'size': 14})\n",
    "plt.title(\"Overall Confusion Matrix (All Folds)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"results\", \"confusion_overall.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Print Fold Metrics\n",
    "print(\"\\nðŸ“‹ Fold-wise Metrics Summary:\")\n",
    "for i in range(len(fold_metrics['fold_no'])):\n",
    "    print(f\"\\nðŸ”¸ Fold {fold_metrics['fold_no'][i]} Metrics:\")\n",
    "    print(f\"  Train Accuracy : {fold_metrics['train_acc'][i]:.4f}\")\n",
    "    print(f\"  Val Accuracy   : {fold_metrics['val_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Accuracy  : {fold_metrics['test_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Loss      : {fold_metrics['test_loss'][i]:.4f}\")\n",
    "    print(f\"  Precision      : {fold_metrics['precision'][i]:.4f}\")\n",
    "    print(f\"  Recall         : {fold_metrics['recall'][i]:.4f}\")\n",
    "    print(f\"  F1 Score       : {fold_metrics['f1_score'][i]:.4f}\")\n",
    "    print(f\"  Test AUC       : {fold_metrics['test_auc'][i]:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "best_model.save(os.path.join(\"results\", \"best_model.keras\"))\n",
    "print(f\"\\nðŸ’¾ Best model (Fold {best_fold}) saved as 'results/best_model.keras'\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
