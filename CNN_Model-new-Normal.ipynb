{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bba84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes: ['ICTAL' 'INTERICTAL' 'NORMAL']\n",
      "Binary classes: 0 (NORMAL) vs 1 (INTERICTAL + ICTAL)\n",
      "Binary labels distribution: (array([0, 1]), array([4400, 6600], dtype=int64))\n",
      "Dataset shape: (11000, 347, 1)\n",
      "Class weights: {0: 1.25, 1: 0.8333333333333334}\n",
      "ğŸ² Random state used for this run: 337\n",
      "\n",
      "============================================================\n",
      " MODEL ARCHITECTURE: IMPROVED (Your architecture + SEBlock)\n",
      " DATA AUGMENTATION: ENABLED âœ“\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      " FOLD 1\n",
      "============================================================\n",
      "\n",
      "Train: 7480, Val: 1320, Test: 2200\n",
      "Test set distribution: (array([0, 1]), array([ 880, 1320], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "ğŸ—ï¸  Using IMPROVED model with SEBlock attention\n",
      "\n",
      "ğŸš€ Training Fold 1...\n",
      "ğŸ“Š Using data augmentation with real-time generator\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.6324 - loss: 1.3520 - val_accuracy: 0.8621 - val_loss: 1.1077 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 34ms/step - accuracy: 0.7934 - loss: 1.0781 - val_accuracy: 0.8886 - val_loss: 0.9041 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 34ms/step - accuracy: 0.8210 - loss: 0.9108 - val_accuracy: 0.8977 - val_loss: 0.7543 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 34ms/step - accuracy: 0.8503 - loss: 0.7635 - val_accuracy: 0.9045 - val_loss: 0.6222 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 34ms/step - accuracy: 0.8623 - loss: 0.6489 - val_accuracy: 0.9189 - val_loss: 0.5141 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 34ms/step - accuracy: 0.8787 - loss: 0.5574 - val_accuracy: 0.9341 - val_loss: 0.4334 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 37ms/step - accuracy: 0.8880 - loss: 0.4875 - val_accuracy: 0.9311 - val_loss: 0.3818 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 35ms/step - accuracy: 0.8981 - loss: 0.4276 - val_accuracy: 0.9409 - val_loss: 0.3298 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 35ms/step - accuracy: 0.8979 - loss: 0.3971 - val_accuracy: 0.9417 - val_loss: 0.3006 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9041 - loss: 0.3550 - val_accuracy: 0.9402 - val_loss: 0.2747 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 35ms/step - accuracy: 0.9080 - loss: 0.3320 - val_accuracy: 0.9432 - val_loss: 0.2516 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 36ms/step - accuracy: 0.9160 - loss: 0.3027 - val_accuracy: 0.9447 - val_loss: 0.2337 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35419s\u001b[0m 38s/step - accuracy: 0.9168 - loss: 0.2897 - val_accuracy: 0.9492 - val_loss: 0.2119 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 48ms/step - accuracy: 0.9166 - loss: 0.2718 - val_accuracy: 0.9492 - val_loss: 0.1973 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.9209 - loss: 0.2609 - val_accuracy: 0.9485 - val_loss: 0.1935 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 36ms/step - accuracy: 0.9247 - loss: 0.2477 - val_accuracy: 0.9523 - val_loss: 0.1791 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9227 - loss: 0.2434 - val_accuracy: 0.9530 - val_loss: 0.1719 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 46ms/step - accuracy: 0.9241 - loss: 0.2272 - val_accuracy: 0.9439 - val_loss: 0.1731 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 38ms/step - accuracy: 0.9270 - loss: 0.2174 - val_accuracy: 0.9530 - val_loss: 0.1618 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9294 - loss: 0.2095 - val_accuracy: 0.9553 - val_loss: 0.1511 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9309 - loss: 0.2079 - val_accuracy: 0.9508 - val_loss: 0.1484 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9317 - loss: 0.1961 - val_accuracy: 0.9553 - val_loss: 0.1462 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9336 - loss: 0.1930 - val_accuracy: 0.9500 - val_loss: 0.1428 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9317 - loss: 0.1884 - val_accuracy: 0.9417 - val_loss: 0.1517 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9362 - loss: 0.1839 - val_accuracy: 0.9500 - val_loss: 0.1382 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9345 - loss: 0.1810 - val_accuracy: 0.9545 - val_loss: 0.1272 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 36ms/step - accuracy: 0.9368 - loss: 0.1767 - val_accuracy: 0.9508 - val_loss: 0.1255 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9350 - loss: 0.1755 - val_accuracy: 0.9598 - val_loss: 0.1237 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 37ms/step - accuracy: 0.9401 - loss: 0.1703 - val_accuracy: 0.9629 - val_loss: 0.1141 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9389 - loss: 0.1684 - val_accuracy: 0.9568 - val_loss: 0.1206 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9402 - loss: 0.1642 - val_accuracy: 0.9598 - val_loss: 0.1114 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9354 - loss: 0.1635 - val_accuracy: 0.9568 - val_loss: 0.1150 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9393 - loss: 0.1578 - val_accuracy: 0.9606 - val_loss: 0.1091 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 35ms/step - accuracy: 0.9400 - loss: 0.1577 - val_accuracy: 0.9530 - val_loss: 0.1146 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9370 - loss: 0.1556 - val_accuracy: 0.9621 - val_loss: 0.1066 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9376 - loss: 0.1566 - val_accuracy: 0.9568 - val_loss: 0.1054 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 41ms/step - accuracy: 0.9406 - loss: 0.1488 - val_accuracy: 0.9576 - val_loss: 0.1067 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 38ms/step - accuracy: 0.9430 - loss: 0.1502 - val_accuracy: 0.9576 - val_loss: 0.1064 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.9461 - loss: 0.1439 - val_accuracy: 0.9591 - val_loss: 0.1009 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 38ms/step - accuracy: 0.9401 - loss: 0.1463 - val_accuracy: 0.9568 - val_loss: 0.1021 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 38ms/step - accuracy: 0.9448 - loss: 0.1403 - val_accuracy: 0.9598 - val_loss: 0.0986 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 37ms/step - accuracy: 0.9444 - loss: 0.1428 - val_accuracy: 0.9606 - val_loss: 0.0975 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.9451 - loss: 0.1403 - val_accuracy: 0.9530 - val_loss: 0.1014 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.9441 - loss: 0.1405 - val_accuracy: 0.9530 - val_loss: 0.1074 - learning_rate: 1.0000e-04\n",
      "Epoch 45/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9436 - loss: 0.1410 - val_accuracy: 0.9523 - val_loss: 0.1023 - learning_rate: 1.0000e-04\n",
      "Epoch 46/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 38ms/step - accuracy: 0.9434 - loss: 0.1394 - val_accuracy: 0.9621 - val_loss: 0.0983 - learning_rate: 1.0000e-04\n",
      "Epoch 47/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.9451 - loss: 0.1386 - val_accuracy: 0.9553 - val_loss: 0.1048 - learning_rate: 1.0000e-04\n",
      "Epoch 48/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.9448 - loss: 0.1367 - val_accuracy: 0.9523 - val_loss: 0.0992 - learning_rate: 1.0000e-04\n",
      "Epoch 49/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - accuracy: 0.9406 - loss: 0.1332 - val_accuracy: 0.9636 - val_loss: 0.0906 - learning_rate: 1.0000e-04\n",
      "Epoch 50/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 57ms/step - accuracy: 0.9439 - loss: 0.1329 - val_accuracy: 0.9652 - val_loss: 0.0911 - learning_rate: 1.0000e-04\n",
      "Epoch 51/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 54ms/step - accuracy: 0.9464 - loss: 0.1338 - val_accuracy: 0.9591 - val_loss: 0.0897 - learning_rate: 1.0000e-04\n",
      "Epoch 52/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 49ms/step - accuracy: 0.9479 - loss: 0.1292 - val_accuracy: 0.9621 - val_loss: 0.0966 - learning_rate: 1.0000e-04\n",
      "Epoch 53/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9468 - loss: 0.1338 - val_accuracy: 0.9606 - val_loss: 0.0924 - learning_rate: 1.0000e-04\n",
      "Epoch 54/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 48ms/step - accuracy: 0.9452 - loss: 0.1320 - val_accuracy: 0.9614 - val_loss: 0.0887 - learning_rate: 1.0000e-04\n",
      "Epoch 55/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 54ms/step - accuracy: 0.9456 - loss: 0.1299 - val_accuracy: 0.9598 - val_loss: 0.0887 - learning_rate: 1.0000e-04\n",
      "Epoch 56/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9501 - loss: 0.1268 - val_accuracy: 0.9629 - val_loss: 0.0916 - learning_rate: 1.0000e-04\n",
      "Epoch 57/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.9453 - loss: 0.1326 - val_accuracy: 0.9591 - val_loss: 0.0908 - learning_rate: 1.0000e-04\n",
      "Epoch 58/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 46ms/step - accuracy: 0.9517 - loss: 0.1267 - val_accuracy: 0.9614 - val_loss: 0.0874 - learning_rate: 1.0000e-04\n",
      "Epoch 59/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 50ms/step - accuracy: 0.9488 - loss: 0.1276 - val_accuracy: 0.9614 - val_loss: 0.0866 - learning_rate: 1.0000e-04\n",
      "Epoch 60/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 46ms/step - accuracy: 0.9491 - loss: 0.1296 - val_accuracy: 0.9606 - val_loss: 0.0858 - learning_rate: 1.0000e-04\n",
      "Epoch 61/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 47ms/step - accuracy: 0.9511 - loss: 0.1243 - val_accuracy: 0.9667 - val_loss: 0.0862 - learning_rate: 1.0000e-04\n",
      "Epoch 62/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 47ms/step - accuracy: 0.9491 - loss: 0.1277 - val_accuracy: 0.9614 - val_loss: 0.0863 - learning_rate: 1.0000e-04\n",
      "Epoch 63/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 48ms/step - accuracy: 0.9483 - loss: 0.1249 - val_accuracy: 0.9644 - val_loss: 0.0917 - learning_rate: 1.0000e-04\n",
      "Epoch 64/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 51ms/step - accuracy: 0.9475 - loss: 0.1263 - val_accuracy: 0.9636 - val_loss: 0.0918 - learning_rate: 1.0000e-04\n",
      "Epoch 65/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 44ms/step - accuracy: 0.9501 - loss: 0.1243 - val_accuracy: 0.9636 - val_loss: 0.0867 - learning_rate: 1.0000e-04\n",
      "Epoch 66/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9515 - loss: 0.1201 - val_accuracy: 0.9614 - val_loss: 0.0860 - learning_rate: 1.0000e-04\n",
      "Epoch 67/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 45ms/step - accuracy: 0.9496 - loss: 0.1194 - val_accuracy: 0.9636 - val_loss: 0.0834 - learning_rate: 1.0000e-04\n",
      "Epoch 68/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 45ms/step - accuracy: 0.9508 - loss: 0.1213 - val_accuracy: 0.9621 - val_loss: 0.0831 - learning_rate: 1.0000e-04\n",
      "Epoch 69/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 48ms/step - accuracy: 0.9535 - loss: 0.1216 - val_accuracy: 0.9621 - val_loss: 0.0937 - learning_rate: 1.0000e-04\n",
      "Epoch 70/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 50ms/step - accuracy: 0.9448 - loss: 0.1245 - val_accuracy: 0.9652 - val_loss: 0.0888 - learning_rate: 1.0000e-04\n",
      "Epoch 71/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 56ms/step - accuracy: 0.9543 - loss: 0.1187 - val_accuracy: 0.9659 - val_loss: 0.0830 - learning_rate: 1.0000e-04\n",
      "Epoch 72/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 46ms/step - accuracy: 0.9528 - loss: 0.1180 - val_accuracy: 0.9621 - val_loss: 0.0867 - learning_rate: 1.0000e-04\n",
      "Epoch 73/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 38ms/step - accuracy: 0.9525 - loss: 0.1168 - val_accuracy: 0.9652 - val_loss: 0.0818 - learning_rate: 1.0000e-04\n",
      "Epoch 74/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 37ms/step - accuracy: 0.9512 - loss: 0.1179 - val_accuracy: 0.9629 - val_loss: 0.0823 - learning_rate: 1.0000e-04\n",
      "Epoch 75/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 37ms/step - accuracy: 0.9536 - loss: 0.1162 - val_accuracy: 0.9629 - val_loss: 0.0815 - learning_rate: 1.0000e-04\n",
      "Epoch 76/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 37ms/step - accuracy: 0.9525 - loss: 0.1182 - val_accuracy: 0.9614 - val_loss: 0.0839 - learning_rate: 1.0000e-04\n",
      "Epoch 77/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 40ms/step - accuracy: 0.9523 - loss: 0.1145 - val_accuracy: 0.9614 - val_loss: 0.0806 - learning_rate: 1.0000e-04\n",
      "Epoch 78/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 40ms/step - accuracy: 0.9513 - loss: 0.1138 - val_accuracy: 0.9629 - val_loss: 0.0811 - learning_rate: 1.0000e-04\n",
      "Epoch 79/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8072s\u001b[0m 9s/step - accuracy: 0.9523 - loss: 0.1138 - val_accuracy: 0.9561 - val_loss: 0.0897 - learning_rate: 1.0000e-04\n",
      "Epoch 80/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9528 - loss: 0.1180 - val_accuracy: 0.9621 - val_loss: 0.0847 - learning_rate: 1.0000e-04\n",
      "Epoch 81/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 40ms/step - accuracy: 0.9531 - loss: 0.1130 - val_accuracy: 0.9606 - val_loss: 0.0839 - learning_rate: 1.0000e-04\n",
      "Epoch 82/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.9535 - loss: 0.1172 - val_accuracy: 0.9621 - val_loss: 0.0815 - learning_rate: 1.0000e-04\n",
      "Epoch 83/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9513 - loss: 0.1130 - val_accuracy: 0.9583 - val_loss: 0.0921 - learning_rate: 1.0000e-04\n",
      "Epoch 84/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9544 - loss: 0.1114 - val_accuracy: 0.9621 - val_loss: 0.0844 - learning_rate: 1.0000e-04\n",
      "Epoch 85/150\n",
      "\u001b[1m934/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9507 - loss: 0.1169\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.999999848427251e-05.\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 41ms/step - accuracy: 0.9520 - loss: 0.1169 - val_accuracy: 0.9583 - val_loss: 0.0830 - learning_rate: 1.0000e-04\n",
      "Epoch 86/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9560 - loss: 0.1137 - val_accuracy: 0.9636 - val_loss: 0.0819 - learning_rate: 6.0000e-05\n",
      "Epoch 87/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 41ms/step - accuracy: 0.9584 - loss: 0.1071 - val_accuracy: 0.9629 - val_loss: 0.0834 - learning_rate: 6.0000e-05\n",
      "Epoch 88/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.9591 - loss: 0.1076 - val_accuracy: 0.9621 - val_loss: 0.0797 - learning_rate: 6.0000e-05\n",
      "Epoch 89/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 40ms/step - accuracy: 0.9556 - loss: 0.1051 - val_accuracy: 0.9614 - val_loss: 0.0782 - learning_rate: 6.0000e-05\n",
      "Epoch 90/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 40ms/step - accuracy: 0.9568 - loss: 0.1070 - val_accuracy: 0.9629 - val_loss: 0.0794 - learning_rate: 6.0000e-05\n",
      "Epoch 91/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 40ms/step - accuracy: 0.9608 - loss: 0.1012 - val_accuracy: 0.9598 - val_loss: 0.0785 - learning_rate: 6.0000e-05\n",
      "Epoch 92/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9564 - loss: 0.1035 - val_accuracy: 0.9644 - val_loss: 0.0792 - learning_rate: 6.0000e-05\n",
      "Epoch 93/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 47ms/step - accuracy: 0.9592 - loss: 0.1048 - val_accuracy: 0.9636 - val_loss: 0.0788 - learning_rate: 6.0000e-05\n",
      "Epoch 94/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 49ms/step - accuracy: 0.9602 - loss: 0.1042 - val_accuracy: 0.9606 - val_loss: 0.0780 - learning_rate: 6.0000e-05\n",
      "Epoch 95/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9568 - loss: 0.1043 - val_accuracy: 0.9644 - val_loss: 0.0796 - learning_rate: 6.0000e-05\n",
      "Epoch 96/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9596 - loss: 0.1005 - val_accuracy: 0.9659 - val_loss: 0.0765 - learning_rate: 6.0000e-05\n",
      "Epoch 97/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9560 - loss: 0.1032 - val_accuracy: 0.9652 - val_loss: 0.0807 - learning_rate: 6.0000e-05\n",
      "Epoch 98/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 46ms/step - accuracy: 0.9583 - loss: 0.1016 - val_accuracy: 0.9629 - val_loss: 0.0780 - learning_rate: 6.0000e-05\n",
      "Epoch 99/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.9616 - loss: 0.0971 - val_accuracy: 0.9674 - val_loss: 0.0745 - learning_rate: 6.0000e-05\n",
      "Epoch 100/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 40ms/step - accuracy: 0.9612 - loss: 0.1007 - val_accuracy: 0.9636 - val_loss: 0.0791 - learning_rate: 6.0000e-05\n",
      "Epoch 101/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 40ms/step - accuracy: 0.9607 - loss: 0.0997 - val_accuracy: 0.9583 - val_loss: 0.0903 - learning_rate: 6.0000e-05\n",
      "Epoch 102/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 46ms/step - accuracy: 0.9615 - loss: 0.0996 - val_accuracy: 0.9652 - val_loss: 0.0759 - learning_rate: 6.0000e-05\n",
      "Epoch 103/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9592 - loss: 0.0988 - val_accuracy: 0.9629 - val_loss: 0.0758 - learning_rate: 6.0000e-05\n",
      "Epoch 104/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.9590 - loss: 0.1005 - val_accuracy: 0.9682 - val_loss: 0.0757 - learning_rate: 6.0000e-05\n",
      "Epoch 105/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.9608 - loss: 0.0938 - val_accuracy: 0.9659 - val_loss: 0.0764 - learning_rate: 6.0000e-05\n",
      "Epoch 106/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9596 - loss: 0.1007 - val_accuracy: 0.9636 - val_loss: 0.0812 - learning_rate: 6.0000e-05\n",
      "Epoch 107/150\n",
      "\u001b[1m934/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9607 - loss: 0.0995\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.599999909056351e-05.\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9603 - loss: 0.1001 - val_accuracy: 0.9667 - val_loss: 0.0787 - learning_rate: 6.0000e-05\n",
      "Epoch 108/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.9616 - loss: 0.0934 - val_accuracy: 0.9667 - val_loss: 0.0733 - learning_rate: 3.6000e-05\n",
      "Epoch 109/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9643 - loss: 0.0899 - val_accuracy: 0.9652 - val_loss: 0.0717 - learning_rate: 3.6000e-05\n",
      "Epoch 110/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 88ms/step - accuracy: 0.9634 - loss: 0.0925 - val_accuracy: 0.9674 - val_loss: 0.0730 - learning_rate: 3.6000e-05\n",
      "Epoch 111/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 62ms/step - accuracy: 0.9643 - loss: 0.0935 - val_accuracy: 0.9636 - val_loss: 0.0720 - learning_rate: 3.6000e-05\n",
      "Epoch 112/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 49ms/step - accuracy: 0.9644 - loss: 0.0903 - val_accuracy: 0.9667 - val_loss: 0.0736 - learning_rate: 3.6000e-05\n",
      "Epoch 113/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 53ms/step - accuracy: 0.9647 - loss: 0.0897 - val_accuracy: 0.9659 - val_loss: 0.0718 - learning_rate: 3.6000e-05\n",
      "Epoch 114/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 51ms/step - accuracy: 0.9611 - loss: 0.0917 - val_accuracy: 0.9667 - val_loss: 0.0731 - learning_rate: 3.6000e-05\n",
      "Epoch 115/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 54ms/step - accuracy: 0.9639 - loss: 0.0904 - val_accuracy: 0.9682 - val_loss: 0.0717 - learning_rate: 3.6000e-05\n",
      "Epoch 116/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 56ms/step - accuracy: 0.9659 - loss: 0.0901 - val_accuracy: 0.9674 - val_loss: 0.0716 - learning_rate: 3.6000e-05\n",
      "Epoch 117/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 53ms/step - accuracy: 0.9611 - loss: 0.0918 - val_accuracy: 0.9682 - val_loss: 0.0700 - learning_rate: 3.6000e-05\n",
      "Epoch 118/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 50ms/step - accuracy: 0.9636 - loss: 0.0920 - val_accuracy: 0.9674 - val_loss: 0.0703 - learning_rate: 3.6000e-05\n",
      "Epoch 119/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 62ms/step - accuracy: 0.9628 - loss: 0.0918 - val_accuracy: 0.9674 - val_loss: 0.0712 - learning_rate: 3.6000e-05\n",
      "Epoch 120/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 64ms/step - accuracy: 0.9628 - loss: 0.0895 - val_accuracy: 0.9689 - val_loss: 0.0703 - learning_rate: 3.6000e-05\n",
      "Epoch 121/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 72ms/step - accuracy: 0.9651 - loss: 0.0858 - val_accuracy: 0.9689 - val_loss: 0.0694 - learning_rate: 3.6000e-05\n",
      "Epoch 122/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 74ms/step - accuracy: 0.9654 - loss: 0.0865 - val_accuracy: 0.9674 - val_loss: 0.0704 - learning_rate: 3.6000e-05\n",
      "Epoch 123/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 75ms/step - accuracy: 0.9642 - loss: 0.0864 - val_accuracy: 0.9682 - val_loss: 0.0713 - learning_rate: 3.6000e-05\n",
      "Epoch 124/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 74ms/step - accuracy: 0.9634 - loss: 0.0894 - val_accuracy: 0.9697 - val_loss: 0.0703 - learning_rate: 3.6000e-05\n",
      "Epoch 125/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 63ms/step - accuracy: 0.9652 - loss: 0.0887 - val_accuracy: 0.9705 - val_loss: 0.0697 - learning_rate: 3.6000e-05\n",
      "Epoch 126/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9646 - loss: 0.0853 - val_accuracy: 0.9682 - val_loss: 0.0698 - learning_rate: 3.6000e-05\n",
      "Epoch 127/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9638 - loss: 0.0883 - val_accuracy: 0.9697 - val_loss: 0.0691 - learning_rate: 3.6000e-05\n",
      "Epoch 128/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 60ms/step - accuracy: 0.9670 - loss: 0.0854 - val_accuracy: 0.9689 - val_loss: 0.0729 - learning_rate: 3.6000e-05\n",
      "Epoch 129/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 52ms/step - accuracy: 0.9642 - loss: 0.0851 - val_accuracy: 0.9720 - val_loss: 0.0695 - learning_rate: 3.6000e-05\n",
      "Epoch 130/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 41ms/step - accuracy: 0.9646 - loss: 0.0888 - val_accuracy: 0.9697 - val_loss: 0.0720 - learning_rate: 3.6000e-05\n",
      "Epoch 131/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 41ms/step - accuracy: 0.9662 - loss: 0.0835 - val_accuracy: 0.9674 - val_loss: 0.0728 - learning_rate: 3.6000e-05\n",
      "Epoch 132/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 41ms/step - accuracy: 0.9642 - loss: 0.0869 - val_accuracy: 0.9697 - val_loss: 0.0705 - learning_rate: 3.6000e-05\n",
      "Epoch 133/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.9658 - loss: 0.0861 - val_accuracy: 0.9682 - val_loss: 0.0699 - learning_rate: 3.6000e-05\n",
      "Epoch 134/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 41ms/step - accuracy: 0.9664 - loss: 0.0851 - val_accuracy: 0.9697 - val_loss: 0.0697 - learning_rate: 3.6000e-05\n",
      "Epoch 135/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 43ms/step - accuracy: 0.9648 - loss: 0.0847 - val_accuracy: 0.9682 - val_loss: 0.0688 - learning_rate: 3.6000e-05\n",
      "Epoch 136/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.9647 - loss: 0.0869 - val_accuracy: 0.9697 - val_loss: 0.0695 - learning_rate: 3.6000e-05\n",
      "Epoch 137/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 46ms/step - accuracy: 0.9655 - loss: 0.0855 - val_accuracy: 0.9659 - val_loss: 0.0711 - learning_rate: 3.6000e-05\n",
      "Epoch 138/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9638 - loss: 0.0849 - val_accuracy: 0.9712 - val_loss: 0.0666 - learning_rate: 3.6000e-05\n",
      "Epoch 139/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9666 - loss: 0.0843 - val_accuracy: 0.9727 - val_loss: 0.0672 - learning_rate: 3.6000e-05\n",
      "Epoch 140/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9660 - loss: 0.0846 - val_accuracy: 0.9705 - val_loss: 0.0686 - learning_rate: 3.6000e-05\n",
      "Epoch 141/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 41ms/step - accuracy: 0.9666 - loss: 0.0828 - val_accuracy: 0.9689 - val_loss: 0.0681 - learning_rate: 3.6000e-05\n",
      "Epoch 142/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.9667 - loss: 0.0863 - val_accuracy: 0.9697 - val_loss: 0.0684 - learning_rate: 3.6000e-05\n",
      "Epoch 143/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - accuracy: 0.9632 - loss: 0.0831 - val_accuracy: 0.9682 - val_loss: 0.0786 - learning_rate: 3.6000e-05\n",
      "Epoch 144/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - accuracy: 0.9691 - loss: 0.0839 - val_accuracy: 0.9705 - val_loss: 0.0687 - learning_rate: 3.6000e-05\n",
      "Epoch 145/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 150ms/step - accuracy: 0.9675 - loss: 0.0808 - val_accuracy: 0.9697 - val_loss: 0.0680 - learning_rate: 3.6000e-05\n",
      "Epoch 146/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9652 - loss: 0.0865\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 2.1599998581223188e-05.\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 58ms/step - accuracy: 0.9679 - loss: 0.0824 - val_accuracy: 0.9712 - val_loss: 0.0681 - learning_rate: 3.6000e-05\n",
      "Epoch 147/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 66ms/step - accuracy: 0.9694 - loss: 0.0809 - val_accuracy: 0.9727 - val_loss: 0.0670 - learning_rate: 2.1600e-05\n",
      "Epoch 148/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 71ms/step - accuracy: 0.9698 - loss: 0.0775 - val_accuracy: 0.9697 - val_loss: 0.0678 - learning_rate: 2.1600e-05\n",
      "Epoch 149/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 60ms/step - accuracy: 0.9662 - loss: 0.0799 - val_accuracy: 0.9720 - val_loss: 0.0679 - learning_rate: 2.1600e-05\n",
      "Epoch 150/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 52ms/step - accuracy: 0.9660 - loss: 0.0831 - val_accuracy: 0.9727 - val_loss: 0.0678 - learning_rate: 2.1600e-05\n",
      "Restoring model weights from the end of the best epoch: 138.\n",
      "\n",
      "âœ… Fold 1 Results:\n",
      "  Test Accuracy: 0.9727\n",
      "  Test AUC: 0.9969\n",
      "  Test Loss: 0.0694\n",
      "ğŸŒŸ New best model! Fold 1 with accuracy: 0.9727\n",
      "\n",
      "Fold 1 Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                NORMAL     0.9702    0.9614    0.9658       880\n",
      "ALL (INTERICTAL+ICTAL)     0.9744    0.9803    0.9773      1320\n",
      "\n",
      "              accuracy                         0.9727      2200\n",
      "             macro avg     0.9723    0.9708    0.9715      2200\n",
      "          weighted avg     0.9727    0.9727    0.9727      2200\n",
      "\n",
      "\n",
      "============================================================\n",
      " FOLD 2\n",
      "============================================================\n",
      "\n",
      "Train: 7480, Val: 1320, Test: 2200\n",
      "Test set distribution: (array([0, 1]), array([ 880, 1320], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸  Using IMPROVED model with SEBlock attention\n",
      "\n",
      "ğŸš€ Training Fold 2...\n",
      "ğŸ“Š Using data augmentation with real-time generator\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 50ms/step - accuracy: 0.6447 - loss: 1.3167 - val_accuracy: 0.8500 - val_loss: 1.0571 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 48ms/step - accuracy: 0.8023 - loss: 1.0203 - val_accuracy: 0.9008 - val_loss: 0.8404 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.8314 - loss: 0.8454 - val_accuracy: 0.9152 - val_loss: 0.6879 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 50ms/step - accuracy: 0.8496 - loss: 0.7130 - val_accuracy: 0.9197 - val_loss: 0.5763 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.8647 - loss: 0.6094 - val_accuracy: 0.9250 - val_loss: 0.4852 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - accuracy: 0.8731 - loss: 0.5322 - val_accuracy: 0.9265 - val_loss: 0.4169 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 41ms/step - accuracy: 0.8882 - loss: 0.4658 - val_accuracy: 0.9386 - val_loss: 0.3639 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 47ms/step - accuracy: 0.8951 - loss: 0.4174 - val_accuracy: 0.9402 - val_loss: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m118/935\u001b[0m \u001b[32mâ”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m43s\u001b[0m 53ms/step - accuracy: 0.8946 - loss: 0.3938"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 474\u001b[0m\n\u001b[0;32m    466\u001b[0m     train_generator \u001b[38;5;241m=\u001b[39m AugmentedDataGenerator(\n\u001b[0;32m    467\u001b[0m         X_train, y_train, \n\u001b[0;32m    468\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[0;32m    469\u001b[0m         augmentation_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m    470\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: Validation data is NOT augmented\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    475\u001b[0m         train_generator,\n\u001b[0;32m    476\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[0;32m    477\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m    478\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    479\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39mclass_weight_dict,\n\u001b[0;32m    480\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    481\u001b[0m     )\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“Š Training without data augmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1689\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1690\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1691\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1692\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1693\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1694\u001b[0m   )\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, \n",
    "    GlobalAveragePooling1D, Input, Activation, SpatialDropout1D, \n",
    "    LSTM, Bidirectional, Multiply, Reshape, LeakyReLU\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# ===================== DATA AUGMENTATION =====================\n",
    "def augment_signal(segment, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply random augmentation to EEG segment\n",
    "    \n",
    "    Args:\n",
    "        segment: Input EEG segment (1D array)\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented segment\n",
    "    \"\"\"\n",
    "    if np.random.random() > augmentation_prob:\n",
    "        return segment  # No augmentation\n",
    "    \n",
    "    # Choose augmentation type\n",
    "    aug_type = np.random.choice(['noise', 'scale', 'shift', 'time_shift'], p=[0.3, 0.3, 0.2, 0.2])\n",
    "    \n",
    "    if aug_type == 'noise':\n",
    "        # Add Gaussian noise\n",
    "        noise_level = np.random.uniform(0.01, 0.05)\n",
    "        noise = np.random.normal(0, noise_level, segment.shape)\n",
    "        return segment + noise\n",
    "    \n",
    "    elif aug_type == 'scale':\n",
    "        # Random amplitude scaling\n",
    "        scale = np.random.uniform(0.9, 1.1)\n",
    "        return segment * scale\n",
    "    \n",
    "    elif aug_type == 'shift':\n",
    "        # Random DC shift\n",
    "        shift = np.random.uniform(-0.1, 0.1)\n",
    "        return segment + shift\n",
    "    \n",
    "    elif aug_type == 'time_shift':\n",
    "        # Random time shift (circular shift)\n",
    "        shift_amount = np.random.randint(-20, 20)\n",
    "        return np.roll(segment, shift_amount)\n",
    "    \n",
    "    return segment\n",
    "\n",
    "\n",
    "def augment_batch(X_batch, y_batch, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply augmentation to a batch of data\n",
    "    \n",
    "    Args:\n",
    "        X_batch: Batch of EEG segments (batch_size, time_steps, channels)\n",
    "        y_batch: Batch of labels\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented batch\n",
    "    \"\"\"\n",
    "    X_augmented = np.zeros_like(X_batch)\n",
    "    \n",
    "    for i in range(len(X_batch)):\n",
    "        # Only augment positive class (seizure) more aggressively\n",
    "        if y_batch[i] == 1:\n",
    "            prob = augmentation_prob * 1.5  # Higher probability for minority class\n",
    "        else:\n",
    "            prob = augmentation_prob\n",
    "        \n",
    "        X_augmented[i, :, 0] = augment_signal(X_batch[i, :, 0], prob)\n",
    "    \n",
    "    return X_augmented\n",
    "\n",
    "\n",
    "class DataAugmentationCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback to apply data augmentation during training\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, augmentation_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.X_train_original = X_train.copy()\n",
    "        self.y_train = y_train\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Apply augmentation at the start of each epoch\n",
    "        if epoch > 0:  # Skip first epoch to see baseline performance\n",
    "            X_augmented = augment_batch(\n",
    "                self.X_train_original, \n",
    "                self.y_train, \n",
    "                self.augmentation_prob\n",
    "            )\n",
    "            # Update model's training data\n",
    "            self.model.stop_training = False\n",
    "\n",
    "\n",
    "# ===================== CUSTOM DATA GENERATOR =====================\n",
    "class AugmentedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator with real-time augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, augmentation_prob=0.5, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.X))\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        \n",
    "        # Get batch data\n",
    "        X_batch = self.X[batch_indices].copy()\n",
    "        y_batch = self.y[batch_indices]\n",
    "        \n",
    "        # Apply augmentation\n",
    "        X_batch = augment_batch(X_batch, y_batch, self.augmentation_prob)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "# ===================== SE BLOCK IMPLEMENTATION =====================\n",
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block for channel attention\n",
    "    Paper: \"Interpretable classification of epileptic EEG signals...\"\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction=8, **kwargs):\n",
    "        super(SEBlock, self).__init__(**kwargs)\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.squeeze = GlobalAveragePooling1D()\n",
    "        \n",
    "        # Excitation network\n",
    "        self.fc1 = Dense(\n",
    "            channels // self.reduction, \n",
    "            activation='relu', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.fc2 = Dense(\n",
    "            channels, \n",
    "            activation='sigmoid', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        \n",
    "        super(SEBlock, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Squeeze: Global average pooling\n",
    "        squeeze = self.squeeze(inputs)\n",
    "        \n",
    "        # Excitation: Learn channel importance\n",
    "        excitation = self.fc1(squeeze)\n",
    "        excitation = self.fc2(excitation)\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        excitation = tf.reshape(excitation, [-1, 1, tf.shape(inputs)[-1]])\n",
    "        \n",
    "        # Scale: Multiply input with learned weights\n",
    "        return Multiply()([inputs, excitation])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SEBlock, self).get_config()\n",
    "        config.update({\"reduction\": self.reduction})\n",
    "        return config\n",
    "\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "X = np.load(r\"preprocessed\\ALL_X.npy\")\n",
    "y = np.load(r\"preprocessed\\ALL_y.npy\")\n",
    "\n",
    "# Convert to Binary Classification \n",
    "y_encoded = np.where(y == 'NORMAL', 0, 1)\n",
    "\n",
    "print(\"Original classes:\", np.unique(y))\n",
    "print(\"Binary classes: 0 (NORMAL) vs 1 (INTERICTAL + ICTAL)\")\n",
    "print(\"Binary labels distribution:\", np.unique(y_encoded, return_counts=True))\n",
    "\n",
    "# Prepare Data\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "# Compute Class Weights \n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# ===================== PREPARE CROSS VALIDATION =====================\n",
    "random_state = np.random.randint(0, 10000)\n",
    "print(f\"ğŸ² Random state used for this run: {random_state}\")\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "fold_indices = [(train_val_idx, test_idx) for train_val_idx, test_idx in kfold.split(X, y_encoded)]\n",
    "\n",
    "# Save indices for reproducibility\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "np.save(\"results/fold_indices.npy\", np.array(fold_indices, dtype=object), allow_pickle=True)\n",
    "\n",
    "\n",
    "# ===================== LOSS FUNCTIONS =====================\n",
    "def focal_loss(alpha=0.75, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        \n",
    "        return K.mean(alpha_t * focal_term * cross_entropy)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def hybrid_focal_loss(alpha=0.70, gamma=1.5, focal_weight=0.5):\n",
    "    \"\"\"Hybrid: Focal + BCE\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        bce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        \n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal = alpha_t * focal_term * bce\n",
    "        \n",
    "        combined = focal_weight * focal + (1 - focal_weight) * bce\n",
    "        return K.mean(combined)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# ===================== MODEL BUILDING =====================\n",
    "def build_model(input_shape):\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # ========== Block 1: Local patterns ==========\n",
    "    x = Conv1D(48, kernel_size=7, padding='same', kernel_regularizer=l2(0.002))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    \n",
    "    # ========== Block 2: Mid-level features ==========\n",
    "    x = Conv1D(96, kernel_size=5, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.35)(x)\n",
    "    \n",
    "    # ========== Block 3: High-level features ==========\n",
    "    x = Conv1D(128, kernel_size=3, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    \n",
    "    # ========== LSTM Temporal Modeling ==========\n",
    "    # x = Bidirectional(LSTM(64, return_sequences=False, kernel_regularizer=l2(0.001)))(x)\n",
    "    # x = Dropout(0.4)(x)\n",
    "    \n",
    "    # ========== Dense Classification ==========\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===================== TRAINING =====================\n",
    "# Choose which model to use\n",
    "\n",
    "USE_DATA_AUGMENTATION = True  \n",
    "\n",
    "acc_per_fold = []\n",
    "auc_per_fold = []\n",
    "conf_matrices = []\n",
    "class_names = ['NORMAL', 'ALL (INTERICTAL+ICTAL)']\n",
    "\n",
    "# Track metrics for best model selection\n",
    "fold_metrics = {\n",
    "    'fold_no': [],\n",
    "    'test_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_auc': [],\n",
    "    'val_acc': [],\n",
    "    'train_acc': [],\n",
    "    'f1_score': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "best_model = None\n",
    "best_fold = None\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "\n",
    "for fold_no, (train_val_idx, test_idx) in enumerate(fold_indices, start=1):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" FOLD {fold_no}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Split into train/val/test\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y_encoded[train_val_idx], y_encoded[test_idx]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.15, stratify=y_train_val, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    print(f\"Test set distribution: {np.unique(y_test, return_counts=True)}\")\n",
    "\n",
    "    # Build model\n",
    "\n",
    "    model = build_model(input_shape=(X.shape[1], 1))\n",
    "\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss=hybrid_focal_loss(alpha=0.70, gamma=1.5, focal_weight=0.5),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=22, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1, \n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.6,\n",
    "        patience=8, \n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"results/model_fold{fold_no}.weights.h5\", \n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True,\n",
    "        verbose=0,  \n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    callbacks = [early_stop, reduce_lr, checkpoint]\n",
    "    \n",
    "    # Train Model\n",
    "    print(f\"\\nğŸš€ Training Fold {fold_no}...\")\n",
    "    \n",
    "    if USE_DATA_AUGMENTATION:\n",
    "        print(\"ğŸ“Š Using data augmentation with real-time generator\")\n",
    "        \n",
    "        # Create data generators\n",
    "        train_generator = AugmentedDataGenerator(\n",
    "            X_train, y_train, \n",
    "            batch_size=8, \n",
    "            augmentation_prob=0.5,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Note: Validation data is NOT augmented\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=150,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        print(\"ğŸ“Š Training without data augmentation\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=150,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_weights(f\"results/model_fold{fold_no}.weights.h5\")\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # AUC Score\n",
    "    test_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    auc_per_fold.append(test_auc)\n",
    "    \n",
    "    print(f\"\\nâœ… Fold {fold_no} Results:\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "    \n",
    "    # Classification Metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store metrics\n",
    "    fold_metrics['fold_no'].append(fold_no)\n",
    "    fold_metrics['test_acc'].append(test_acc)\n",
    "    fold_metrics['test_loss'].append(test_loss)\n",
    "    fold_metrics['test_auc'].append(test_auc)\n",
    "    fold_metrics['val_acc'].append(max(history.history['val_accuracy']))\n",
    "    fold_metrics['train_acc'].append(max(history.history['accuracy']))\n",
    "    fold_metrics['f1_score'].append(f1)\n",
    "    fold_metrics['precision'].append(precision)\n",
    "    fold_metrics['recall'].append(recall)\n",
    "    \n",
    "    # Check if this is the best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_fold = fold_no\n",
    "        best_model = model\n",
    "        best_cm = cm\n",
    "        best_y_test = y_test\n",
    "        best_y_pred = y_pred\n",
    "        best_y_pred_prob = y_pred_prob\n",
    "        print(f\"ğŸŒŸ New best model! Fold {fold_no} with accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_no} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                annot_kws={'size': 14})\n",
    "    plt.title(f\"Fold {fold_no} Confusion Matrix\\nAcc: {test_acc:.4f} | AUC: {test_auc:.4f}\", \n",
    "              fontsize=14)\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/confusion_fold{fold_no}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Training History \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    plt.axhline(y=test_acc, color='r', linestyle='--', label=f'Test Acc: {test_acc:.4f}')\n",
    "    plt.title(f'Fold {fold_no} - Model Accuracy', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Accuracy', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    plt.title(f'Fold {fold_no} - Model Loss', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Loss', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/training_history_fold{fold_no}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“Š Mean Test Accuracy across folds: {np.mean(acc_per_fold):.4f} (Â±{np.std(acc_per_fold):.4f})\")\n",
    "print(f\"ğŸ“Š Mean Test AUC across folds: {np.mean(auc_per_fold):.4f} (Â±{np.std(auc_per_fold):.4f})\")\n",
    "\n",
    "# Combine confusion matrices\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(total_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            annot_kws={'size': 14})\n",
    "plt.title(\"Overall Confusion Matrix (All Folds)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/confusion_overall.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Print Fold Metrics\n",
    "print(\"\\nğŸ“‹ Fold-wise Metrics Summary:\")\n",
    "for i in range(len(fold_metrics['fold_no'])):\n",
    "    print(f\"\\nğŸ”¸ Fold {fold_metrics['fold_no'][i]} Metrics:\")\n",
    "    print(f\"  Train Accuracy : {fold_metrics['train_acc'][i]:.4f}\")\n",
    "    print(f\"  Val Accuracy   : {fold_metrics['val_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Accuracy  : {fold_metrics['test_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Loss      : {fold_metrics['test_loss'][i]:.4f}\")\n",
    "    print(f\"  Precision      : {fold_metrics['precision'][i]:.4f}\")\n",
    "    print(f\"  Recall         : {fold_metrics['recall'][i]:.4f}\")\n",
    "    print(f\"  F1 Score       : {fold_metrics['f1_score'][i]:.4f}\")\n",
    "    print(f\"  Test AUC       : {fold_metrics['test_auc'][i]:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "best_model.save(\"results/best_model.keras\")\n",
    "print(f\"\\nğŸ’¾ Best model (Fold {best_fold}) saved as 'results/best_model.keras'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49321111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes: ['ICTAL' 'INTERICTAL' 'NORMAL']\n",
      "Binary classes: 0 (NORMAL) vs 1 (INTERICTAL + ICTAL)\n",
      "Binary labels distribution: (array([0, 1]), array([4400, 6600], dtype=int64))\n",
      "Dataset shape: (11000, 347, 1)\n",
      "Class weights: {0: 1.25, 1: 0.8333333333333334}\n",
      "ğŸ² Random state used for this run: 374\n",
      "\n",
      "============================================================\n",
      " FOLD 1\n",
      "============================================================\n",
      "\n",
      "Train: 7480, Val: 1320, Test: 2200\n",
      "Test set distribution: (array([0, 1]), array([ 880, 1320], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "ğŸš€ Training Fold 1...\n",
      "ğŸ“Š Using data augmentation with real-time generator\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.6198 - loss: 1.4365 - val_accuracy: 0.8083 - val_loss: 1.2278 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 29ms/step - accuracy: 0.7765 - loss: 1.1640 - val_accuracy: 0.8652 - val_loss: 1.0221 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 31ms/step - accuracy: 0.8135 - loss: 0.9859 - val_accuracy: 0.8871 - val_loss: 0.8632 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.8301 - loss: 0.8506 - val_accuracy: 0.8992 - val_loss: 0.7144 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 37ms/step - accuracy: 0.8406 - loss: 0.7306 - val_accuracy: 0.9288 - val_loss: 0.5877 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 29ms/step - accuracy: 0.8624 - loss: 0.6286 - val_accuracy: 0.9242 - val_loss: 0.5148 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 26ms/step - accuracy: 0.8747 - loss: 0.5528 - val_accuracy: 0.9159 - val_loss: 0.4491 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 24ms/step - accuracy: 0.8849 - loss: 0.4902 - val_accuracy: 0.9318 - val_loss: 0.3915 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 24ms/step - accuracy: 0.8896 - loss: 0.4461 - val_accuracy: 0.9447 - val_loss: 0.3411 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 24ms/step - accuracy: 0.9040 - loss: 0.3977 - val_accuracy: 0.9455 - val_loss: 0.3113 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 24ms/step - accuracy: 0.9016 - loss: 0.3716 - val_accuracy: 0.9515 - val_loss: 0.2811 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 30ms/step - accuracy: 0.9075 - loss: 0.3453 - val_accuracy: 0.9500 - val_loss: 0.2611 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 37ms/step - accuracy: 0.9087 - loss: 0.3254 - val_accuracy: 0.9182 - val_loss: 0.2698 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 41ms/step - accuracy: 0.9067 - loss: 0.3061 - val_accuracy: 0.9553 - val_loss: 0.2194 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m935/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 40ms/step - accuracy: 0.9132 - loss: 0.2823 - val_accuracy: 0.9538 - val_loss: 0.2128 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m567/935\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.9081 - loss: 0.2833"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 417\u001b[0m\n\u001b[0;32m    409\u001b[0m     train_generator \u001b[38;5;241m=\u001b[39m AugmentedDataGenerator(\n\u001b[0;32m    410\u001b[0m         X_train, y_train, \n\u001b[0;32m    411\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[0;32m    412\u001b[0m         augmentation_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m    413\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    414\u001b[0m     )\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# Note: Validation data is NOT augmented\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    418\u001b[0m         train_generator,\n\u001b[0;32m    419\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[0;32m    420\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m    421\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    422\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39mclass_weight_dict,\n\u001b[0;32m    423\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    424\u001b[0m     )\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“Š Training without data augmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1689\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1690\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1691\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1692\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1693\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1694\u001b[0m   )\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, \n",
    "    GlobalAveragePooling1D, Input, Activation, SpatialDropout1D, \n",
    "    LSTM, Bidirectional, Multiply, Reshape, LeakyReLU\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# ===================== DATA AUGMENTATION =====================\n",
    "def augment_signal(segment, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply random augmentation to EEG segment\n",
    "    \n",
    "    Args:\n",
    "        segment: Input EEG segment (1D array)\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented segment\n",
    "    \"\"\"\n",
    "    if np.random.random() > augmentation_prob:\n",
    "        return segment  # No augmentation\n",
    "    \n",
    "    # Choose augmentation type\n",
    "    aug_type = np.random.choice(['noise', 'scale', 'shift', 'time_shift'], p=[0.3, 0.3, 0.2, 0.2])\n",
    "    \n",
    "    if aug_type == 'noise':\n",
    "        # Add Gaussian noise\n",
    "        noise_level = np.random.uniform(0.01, 0.05)\n",
    "        noise = np.random.normal(0, noise_level, segment.shape)\n",
    "        return segment + noise\n",
    "    \n",
    "    elif aug_type == 'scale':\n",
    "        # Random amplitude scaling\n",
    "        scale = np.random.uniform(0.9, 1.1)\n",
    "        return segment * scale\n",
    "    \n",
    "    elif aug_type == 'shift':\n",
    "        # Random DC shift\n",
    "        shift = np.random.uniform(-0.1, 0.1)\n",
    "        return segment + shift\n",
    "    \n",
    "    elif aug_type == 'time_shift':\n",
    "        # Random time shift (circular shift)\n",
    "        shift_amount = np.random.randint(-20, 20)\n",
    "        return np.roll(segment, shift_amount)\n",
    "    \n",
    "    return segment\n",
    "\n",
    "\n",
    "def augment_batch(X_batch, y_batch, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply augmentation to a batch of data\n",
    "    \n",
    "    Args:\n",
    "        X_batch: Batch of EEG segments (batch_size, time_steps, channels)\n",
    "        y_batch: Batch of labels\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented batch\n",
    "    \"\"\"\n",
    "    X_augmented = np.zeros_like(X_batch)\n",
    "    \n",
    "    for i in range(len(X_batch)):\n",
    "        # Only augment positive class (seizure) more aggressively\n",
    "        if y_batch[i] == 1:\n",
    "            prob = augmentation_prob * 1.5  # Higher probability for minority class\n",
    "        else:\n",
    "            prob = augmentation_prob\n",
    "        \n",
    "        X_augmented[i, :, 0] = augment_signal(X_batch[i, :, 0], prob)\n",
    "    \n",
    "    return X_augmented\n",
    "\n",
    "\n",
    "class DataAugmentationCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback to apply data augmentation during training\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, augmentation_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.X_train_original = X_train.copy()\n",
    "        self.y_train = y_train\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Apply augmentation at the start of each epoch\n",
    "        if epoch > 0:  # Skip first epoch to see baseline performance\n",
    "            X_augmented = augment_batch(\n",
    "                self.X_train_original, \n",
    "                self.y_train, \n",
    "                self.augmentation_prob\n",
    "            )\n",
    "            # Update model's training data\n",
    "            self.model.stop_training = False\n",
    "\n",
    "\n",
    "# ===================== CUSTOM DATA GENERATOR =====================\n",
    "class AugmentedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator with real-time augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, augmentation_prob=0.5, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.X))\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        \n",
    "        # Get batch data\n",
    "        X_batch = self.X[batch_indices].copy()\n",
    "        y_batch = self.y[batch_indices]\n",
    "        \n",
    "        # Apply augmentation\n",
    "        X_batch = augment_batch(X_batch, y_batch, self.augmentation_prob)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "# ===================== SE BLOCK IMPLEMENTATION =====================\n",
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block for channel attention\n",
    "    Paper: \"Interpretable classification of epileptic EEG signals...\"\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction=8, **kwargs):\n",
    "        super(SEBlock, self).__init__(**kwargs)\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.squeeze = GlobalAveragePooling1D()\n",
    "        \n",
    "        # Excitation network\n",
    "        self.fc1 = Dense(\n",
    "            channels // self.reduction, \n",
    "            activation='relu', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.fc2 = Dense(\n",
    "            channels, \n",
    "            activation='sigmoid', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        \n",
    "        super(SEBlock, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Squeeze: Global average pooling\n",
    "        squeeze = self.squeeze(inputs)\n",
    "        \n",
    "        # Excitation: Learn channel importance\n",
    "        excitation = self.fc1(squeeze)\n",
    "        excitation = self.fc2(excitation)\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        excitation = tf.reshape(excitation, [-1, 1, tf.shape(inputs)[-1]])\n",
    "        \n",
    "        # Scale: Multiply input with learned weights\n",
    "        return Multiply()([inputs, excitation])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SEBlock, self).get_config()\n",
    "        config.update({\"reduction\": self.reduction})\n",
    "        return config\n",
    "\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "X = np.load(r\"preprocessed\\ALL_X.npy\")\n",
    "y = np.load(r\"preprocessed\\ALL_y.npy\")\n",
    "\n",
    "# Convert to Binary Classification \n",
    "y_encoded = np.where(y == 'NORMAL', 0, 1)\n",
    "\n",
    "print(\"Original classes:\", np.unique(y))\n",
    "print(\"Binary classes: 0 (NORMAL) vs 1 (INTERICTAL + ICTAL)\")\n",
    "print(\"Binary labels distribution:\", np.unique(y_encoded, return_counts=True))\n",
    "\n",
    "# Prepare Data\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "# Compute Class Weights \n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# ===================== PREPARE CROSS VALIDATION =====================\n",
    "random_state = np.random.randint(0, 10000)\n",
    "print(f\"ğŸ² Random state used for this run: {random_state}\")\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "fold_indices = [(train_val_idx, test_idx) for train_val_idx, test_idx in kfold.split(X, y_encoded)]\n",
    "\n",
    "# Save indices for reproducibility\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "np.save(\"results/fold_indices.npy\", np.array(fold_indices, dtype=object), allow_pickle=True)\n",
    "\n",
    "\n",
    "# ===================== LOSS FUNCTIONS =====================\n",
    "def focal_loss(alpha=0.75, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        \n",
    "        return K.mean(alpha_t * focal_term * cross_entropy)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def hybrid_focal_loss(alpha=0.75, gamma=1.7, focal_weight=0.55):\n",
    "    \"\"\"Hybrid: Focal + BCE\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        bce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        \n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal = alpha_t * focal_term * bce\n",
    "        \n",
    "        combined = focal_weight * focal + (1 - focal_weight) * bce\n",
    "        return K.mean(combined)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# ===================== MODEL BUILDING =====================\n",
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    MODIFIED MODEL:\n",
    "    - Added 4th CNN block\n",
    "    - Changed Bidirectional LSTM to regular LSTM\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # ========== Block 1: Local patterns ==========\n",
    "    x = Conv1D(48, kernel_size=7, padding='same', kernel_regularizer=l2(0.002))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    \n",
    "    # ========== Block 2: Mid-level features ==========\n",
    "    x = Conv1D(96, kernel_size=5, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.35)(x)\n",
    "    \n",
    "    # ========== Block 3: High-level features ==========\n",
    "    x = Conv1D(128, kernel_size=3, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    \n",
    "    # ========== Block 4: Deep features (NEW LAYER) ==========\n",
    "    x = Conv1D(160, kernel_size=3, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    \n",
    "    # ========== LSTM Temporal Modeling (Changed from Bidirectional to regular LSTM) ==========\n",
    "    x = LSTM(64, return_sequences=False, kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # ========== Dense Classification ==========\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# ===================== TRAINING =====================\n",
    "# Choose which model to use\n",
    "\n",
    "USE_DATA_AUGMENTATION = True  \n",
    "\n",
    "acc_per_fold = []\n",
    "auc_per_fold = []\n",
    "conf_matrices = []\n",
    "class_names = ['NORMAL', 'ALL (INTERICTAL+ICTAL)']\n",
    "\n",
    "# Track metrics for best model selection\n",
    "fold_metrics = {\n",
    "    'fold_no': [],\n",
    "    'test_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_auc': [],\n",
    "    'val_acc': [],\n",
    "    'train_acc': [],\n",
    "    'f1_score': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "best_model = None\n",
    "best_fold = None\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "for fold_no, (train_val_idx, test_idx) in enumerate(fold_indices, start=1):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" FOLD {fold_no}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Split into train/val/test\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y_encoded[train_val_idx], y_encoded[test_idx]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.15, stratify=y_train_val, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    print(f\"Test set distribution: {np.unique(y_test, return_counts=True)}\")\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(input_shape=(X.shape[1], 1))\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=5e-5),\n",
    "        loss=hybrid_focal_loss(alpha=0.70, gamma=1.5, focal_weight=0.5),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=25, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1, \n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.6,\n",
    "        patience=8, \n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"results/model_fold{fold_no}.weights.h5\", \n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True,\n",
    "        verbose=0,  \n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    callbacks = [early_stop, reduce_lr, checkpoint]\n",
    "    \n",
    "    # Train Model\n",
    "    print(f\"\\nğŸš€ Training Fold {fold_no}...\")\n",
    "    \n",
    "    if USE_DATA_AUGMENTATION:\n",
    "        print(\"ğŸ“Š Using data augmentation with real-time generator\")\n",
    "        \n",
    "        # Create data generators\n",
    "        train_generator = AugmentedDataGenerator(\n",
    "            X_train, y_train, \n",
    "            batch_size=12, \n",
    "            augmentation_prob=0.55,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Note: Validation data is NOT augmented\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=150,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        print(\"ğŸ“Š Training without data augmentation\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=150,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_weights(f\"results/model_fold{fold_no}.weights.h5\")\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # AUC Score\n",
    "    test_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    auc_per_fold.append(test_auc)\n",
    "    \n",
    "    print(f\"\\nâœ… Fold {fold_no} Results:\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "    \n",
    "    # Classification Metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store metrics\n",
    "    fold_metrics['fold_no'].append(fold_no)\n",
    "    fold_metrics['test_acc'].append(test_acc)\n",
    "    fold_metrics['test_loss'].append(test_loss)\n",
    "    fold_metrics['test_auc'].append(test_auc)\n",
    "    fold_metrics['val_acc'].append(max(history.history['val_accuracy']))\n",
    "    fold_metrics['train_acc'].append(max(history.history['accuracy']))\n",
    "    fold_metrics['f1_score'].append(f1)\n",
    "    fold_metrics['precision'].append(precision)\n",
    "    fold_metrics['recall'].append(recall)\n",
    "    \n",
    "    # Check if this is the best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_fold = fold_no\n",
    "        best_model = model\n",
    "        best_cm = cm\n",
    "        best_y_test = y_test\n",
    "        best_y_pred = y_pred\n",
    "        best_y_pred_prob = y_pred_prob\n",
    "        print(f\"ğŸŒŸ New best model! Fold {fold_no} with accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_no} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                annot_kws={'size': 14})\n",
    "    plt.title(f\"Fold {fold_no} Confusion Matrix\\nAcc: {test_acc:.4f} | AUC: {test_auc:.4f}\", \n",
    "              fontsize=14)\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/confusion_fold{fold_no}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Training History \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    plt.axhline(y=test_acc, color='r', linestyle='--', label=f'Test Acc: {test_acc:.4f}')\n",
    "    plt.title(f'Fold {fold_no} - Model Accuracy', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Accuracy', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    plt.title(f'Fold {fold_no} - Model Loss', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Loss', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/training_history_fold{fold_no}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“Š Mean Test Accuracy across folds: {np.mean(acc_per_fold):.4f} (Â±{np.std(acc_per_fold):.4f})\")\n",
    "print(f\"ğŸ“Š Mean Test AUC across folds: {np.mean(auc_per_fold):.4f} (Â±{np.std(auc_per_fold):.4f})\")\n",
    "\n",
    "# Combine confusion matrices\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(total_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            annot_kws={'size': 14})\n",
    "plt.title(\"Overall Confusion Matrix (All Folds)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/confusion_overall.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Print Fold Metrics\n",
    "print(\"\\nğŸ“‹ Fold-wise Metrics Summary:\")\n",
    "for i in range(len(fold_metrics['fold_no'])):\n",
    "    print(f\"\\nğŸ”¸ Fold {fold_metrics['fold_no'][i]} Metrics:\")\n",
    "    print(f\"  Train Accuracy : {fold_metrics['train_acc'][i]:.4f}\")\n",
    "    print(f\"  Val Accuracy   : {fold_metrics['val_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Accuracy  : {fold_metrics['test_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Loss      : {fold_metrics['test_loss'][i]:.4f}\")\n",
    "    print(f\"  Precision      : {fold_metrics['precision'][i]:.4f}\")\n",
    "    print(f\"  Recall         : {fold_metrics['recall'][i]:.4f}\")\n",
    "    print(f\"  F1 Score       : {fold_metrics['f1_score'][i]:.4f}\")\n",
    "    print(f\"  Test AUC       : {fold_metrics['test_auc'][i]:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "best_model.save(\"results/best_model.keras\")\n",
    "print(f\"\\nğŸ’¾ Best model (Fold {best_fold}) saved as 'results/best_model.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6edc0466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes: ['ICTAL' 'INTERICTAL' 'NORMAL']\n",
      "Binary classes: 0 (NORMAL) vs 1 (INTERICTAL + ICTAL)\n",
      "Binary labels distribution: (array([0, 1]), array([4400, 6600], dtype=int64))\n",
      "Dataset shape: (11000, 347, 1)\n",
      "Class weights: {0: 1.25, 1: 0.8333333333333334}\n",
      "ğŸ² Random state used for this run: 358\n",
      "\n",
      "============================================================\n",
      " FOLD 1\n",
      "============================================================\n",
      "\n",
      "Train: 7480, Val: 1320, Test: 2200\n",
      "Test set distribution: (array([0, 1]), array([ 880, 1320], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—ï¸ Model Architecture: 5 CNN Blocks (48â†’96â†’128â†’160â†’192) + LSTM(64) + Dense(64â†’32)\n",
      "\n",
      "ğŸš€ Training Fold 1...\n",
      "ğŸ“Š Using enhanced data augmentation\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 46ms/step - accuracy: 0.4967 - loss: 1.5148 - val_accuracy: 0.6000 - val_loss: 1.4668 - learning_rate: 2.0000e-05\n",
      "Epoch 2/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6057 - loss: 1.4277 - val_accuracy: 0.6000 - val_loss: 1.3661 - learning_rate: 4.0000e-05\n",
      "Epoch 3/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.7254 - loss: 1.2895 - val_accuracy: 0.6636 - val_loss: 1.2664 - learning_rate: 6.0000e-05\n",
      "Epoch 4/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.8012 - loss: 1.1446 - val_accuracy: 0.7288 - val_loss: 1.1600 - learning_rate: 8.0000e-05\n",
      "Epoch 5/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.8389 - loss: 1.0099 - val_accuracy: 0.8205 - val_loss: 0.9538 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.8441 - loss: 0.8855 - val_accuracy: 0.8538 - val_loss: 0.8222 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.8675 - loss: 0.7751 - val_accuracy: 0.9030 - val_loss: 0.6884 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.8789 - loss: 0.6845 - val_accuracy: 0.9318 - val_loss: 0.5902 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 48ms/step - accuracy: 0.8905 - loss: 0.6084 - val_accuracy: 0.9371 - val_loss: 0.5206 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9032 - loss: 0.5462 - val_accuracy: 0.9455 - val_loss: 0.4622 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 44ms/step - accuracy: 0.9124 - loss: 0.4948 - val_accuracy: 0.9379 - val_loss: 0.4267 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9099 - loss: 0.4495 - val_accuracy: 0.9538 - val_loss: 0.3737 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9172 - loss: 0.4112 - val_accuracy: 0.9523 - val_loss: 0.3443 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9162 - loss: 0.3802 - val_accuracy: 0.9432 - val_loss: 0.3235 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9189 - loss: 0.3516 - val_accuracy: 0.9485 - val_loss: 0.2968 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9237 - loss: 0.3248 - val_accuracy: 0.9576 - val_loss: 0.2671 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9215 - loss: 0.3089 - val_accuracy: 0.9583 - val_loss: 0.2481 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9245 - loss: 0.2890 - val_accuracy: 0.9598 - val_loss: 0.2276 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9213 - loss: 0.2735 - val_accuracy: 0.9500 - val_loss: 0.2227 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9278 - loss: 0.2626 - val_accuracy: 0.9591 - val_loss: 0.2039 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9275 - loss: 0.2451 - val_accuracy: 0.9538 - val_loss: 0.2006 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9294 - loss: 0.2315 - val_accuracy: 0.9644 - val_loss: 0.1801 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 62ms/step - accuracy: 0.9290 - loss: 0.2212 - val_accuracy: 0.9621 - val_loss: 0.1692 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 64ms/step - accuracy: 0.9358 - loss: 0.2101 - val_accuracy: 0.9598 - val_loss: 0.1649 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9311 - loss: 0.2069 - val_accuracy: 0.9583 - val_loss: 0.1601 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9364 - loss: 0.1992 - val_accuracy: 0.9644 - val_loss: 0.1537 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 45ms/step - accuracy: 0.9336 - loss: 0.1909 - val_accuracy: 0.9492 - val_loss: 0.1539 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9361 - loss: 0.1855 - val_accuracy: 0.9652 - val_loss: 0.1345 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9362 - loss: 0.1808 - val_accuracy: 0.9689 - val_loss: 0.1305 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9373 - loss: 0.1757 - val_accuracy: 0.9523 - val_loss: 0.1346 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9410 - loss: 0.1674 - val_accuracy: 0.9705 - val_loss: 0.1203 - learning_rate: 7.0000e-05\n",
      "Epoch 32/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9401 - loss: 0.1645 - val_accuracy: 0.9667 - val_loss: 0.1190 - learning_rate: 4.9000e-05\n",
      "Epoch 33/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9452 - loss: 0.1605 - val_accuracy: 0.9606 - val_loss: 0.1213 - learning_rate: 3.4300e-05\n",
      "Epoch 34/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9426 - loss: 0.1555 - val_accuracy: 0.9659 - val_loss: 0.1178 - learning_rate: 2.4010e-05\n",
      "Epoch 35/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.9436 - loss: 0.1537 - val_accuracy: 0.9682 - val_loss: 0.1138 - learning_rate: 1.6807e-05\n",
      "Epoch 36/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9453 - loss: 0.1537 - val_accuracy: 0.9689 - val_loss: 0.1149 - learning_rate: 1.1765e-05\n",
      "Epoch 37/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9463 - loss: 0.1507 - val_accuracy: 0.9652 - val_loss: 0.1157 - learning_rate: 8.2354e-06\n",
      "Epoch 38/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.9451 - loss: 0.1521 - val_accuracy: 0.9621 - val_loss: 0.1167 - learning_rate: 5.7648e-06\n",
      "Epoch 39/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9448 - loss: 0.1508 - val_accuracy: 0.9667 - val_loss: 0.1143 - learning_rate: 4.0354e-06\n",
      "Epoch 40/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.9448 - loss: 0.1501 - val_accuracy: 0.9644 - val_loss: 0.1148 - learning_rate: 2.8248e-06\n",
      "Epoch 41/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.9468 - loss: 0.1523 - val_accuracy: 0.9667 - val_loss: 0.1134 - learning_rate: 1.9773e-06\n",
      "Epoch 42/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9481 - loss: 0.1501 - val_accuracy: 0.9652 - val_loss: 0.1140 - learning_rate: 1.3841e-06\n",
      "Epoch 43/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9484 - loss: 0.1449 - val_accuracy: 0.9652 - val_loss: 0.1137 - learning_rate: 9.6889e-07\n",
      "Epoch 44/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.9455 - loss: 0.1511 - val_accuracy: 0.9652 - val_loss: 0.1139 - learning_rate: 6.7822e-07\n",
      "Epoch 45/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9496 - loss: 0.1468 - val_accuracy: 0.9652 - val_loss: 0.1143 - learning_rate: 4.7476e-07\n",
      "Epoch 46/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9426 - loss: 0.1519 - val_accuracy: 0.9652 - val_loss: 0.1140 - learning_rate: 3.3233e-07\n",
      "Epoch 47/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.9429 - loss: 0.1526 - val_accuracy: 0.9629 - val_loss: 0.1147 - learning_rate: 2.3263e-07\n",
      "Epoch 48/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9487 - loss: 0.1459 - val_accuracy: 0.9652 - val_loss: 0.1133 - learning_rate: 1.6284e-07\n",
      "Epoch 49/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9455 - loss: 0.1510 - val_accuracy: 0.9652 - val_loss: 0.1139 - learning_rate: 1.1399e-07\n",
      "Epoch 50/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9453 - loss: 0.1477 - val_accuracy: 0.9652 - val_loss: 0.1140 - learning_rate: 1.0000e-07\n",
      "Epoch 51/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - accuracy: 0.9468 - loss: 0.1506 - val_accuracy: 0.9652 - val_loss: 0.1139 - learning_rate: 1.0000e-07\n",
      "Epoch 52/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9449 - loss: 0.1536 - val_accuracy: 0.9652 - val_loss: 0.1139 - learning_rate: 1.0000e-07\n",
      "Epoch 53/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - accuracy: 0.9434 - loss: 0.1502 - val_accuracy: 0.9652 - val_loss: 0.1141 - learning_rate: 1.0000e-07\n",
      "Epoch 54/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9444 - loss: 0.1538 - val_accuracy: 0.9674 - val_loss: 0.1128 - learning_rate: 1.0000e-07\n",
      "Epoch 55/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.9472 - loss: 0.1506 - val_accuracy: 0.9652 - val_loss: 0.1135 - learning_rate: 1.0000e-07\n",
      "Epoch 56/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9443 - loss: 0.1514 - val_accuracy: 0.9644 - val_loss: 0.1145 - learning_rate: 1.0000e-07\n",
      "Epoch 57/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9488 - loss: 0.1503 - val_accuracy: 0.9659 - val_loss: 0.1132 - learning_rate: 1.0000e-07\n",
      "Epoch 58/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9448 - loss: 0.1495 - val_accuracy: 0.9652 - val_loss: 0.1133 - learning_rate: 1.0000e-07\n",
      "Epoch 59/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9465 - loss: 0.1496 - val_accuracy: 0.9652 - val_loss: 0.1139 - learning_rate: 1.0000e-07\n",
      "Epoch 60/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 59ms/step - accuracy: 0.9496 - loss: 0.1490 - val_accuracy: 0.9652 - val_loss: 0.1138 - learning_rate: 1.0000e-07\n",
      "Epoch 61/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9471 - loss: 0.1470 - val_accuracy: 0.9644 - val_loss: 0.1145 - learning_rate: 1.0000e-07\n",
      "Epoch 62/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9444 - loss: 0.1491 - val_accuracy: 0.9659 - val_loss: 0.1131 - learning_rate: 1.0000e-07\n",
      "Epoch 63/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9440 - loss: 0.1550 - val_accuracy: 0.9652 - val_loss: 0.1133 - learning_rate: 1.0000e-07\n",
      "Epoch 64/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9457 - loss: 0.1521 - val_accuracy: 0.9652 - val_loss: 0.1136 - learning_rate: 1.0000e-07\n",
      "Epoch 65/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9469 - loss: 0.1536 - val_accuracy: 0.9652 - val_loss: 0.1132 - learning_rate: 1.0000e-07\n",
      "Epoch 66/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9491 - loss: 0.1440 - val_accuracy: 0.9652 - val_loss: 0.1141 - learning_rate: 1.0000e-07\n",
      "Epoch 67/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9473 - loss: 0.1491 - val_accuracy: 0.9652 - val_loss: 0.1135 - learning_rate: 1.0000e-07\n",
      "Epoch 68/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 62ms/step - accuracy: 0.9472 - loss: 0.1493 - val_accuracy: 0.9652 - val_loss: 0.1137 - learning_rate: 1.0000e-07\n",
      "Epoch 69/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 59ms/step - accuracy: 0.9448 - loss: 0.1498 - val_accuracy: 0.9652 - val_loss: 0.1137 - learning_rate: 1.0000e-07\n",
      "Epoch 70/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.9440 - loss: 0.1518 - val_accuracy: 0.9652 - val_loss: 0.1142 - learning_rate: 1.0000e-07\n",
      "Epoch 71/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9459 - loss: 0.1500 - val_accuracy: 0.9652 - val_loss: 0.1134 - learning_rate: 1.0000e-07\n",
      "Epoch 72/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9465 - loss: 0.1528 - val_accuracy: 0.9667 - val_loss: 0.1130 - learning_rate: 1.0000e-07\n",
      "Epoch 73/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.9469 - loss: 0.1486 - val_accuracy: 0.9667 - val_loss: 0.1131 - learning_rate: 1.0000e-07\n",
      "Epoch 74/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9476 - loss: 0.1509 - val_accuracy: 0.9652 - val_loss: 0.1134 - learning_rate: 1.0000e-07\n",
      "Epoch 75/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.9418 - loss: 0.1529 - val_accuracy: 0.9652 - val_loss: 0.1136 - learning_rate: 1.0000e-07\n",
      "Epoch 76/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 62ms/step - accuracy: 0.9461 - loss: 0.1496 - val_accuracy: 0.9652 - val_loss: 0.1138 - learning_rate: 1.0000e-07\n",
      "Epoch 77/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9460 - loss: 0.1512 - val_accuracy: 0.9652 - val_loss: 0.1135 - learning_rate: 1.0000e-07\n",
      "Epoch 78/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9472 - loss: 0.1530 - val_accuracy: 0.9652 - val_loss: 0.1129 - learning_rate: 1.0000e-07\n",
      "Epoch 79/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9460 - loss: 0.1504 - val_accuracy: 0.9652 - val_loss: 0.1133 - learning_rate: 1.0000e-07\n",
      "Epoch 79: early stopping\n",
      "Restoring model weights from the end of the best epoch: 54.\n",
      "\n",
      "âœ… Fold 1 Results:\n",
      "  Test Accuracy: 0.9527\n",
      "  Test AUC: 0.9934\n",
      "  Test Loss: 0.1340\n",
      "ğŸŒŸ New best model! Fold 1 with accuracy: 0.9527\n",
      "\n",
      "Fold 1 Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                NORMAL     0.9652    0.9148    0.9393       880\n",
      "ALL (INTERICTAL+ICTAL)     0.9451    0.9780    0.9613      1320\n",
      "\n",
      "              accuracy                         0.9527      2200\n",
      "             macro avg     0.9552    0.9464    0.9503      2200\n",
      "          weighted avg     0.9531    0.9527    0.9525      2200\n",
      "\n",
      "\n",
      "============================================================\n",
      " FOLD 2\n",
      "============================================================\n",
      "\n",
      "Train: 7480, Val: 1320, Test: 2200\n",
      "Test set distribution: (array([0, 1]), array([ 880, 1320], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—ï¸ Model Architecture: 5 CNN Blocks (48â†’96â†’128â†’160â†’192) + LSTM(64) + Dense(64â†’32)\n",
      "\n",
      "ğŸš€ Training Fold 2...\n",
      "ğŸ“Š Using enhanced data augmentation\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 47ms/step - accuracy: 0.5483 - loss: 1.5063 - val_accuracy: 0.6000 - val_loss: 1.4547 - learning_rate: 2.0000e-05\n",
      "Epoch 2/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.6120 - loss: 1.4355 - val_accuracy: 0.6114 - val_loss: 1.3488 - learning_rate: 4.0000e-05\n",
      "Epoch 3/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.7203 - loss: 1.3132 - val_accuracy: 0.7515 - val_loss: 1.2436 - learning_rate: 6.0000e-05\n",
      "Epoch 4/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.8008 - loss: 1.1816 - val_accuracy: 0.8129 - val_loss: 1.1197 - learning_rate: 8.0000e-05\n",
      "Epoch 5/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.8341 - loss: 1.0568 - val_accuracy: 0.8462 - val_loss: 0.9657 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.8553 - loss: 0.9308 - val_accuracy: 0.9053 - val_loss: 0.8192 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.8755 - loss: 0.8199 - val_accuracy: 0.9167 - val_loss: 0.7148 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.8905 - loss: 0.7270 - val_accuracy: 0.9227 - val_loss: 0.6354 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.8988 - loss: 0.6478 - val_accuracy: 0.9341 - val_loss: 0.5638 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.9084 - loss: 0.5753 - val_accuracy: 0.9333 - val_loss: 0.5042 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.9083 - loss: 0.5275 - val_accuracy: 0.9447 - val_loss: 0.4526 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.9146 - loss: 0.4810 - val_accuracy: 0.9364 - val_loss: 0.4126 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9172 - loss: 0.4406 - val_accuracy: 0.9288 - val_loss: 0.3813 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9170 - loss: 0.4047 - val_accuracy: 0.9477 - val_loss: 0.3392 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9221 - loss: 0.3731 - val_accuracy: 0.9447 - val_loss: 0.3135 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9278 - loss: 0.3436 - val_accuracy: 0.9371 - val_loss: 0.2932 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.9271 - loss: 0.3229 - val_accuracy: 0.9477 - val_loss: 0.2666 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 76ms/step - accuracy: 0.9279 - loss: 0.3023 - val_accuracy: 0.9447 - val_loss: 0.2527 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 60ms/step - accuracy: 0.9239 - loss: 0.2879 - val_accuracy: 0.9538 - val_loss: 0.2328 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.9305 - loss: 0.2681 - val_accuracy: 0.9462 - val_loss: 0.2213 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.9326 - loss: 0.2535 - val_accuracy: 0.9242 - val_loss: 0.2291 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 72ms/step - accuracy: 0.9322 - loss: 0.2451 - val_accuracy: 0.9447 - val_loss: 0.1986 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 72ms/step - accuracy: 0.9286 - loss: 0.2347 - val_accuracy: 0.9455 - val_loss: 0.1897 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 76ms/step - accuracy: 0.9342 - loss: 0.2196 - val_accuracy: 0.9652 - val_loss: 0.1725 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9330 - loss: 0.2125 - val_accuracy: 0.9424 - val_loss: 0.1746 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9369 - loss: 0.2052 - val_accuracy: 0.9523 - val_loss: 0.1613 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 65ms/step - accuracy: 0.9368 - loss: 0.1922 - val_accuracy: 0.9545 - val_loss: 0.1524 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9400 - loss: 0.1876 - val_accuracy: 0.9553 - val_loss: 0.1502 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.9357 - loss: 0.1803 - val_accuracy: 0.9553 - val_loss: 0.1499 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.9376 - loss: 0.1783 - val_accuracy: 0.9598 - val_loss: 0.1400 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9385 - loss: 0.1715 - val_accuracy: 0.9568 - val_loss: 0.1340 - learning_rate: 7.0000e-05\n",
      "Epoch 32/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.9409 - loss: 0.1678 - val_accuracy: 0.9576 - val_loss: 0.1299 - learning_rate: 4.9000e-05\n",
      "Epoch 33/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.9439 - loss: 0.1605 - val_accuracy: 0.9636 - val_loss: 0.1259 - learning_rate: 3.4300e-05\n",
      "Epoch 34/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.9445 - loss: 0.1586 - val_accuracy: 0.9614 - val_loss: 0.1249 - learning_rate: 2.4010e-05\n",
      "Epoch 35/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9492 - loss: 0.1530 - val_accuracy: 0.9591 - val_loss: 0.1270 - learning_rate: 1.6807e-05\n",
      "Epoch 36/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9468 - loss: 0.1556 - val_accuracy: 0.9606 - val_loss: 0.1228 - learning_rate: 1.1765e-05\n",
      "Epoch 37/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 63ms/step - accuracy: 0.9453 - loss: 0.1550 - val_accuracy: 0.9636 - val_loss: 0.1216 - learning_rate: 8.2354e-06\n",
      "Epoch 38/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9489 - loss: 0.1507 - val_accuracy: 0.9614 - val_loss: 0.1220 - learning_rate: 5.7648e-06\n",
      "Epoch 39/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.9485 - loss: 0.1542 - val_accuracy: 0.9606 - val_loss: 0.1218 - learning_rate: 4.0354e-06\n",
      "Epoch 40/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9468 - loss: 0.1498 - val_accuracy: 0.9621 - val_loss: 0.1213 - learning_rate: 2.8248e-06\n",
      "Epoch 41/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9491 - loss: 0.1492 - val_accuracy: 0.9614 - val_loss: 0.1213 - learning_rate: 1.9773e-06\n",
      "Epoch 42/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9484 - loss: 0.1503 - val_accuracy: 0.9636 - val_loss: 0.1204 - learning_rate: 1.3841e-06\n",
      "Epoch 43/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9469 - loss: 0.1518 - val_accuracy: 0.9636 - val_loss: 0.1206 - learning_rate: 9.6889e-07\n",
      "Epoch 44/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9487 - loss: 0.1532 - val_accuracy: 0.9644 - val_loss: 0.1208 - learning_rate: 6.7822e-07\n",
      "Epoch 45/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9465 - loss: 0.1533 - val_accuracy: 0.9606 - val_loss: 0.1214 - learning_rate: 4.7476e-07\n",
      "Epoch 46/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.9483 - loss: 0.1534 - val_accuracy: 0.9629 - val_loss: 0.1213 - learning_rate: 3.3233e-07\n",
      "Epoch 47/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9493 - loss: 0.1524 - val_accuracy: 0.9636 - val_loss: 0.1201 - learning_rate: 2.3263e-07\n",
      "Epoch 48/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.9520 - loss: 0.1475 - val_accuracy: 0.9629 - val_loss: 0.1207 - learning_rate: 1.6284e-07\n",
      "Epoch 49/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9512 - loss: 0.1512 - val_accuracy: 0.9644 - val_loss: 0.1203 - learning_rate: 1.1399e-07\n",
      "Epoch 50/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9509 - loss: 0.1513 - val_accuracy: 0.9644 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 51/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9488 - loss: 0.1494 - val_accuracy: 0.9636 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 52/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9475 - loss: 0.1537 - val_accuracy: 0.9644 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 53/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9497 - loss: 0.1486 - val_accuracy: 0.9636 - val_loss: 0.1201 - learning_rate: 1.0000e-07\n",
      "Epoch 54/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.9515 - loss: 0.1518 - val_accuracy: 0.9636 - val_loss: 0.1204 - learning_rate: 1.0000e-07\n",
      "Epoch 55/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9472 - loss: 0.1522 - val_accuracy: 0.9644 - val_loss: 0.1203 - learning_rate: 1.0000e-07\n",
      "Epoch 56/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9504 - loss: 0.1480 - val_accuracy: 0.9636 - val_loss: 0.1203 - learning_rate: 1.0000e-07\n",
      "Epoch 57/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.9511 - loss: 0.1501 - val_accuracy: 0.9644 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 58/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9489 - loss: 0.1500 - val_accuracy: 0.9614 - val_loss: 0.1208 - learning_rate: 1.0000e-07\n",
      "Epoch 59/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9489 - loss: 0.1506 - val_accuracy: 0.9621 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 60/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9507 - loss: 0.1494 - val_accuracy: 0.9636 - val_loss: 0.1207 - learning_rate: 1.0000e-07\n",
      "Epoch 61/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9499 - loss: 0.1489 - val_accuracy: 0.9621 - val_loss: 0.1210 - learning_rate: 1.0000e-07\n",
      "Epoch 62/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9488 - loss: 0.1518 - val_accuracy: 0.9621 - val_loss: 0.1207 - learning_rate: 1.0000e-07\n",
      "Epoch 63/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9477 - loss: 0.1538 - val_accuracy: 0.9644 - val_loss: 0.1207 - learning_rate: 1.0000e-07\n",
      "Epoch 64/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9471 - loss: 0.1525 - val_accuracy: 0.9636 - val_loss: 0.1209 - learning_rate: 1.0000e-07\n",
      "Epoch 65/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9476 - loss: 0.1507 - val_accuracy: 0.9644 - val_loss: 0.1205 - learning_rate: 1.0000e-07\n",
      "Epoch 66/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9525 - loss: 0.1483 - val_accuracy: 0.9636 - val_loss: 0.1204 - learning_rate: 1.0000e-07\n",
      "Epoch 67/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9473 - loss: 0.1519 - val_accuracy: 0.9606 - val_loss: 0.1212 - learning_rate: 1.0000e-07\n",
      "Epoch 68/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9471 - loss: 0.1541 - val_accuracy: 0.9636 - val_loss: 0.1200 - learning_rate: 1.0000e-07\n",
      "Epoch 69/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9495 - loss: 0.1498 - val_accuracy: 0.9606 - val_loss: 0.1212 - learning_rate: 1.0000e-07\n",
      "Epoch 70/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9489 - loss: 0.1501 - val_accuracy: 0.9614 - val_loss: 0.1210 - learning_rate: 1.0000e-07\n",
      "Epoch 71/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.9487 - loss: 0.1490 - val_accuracy: 0.9636 - val_loss: 0.1205 - learning_rate: 1.0000e-07\n",
      "Epoch 72/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9497 - loss: 0.1515 - val_accuracy: 0.9606 - val_loss: 0.1213 - learning_rate: 1.0000e-07\n",
      "Epoch 73/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9473 - loss: 0.1516 - val_accuracy: 0.9636 - val_loss: 0.1204 - learning_rate: 1.0000e-07\n",
      "Epoch 74/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9519 - loss: 0.1476 - val_accuracy: 0.9614 - val_loss: 0.1213 - learning_rate: 1.0000e-07\n",
      "Epoch 75/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - accuracy: 0.9484 - loss: 0.1524 - val_accuracy: 0.9636 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 76/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - accuracy: 0.9437 - loss: 0.1571 - val_accuracy: 0.9644 - val_loss: 0.1202 - learning_rate: 1.0000e-07\n",
      "Epoch 77/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9449 - loss: 0.1565 - val_accuracy: 0.9636 - val_loss: 0.1201 - learning_rate: 1.0000e-07\n",
      "Epoch 78/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9487 - loss: 0.1513 - val_accuracy: 0.9644 - val_loss: 0.1199 - learning_rate: 1.0000e-07\n",
      "Epoch 79/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 55ms/step - accuracy: 0.9503 - loss: 0.1487 - val_accuracy: 0.9644 - val_loss: 0.1204 - learning_rate: 1.0000e-07\n",
      "Epoch 80/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 57ms/step - accuracy: 0.9461 - loss: 0.1519 - val_accuracy: 0.9621 - val_loss: 0.1208 - learning_rate: 1.0000e-07\n",
      "Epoch 81/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9467 - loss: 0.1502 - val_accuracy: 0.9636 - val_loss: 0.1203 - learning_rate: 1.0000e-07\n",
      "Epoch 82/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9471 - loss: 0.1522 - val_accuracy: 0.9606 - val_loss: 0.1212 - learning_rate: 1.0000e-07\n",
      "Epoch 83/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - accuracy: 0.9503 - loss: 0.1488 - val_accuracy: 0.9644 - val_loss: 0.1204 - learning_rate: 1.0000e-07\n",
      "Epoch 84/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9501 - loss: 0.1487 - val_accuracy: 0.9621 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 85/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9489 - loss: 0.1510 - val_accuracy: 0.9621 - val_loss: 0.1205 - learning_rate: 1.0000e-07\n",
      "Epoch 86/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.9469 - loss: 0.1509 - val_accuracy: 0.9621 - val_loss: 0.1204 - learning_rate: 1.0000e-07\n",
      "Epoch 87/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.9487 - loss: 0.1495 - val_accuracy: 0.9644 - val_loss: 0.1200 - learning_rate: 1.0000e-07\n",
      "Epoch 88/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9505 - loss: 0.1524 - val_accuracy: 0.9606 - val_loss: 0.1211 - learning_rate: 1.0000e-07\n",
      "Epoch 89/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9539 - loss: 0.1481 - val_accuracy: 0.9644 - val_loss: 0.1203 - learning_rate: 1.0000e-07\n",
      "Epoch 90/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9505 - loss: 0.1522 - val_accuracy: 0.9621 - val_loss: 0.1204 - learning_rate: 1.0000e-07\n",
      "Epoch 91/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 59ms/step - accuracy: 0.9476 - loss: 0.1534 - val_accuracy: 0.9636 - val_loss: 0.1202 - learning_rate: 1.0000e-07\n",
      "Epoch 92/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9493 - loss: 0.1535 - val_accuracy: 0.9629 - val_loss: 0.1205 - learning_rate: 1.0000e-07\n",
      "Epoch 93/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 59ms/step - accuracy: 0.9469 - loss: 0.1515 - val_accuracy: 0.9621 - val_loss: 0.1204 - learning_rate: 1.0000e-07\n",
      "Epoch 94/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9487 - loss: 0.1521 - val_accuracy: 0.9644 - val_loss: 0.1201 - learning_rate: 1.0000e-07\n",
      "Epoch 95/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9479 - loss: 0.1505 - val_accuracy: 0.9644 - val_loss: 0.1197 - learning_rate: 1.0000e-07\n",
      "Epoch 96/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.9496 - loss: 0.1489 - val_accuracy: 0.9614 - val_loss: 0.1210 - learning_rate: 1.0000e-07\n",
      "Epoch 97/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 61ms/step - accuracy: 0.9495 - loss: 0.1520 - val_accuracy: 0.9621 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 98/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 62ms/step - accuracy: 0.9507 - loss: 0.1483 - val_accuracy: 0.9614 - val_loss: 0.1207 - learning_rate: 1.0000e-07\n",
      "Epoch 99/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.9511 - loss: 0.1497 - val_accuracy: 0.9614 - val_loss: 0.1210 - learning_rate: 1.0000e-07\n",
      "Epoch 100/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 71ms/step - accuracy: 0.9515 - loss: 0.1480 - val_accuracy: 0.9621 - val_loss: 0.1205 - learning_rate: 1.0000e-07\n",
      "Epoch 101/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 65ms/step - accuracy: 0.9481 - loss: 0.1498 - val_accuracy: 0.9644 - val_loss: 0.1200 - learning_rate: 1.0000e-07\n",
      "Epoch 102/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 62ms/step - accuracy: 0.9496 - loss: 0.1507 - val_accuracy: 0.9614 - val_loss: 0.1208 - learning_rate: 1.0000e-07\n",
      "Epoch 103/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 63ms/step - accuracy: 0.9516 - loss: 0.1501 - val_accuracy: 0.9644 - val_loss: 0.1201 - learning_rate: 1.0000e-07\n",
      "Epoch 104/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.9488 - loss: 0.1527 - val_accuracy: 0.9644 - val_loss: 0.1201 - learning_rate: 1.0000e-07\n",
      "Epoch 105/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 63ms/step - accuracy: 0.9496 - loss: 0.1510 - val_accuracy: 0.9636 - val_loss: 0.1203 - learning_rate: 1.0000e-07\n",
      "Epoch 106/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 62ms/step - accuracy: 0.9484 - loss: 0.1534 - val_accuracy: 0.9606 - val_loss: 0.1212 - learning_rate: 1.0000e-07\n",
      "Epoch 107/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 63ms/step - accuracy: 0.9473 - loss: 0.1490 - val_accuracy: 0.9606 - val_loss: 0.1210 - learning_rate: 1.0000e-07\n",
      "Epoch 108/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 68ms/step - accuracy: 0.9521 - loss: 0.1473 - val_accuracy: 0.9621 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 109/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 64ms/step - accuracy: 0.9455 - loss: 0.1503 - val_accuracy: 0.9636 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 110/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 64ms/step - accuracy: 0.9539 - loss: 0.1418 - val_accuracy: 0.9629 - val_loss: 0.1205 - learning_rate: 1.0000e-07\n",
      "Epoch 111/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 63ms/step - accuracy: 0.9467 - loss: 0.1513 - val_accuracy: 0.9606 - val_loss: 0.1211 - learning_rate: 1.0000e-07\n",
      "Epoch 112/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 64ms/step - accuracy: 0.9473 - loss: 0.1546 - val_accuracy: 0.9644 - val_loss: 0.1203 - learning_rate: 1.0000e-07\n",
      "Epoch 113/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 64ms/step - accuracy: 0.9473 - loss: 0.1494 - val_accuracy: 0.9636 - val_loss: 0.1200 - learning_rate: 1.0000e-07\n",
      "Epoch 114/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 65ms/step - accuracy: 0.9493 - loss: 0.1503 - val_accuracy: 0.9614 - val_loss: 0.1206 - learning_rate: 1.0000e-07\n",
      "Epoch 115/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 65ms/step - accuracy: 0.9484 - loss: 0.1540 - val_accuracy: 0.9614 - val_loss: 0.1207 - learning_rate: 1.0000e-07\n",
      "Epoch 116/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 66ms/step - accuracy: 0.9488 - loss: 0.1501 - val_accuracy: 0.9644 - val_loss: 0.1202 - learning_rate: 1.0000e-07\n",
      "Epoch 117/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 65ms/step - accuracy: 0.9492 - loss: 0.1507 - val_accuracy: 0.9644 - val_loss: 0.1203 - learning_rate: 1.0000e-07\n",
      "Epoch 118/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 66ms/step - accuracy: 0.9487 - loss: 0.1516 - val_accuracy: 0.9621 - val_loss: 0.1205 - learning_rate: 1.0000e-07\n",
      "Epoch 119/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 65ms/step - accuracy: 0.9463 - loss: 0.1520 - val_accuracy: 0.9636 - val_loss: 0.1205 - learning_rate: 1.0000e-07\n",
      "Epoch 120/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 66ms/step - accuracy: 0.9485 - loss: 0.1500 - val_accuracy: 0.9644 - val_loss: 0.1202 - learning_rate: 1.0000e-07\n",
      "Epoch 120: early stopping\n",
      "Restoring model weights from the end of the best epoch: 95.\n",
      "\n",
      "âœ… Fold 2 Results:\n",
      "  Test Accuracy: 0.9491\n",
      "  Test AUC: 0.9927\n",
      "  Test Loss: 0.1829\n",
      "\n",
      "Fold 2 Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                NORMAL     0.9103    0.9682    0.9383       880\n",
      "ALL (INTERICTAL+ICTAL)     0.9778    0.9364    0.9567      1320\n",
      "\n",
      "              accuracy                         0.9491      2200\n",
      "             macro avg     0.9441    0.9523    0.9475      2200\n",
      "          weighted avg     0.9508    0.9491    0.9493      2200\n",
      "\n",
      "\n",
      "============================================================\n",
      " FOLD 3\n",
      "============================================================\n",
      "\n",
      "Train: 7480, Val: 1320, Test: 2200\n",
      "Test set distribution: (array([0, 1]), array([ 880, 1320], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—ï¸ Model Architecture: 5 CNN Blocks (48â†’96â†’128â†’160â†’192) + LSTM(64) + Dense(64â†’32)\n",
      "\n",
      "ğŸš€ Training Fold 3...\n",
      "ğŸ“Š Using enhanced data augmentation\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 47ms/step - accuracy: 0.5509 - loss: 1.4976 - val_accuracy: 0.6000 - val_loss: 1.4523 - learning_rate: 2.0000e-05\n",
      "Epoch 2/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.6460 - loss: 1.4005 - val_accuracy: 0.6455 - val_loss: 1.3285 - learning_rate: 4.0000e-05\n",
      "Epoch 3/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.7385 - loss: 1.2611 - val_accuracy: 0.7083 - val_loss: 1.2241 - learning_rate: 6.0000e-05\n",
      "Epoch 4/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.7921 - loss: 1.1211 - val_accuracy: 0.7773 - val_loss: 1.0616 - learning_rate: 8.0000e-05\n",
      "Epoch 5/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.8334 - loss: 0.9710 - val_accuracy: 0.8417 - val_loss: 0.8987 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.8473 - loss: 0.8500 - val_accuracy: 0.8644 - val_loss: 0.7727 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.8660 - loss: 0.7413 - val_accuracy: 0.8356 - val_loss: 0.6929 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.8807 - loss: 0.6584 - val_accuracy: 0.9242 - val_loss: 0.5670 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.9007 - loss: 0.5764 - val_accuracy: 0.9076 - val_loss: 0.5205 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.9036 - loss: 0.5198 - val_accuracy: 0.9515 - val_loss: 0.4414 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9096 - loss: 0.4710 - val_accuracy: 0.9455 - val_loss: 0.4047 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9135 - loss: 0.4320 - val_accuracy: 0.9439 - val_loss: 0.3687 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9138 - loss: 0.3960 - val_accuracy: 0.9212 - val_loss: 0.3527 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 48ms/step - accuracy: 0.9222 - loss: 0.3681 - val_accuracy: 0.9477 - val_loss: 0.3088 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9233 - loss: 0.3424 - val_accuracy: 0.9545 - val_loss: 0.2832 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9231 - loss: 0.3179 - val_accuracy: 0.9386 - val_loss: 0.2749 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9235 - loss: 0.2997 - val_accuracy: 0.9455 - val_loss: 0.2542 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9303 - loss: 0.2812 - val_accuracy: 0.9447 - val_loss: 0.2398 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9286 - loss: 0.2623 - val_accuracy: 0.9462 - val_loss: 0.2240 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9271 - loss: 0.2514 - val_accuracy: 0.9462 - val_loss: 0.2158 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9278 - loss: 0.2440 - val_accuracy: 0.9553 - val_loss: 0.1971 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9337 - loss: 0.2289 - val_accuracy: 0.9508 - val_loss: 0.1905 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9344 - loss: 0.2201 - val_accuracy: 0.9583 - val_loss: 0.1813 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9333 - loss: 0.2118 - val_accuracy: 0.9470 - val_loss: 0.1805 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9330 - loss: 0.2038 - val_accuracy: 0.9561 - val_loss: 0.1660 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9365 - loss: 0.1930 - val_accuracy: 0.9591 - val_loss: 0.1596 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9337 - loss: 0.1901 - val_accuracy: 0.9545 - val_loss: 0.1540 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9360 - loss: 0.1814 - val_accuracy: 0.9455 - val_loss: 0.1552 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9382 - loss: 0.1753 - val_accuracy: 0.9561 - val_loss: 0.1443 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9393 - loss: 0.1700 - val_accuracy: 0.9576 - val_loss: 0.1379 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9370 - loss: 0.1693 - val_accuracy: 0.9553 - val_loss: 0.1352 - learning_rate: 7.0000e-05\n",
      "Epoch 32/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 0.9418 - loss: 0.1624 - val_accuracy: 0.9545 - val_loss: 0.1365 - learning_rate: 4.9000e-05\n",
      "Epoch 33/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9444 - loss: 0.1596 - val_accuracy: 0.9606 - val_loss: 0.1301 - learning_rate: 3.4300e-05\n",
      "Epoch 34/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.9439 - loss: 0.1549 - val_accuracy: 0.9606 - val_loss: 0.1296 - learning_rate: 2.4010e-05\n",
      "Epoch 35/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9477 - loss: 0.1489 - val_accuracy: 0.9583 - val_loss: 0.1304 - learning_rate: 1.6807e-05\n",
      "Epoch 36/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 69ms/step - accuracy: 0.9437 - loss: 0.1497 - val_accuracy: 0.9621 - val_loss: 0.1277 - learning_rate: 1.1765e-05\n",
      "Epoch 37/150\n",
      "\u001b[1m468/468\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35764s\u001b[0m 77s/step - accuracy: 0.9461 - loss: 0.1512 - val_accuracy: 0.9606 - val_loss: 0.1268 - learning_rate: 8.2354e-06\n",
      "Epoch 38/150\n",
      "\u001b[1m119/468\u001b[0m \u001b[32mâ”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m27s\u001b[0m 79ms/step - accuracy: 0.9489 - loss: 0.1520"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 392\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“Š Using enhanced data augmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    385\u001b[0m     train_generator \u001b[38;5;241m=\u001b[39m AugmentedDataGenerator(\n\u001b[0;32m    386\u001b[0m         X_train, y_train, \n\u001b[0;32m    387\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m    388\u001b[0m         augmentation_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m,\n\u001b[0;32m    389\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     )\n\u001b[1;32m--> 392\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    393\u001b[0m         train_generator,\n\u001b[0;32m    394\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[0;32m    395\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m    396\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    397\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39mclass_weight_dict,\n\u001b[0;32m    398\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    399\u001b[0m     )\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    402\u001b[0m         X_train, y_train,\n\u001b[0;32m    403\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    409\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1689\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1690\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1691\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1692\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1693\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1694\u001b[0m   )\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, \n",
    "    GlobalAveragePooling1D, Input, Activation, SpatialDropout1D, \n",
    "    LSTM, Bidirectional, Multiply, Reshape, LeakyReLU, Layer, MultiHeadAttention\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# ===================== ENHANCED DATA AUGMENTATION =====================\n",
    "def augment_signal(segment, augmentation_prob=0.5):\n",
    "    \"\"\"Apply random augmentation to EEG segment with MORE augmentation types\"\"\"\n",
    "    if np.random.random() > augmentation_prob:\n",
    "        return segment\n",
    "    \n",
    "    aug_type = np.random.choice(\n",
    "        ['noise', 'scale', 'shift', 'time_shift', 'smooth', 'spike'], \n",
    "        p=[0.25, 0.25, 0.15, 0.15, 0.1, 0.1]\n",
    "    )\n",
    "    \n",
    "    if aug_type == 'noise':\n",
    "        noise_level = np.random.uniform(0.01, 0.05)\n",
    "        noise = np.random.normal(0, noise_level, segment.shape)\n",
    "        return segment + noise\n",
    "    \n",
    "    elif aug_type == 'scale':\n",
    "        scale = np.random.uniform(0.85, 1.15)\n",
    "        return segment * scale\n",
    "    \n",
    "    elif aug_type == 'shift':\n",
    "        shift = np.random.uniform(-0.15, 0.15)\n",
    "        return segment + shift\n",
    "    \n",
    "    elif aug_type == 'time_shift':\n",
    "        shift_amount = np.random.randint(-30, 30)\n",
    "        return np.roll(segment, shift_amount)\n",
    "    \n",
    "    elif aug_type == 'smooth':\n",
    "        window = 3\n",
    "        return np.convolve(segment, np.ones(window)/window, mode='same')\n",
    "    \n",
    "    elif aug_type == 'spike':\n",
    "        spike_pos = np.random.randint(10, len(segment)-10)\n",
    "        spike_mag = np.random.uniform(0.1, 0.3)\n",
    "        segment_copy = segment.copy()\n",
    "        segment_copy[spike_pos] += spike_mag\n",
    "        return segment_copy\n",
    "    \n",
    "    return segment\n",
    "\n",
    "\n",
    "def augment_batch(X_batch, y_batch, augmentation_prob=0.5):\n",
    "    \"\"\"Enhanced augmentation with higher probability for minority class\"\"\"\n",
    "    X_augmented = np.zeros_like(X_batch)\n",
    "    \n",
    "    for i in range(len(X_batch)):\n",
    "        if y_batch[i] == 1:\n",
    "            prob = augmentation_prob * 1.8\n",
    "        else:\n",
    "            prob = augmentation_prob\n",
    "        \n",
    "        X_augmented[i, :, 0] = augment_signal(X_batch[i, :, 0], prob)\n",
    "    \n",
    "    return X_augmented\n",
    "\n",
    "\n",
    "class AugmentedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Custom data generator with real-time augmentation\"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, augmentation_prob=0.5, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.X))\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        \n",
    "        X_batch = self.X[batch_indices].copy()\n",
    "        y_batch = self.y[batch_indices]\n",
    "        \n",
    "        X_batch = augment_batch(X_batch, y_batch, self.augmentation_prob)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "# ===================== SE BLOCK IMPLEMENTATION =====================\n",
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Squeeze-and-Excitation Block for channel attention\"\"\"\n",
    "    def __init__(self, reduction=8, **kwargs):\n",
    "        super(SEBlock, self).__init__(**kwargs)\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.squeeze = GlobalAveragePooling1D()\n",
    "        \n",
    "        self.fc1 = Dense(\n",
    "            channels // self.reduction, \n",
    "            activation='relu', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.fc2 = Dense(\n",
    "            channels, \n",
    "            activation='sigmoid', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        \n",
    "        super(SEBlock, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        squeeze = self.squeeze(inputs)\n",
    "        excitation = self.fc1(squeeze)\n",
    "        excitation = self.fc2(excitation)\n",
    "        excitation = tf.reshape(excitation, [-1, 1, tf.shape(inputs)[-1]])\n",
    "        return Multiply()([inputs, excitation])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SEBlock, self).get_config()\n",
    "        config.update({\"reduction\": self.reduction})\n",
    "        return config\n",
    "\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "X = np.load(r\"preprocessed\\ALL_X.npy\")\n",
    "y = np.load(r\"preprocessed\\ALL_y.npy\")\n",
    "\n",
    "y_encoded = np.where(y == 'NORMAL', 0, 1)\n",
    "\n",
    "print(\"Original classes:\", np.unique(y))\n",
    "print(\"Binary classes: 0 (NORMAL) vs 1 (INTERICTAL + ICTAL)\")\n",
    "print(\"Binary labels distribution:\", np.unique(y_encoded, return_counts=True))\n",
    "\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# ===================== PREPARE CROSS VALIDATION =====================\n",
    "random_state = np.random.randint(0, 10000)\n",
    "print(f\"ğŸ² Random state used for this run: {random_state}\")\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "fold_indices = [(train_val_idx, test_idx) for train_val_idx, test_idx in kfold.split(X, y_encoded)]\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "np.save(\"results/fold_indices.npy\", np.array(fold_indices, dtype=object), allow_pickle=True)\n",
    "\n",
    "\n",
    "# ===================== LOSS FUNCTIONS =====================\n",
    "def focal_loss(alpha=0.75, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        \n",
    "        return K.mean(alpha_t * focal_term * cross_entropy)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def hybrid_focal_loss(alpha=0.75, gamma=1.8, focal_weight=0.6):\n",
    "    \"\"\"IMPROVED: Fine-tuned parameters\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        bce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        \n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal = alpha_t * focal_term * bce\n",
    "        \n",
    "        combined = focal_weight * focal + (1 - focal_weight) * bce\n",
    "        return K.mean(combined)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# ===================== LEARNING RATE SCHEDULER =====================\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Learning rate schedule with warmup and decay\"\"\"\n",
    "    warmup_epochs = 5\n",
    "    decay_epochs = [30, 60, 90, 120]\n",
    "    \n",
    "    if epoch < warmup_epochs:\n",
    "        return 1e-4 * (epoch + 1) / warmup_epochs\n",
    "    \n",
    "    for decay_epoch in decay_epochs:\n",
    "        if epoch >= decay_epoch:\n",
    "            lr = lr * 0.7\n",
    "    \n",
    "    return max(lr, 1e-7)\n",
    "\n",
    "\n",
    "# ===================== 5 CNN BLOCKS MODEL =====================\n",
    "def build_improved_model(input_shape, use_attention=False):\n",
    "    \"\"\"\n",
    "    5 CNN BLOCKS MODEL:\n",
    "    - Block 1: 48 filters\n",
    "    - Block 2: 96 filters\n",
    "    - Block 3: 128 filters\n",
    "    - Block 4: 160 filters\n",
    "    - Block 5: 192 filters (NEW!)\n",
    "    - LSTM: 64 units\n",
    "    - Dense: 64 â†’ 32 â†’ 1\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # ========== Block 1: Local patterns ==========\n",
    "    x = Conv1D(48, kernel_size=7, padding='same', kernel_regularizer=l2(0.0015))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.25)(x)\n",
    "    \n",
    "    # ========== Block 2: Mid-level features ==========\n",
    "    x = Conv1D(96, kernel_size=5, padding='same', kernel_regularizer=l2(0.0015))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    \n",
    "    # ========== Block 3: High-level features ==========\n",
    "    x = Conv1D(128, kernel_size=3, padding='same', kernel_regularizer=l2(0.0015))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.35)(x)\n",
    "    \n",
    "    # ========== Block 4: Deep features ==========\n",
    "    x = Conv1D(160, kernel_size=3, padding='same', kernel_regularizer=l2(0.0015))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.35)(x)\n",
    "    \n",
    "    # ========== Block 5: Very deep features (NEW BLOCK!) ==========\n",
    "    x = Conv1D(192, kernel_size=3, padding='same', kernel_regularizer=l2(0.0015))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.35)(x)\n",
    "    \n",
    "    # ========== LSTM Temporal Modeling ==========\n",
    "    x = LSTM(64, return_sequences=False, kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.35)(x)\n",
    "    \n",
    "    # ========== Dense Classification ==========\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.002))(x)\n",
    "    x = Dropout(0.45)(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l2(0.002))(x)\n",
    "    x = Dropout(0.45)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# ===================== TRAINING =====================\n",
    "USE_DATA_AUGMENTATION = True\n",
    "USE_ATTENTION = False\n",
    "\n",
    "acc_per_fold = []\n",
    "auc_per_fold = []\n",
    "conf_matrices = []\n",
    "class_names = ['NORMAL', 'ALL (INTERICTAL+ICTAL)']\n",
    "\n",
    "fold_metrics = {\n",
    "    'fold_no': [],\n",
    "    'test_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_auc': [],\n",
    "    'val_acc': [],\n",
    "    'train_acc': [],\n",
    "    'f1_score': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "best_model = None\n",
    "best_fold = None\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "for fold_no, (train_val_idx, test_idx) in enumerate(fold_indices, start=1):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" FOLD {fold_no}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y_encoded[train_val_idx], y_encoded[test_idx]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.15, stratify=y_train_val, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    print(f\"Test set distribution: {np.unique(y_test, return_counts=True)}\")\n",
    "\n",
    "    # Build 5-CNN-block model\n",
    "    model = build_improved_model(input_shape=(X.shape[1], 1), use_attention=USE_ATTENTION)\n",
    "    \n",
    "    print(\"\\nğŸ—ï¸ Model Architecture: 5 CNN Blocks (48â†’96â†’128â†’160â†’192) + LSTM(64) + Dense(64â†’32)\")\n",
    "    \n",
    "    # Compile with AdamW\n",
    "    model.compile(\n",
    "        optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-5),\n",
    "        loss=hybrid_focal_loss(alpha=0.75, gamma=1.8, focal_weight=0.6),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks with learning rate scheduler\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule, verbose=0)\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=25,\n",
    "        restore_best_weights=True, \n",
    "        verbose=1, \n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10, \n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"results/model_fold{fold_no}.weights.h5\", \n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True,\n",
    "        verbose=0,  \n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    callbacks = [early_stop, reduce_lr, checkpoint, lr_scheduler]\n",
    "    \n",
    "    # Train Model\n",
    "    print(f\"\\nğŸš€ Training Fold {fold_no}...\")\n",
    "    \n",
    "    if USE_DATA_AUGMENTATION:\n",
    "        print(\"ğŸ“Š Using enhanced data augmentation\")\n",
    "        \n",
    "        train_generator = AugmentedDataGenerator(\n",
    "            X_train, y_train, \n",
    "            batch_size=16,\n",
    "            augmentation_prob=0.6,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=150,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=150,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    model.load_weights(f\"results/model_fold{fold_no}.weights.h5\")\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    test_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    auc_per_fold.append(test_auc)\n",
    "    \n",
    "    print(f\"\\nâœ… Fold {fold_no} Results:\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    fold_metrics['fold_no'].append(fold_no)\n",
    "    fold_metrics['test_acc'].append(test_acc)\n",
    "    fold_metrics['test_loss'].append(test_loss)\n",
    "    fold_metrics['test_auc'].append(test_auc)\n",
    "    fold_metrics['val_acc'].append(max(history.history['val_accuracy']))\n",
    "    fold_metrics['train_acc'].append(max(history.history['accuracy']))\n",
    "    fold_metrics['f1_score'].append(f1)\n",
    "    fold_metrics['precision'].append(precision)\n",
    "    fold_metrics['recall'].append(recall)\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_fold = fold_no\n",
    "        best_model = model\n",
    "        best_cm = cm\n",
    "        best_y_test = y_test\n",
    "        best_y_pred = y_pred\n",
    "        best_y_pred_prob = y_pred_prob\n",
    "        print(f\"ğŸŒŸ New best model! Fold {fold_no} with accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_no} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                annot_kws={'size': 14})\n",
    "    plt.title(f\"Fold {fold_no} Confusion Matrix\\nAcc: {test_acc:.4f} | AUC: {test_auc:.4f}\", \n",
    "              fontsize=14)\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/confusion_fold{fold_no}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    plt.axhline(y=test_acc, color='r', linestyle='--', label=f'Test Acc: {test_acc:.4f}')\n",
    "    plt.title(f'Fold {fold_no} - Model Accuracy', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Accuracy', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    plt.title(f'Fold {fold_no} - Model Loss', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Loss', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/training_history_fold{fold_no}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CROSS-VALIDATION SUMMARY - 5 CNN BLOCKS MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“Š Mean Test Accuracy: {np.mean(acc_per_fold):.4f} (Â±{np.std(acc_per_fold):.4f})\")\n",
    "print(f\"ğŸ“Š Mean Test AUC: {np.mean(auc_per_fold):.4f} (Â±{np.std(auc_per_fold):.4f})\")\n",
    "\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(total_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            annot_kws={'size': 14})\n",
    "plt.title(\"Overall Confusion Matrix (All Folds)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/confusion_overall.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nğŸ“‹ Fold-wise Metrics Summary:\")\n",
    "for i in range(len(fold_metrics['fold_no'])):\n",
    "    print(f\"\\nğŸ”¸ Fold {fold_metrics['fold_no'][i]} Metrics:\")\n",
    "    print(f\"  Train Accuracy : {fold_metrics['train_acc'][i]:.4f}\")\n",
    "    print(f\"  Val Accuracy   : {fold_metrics['val_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Accuracy  : {fold_metrics['test_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Loss      : {fold_metrics['test_loss'][i]:.4f}\")\n",
    "    print(f\"  Precision      : {fold_metrics['precision'][i]:.4f}\")\n",
    "    print(f\"  Recall         : {fold_metrics['recall'][i]:.4f}\")\n",
    "    print(f\"  F1 Score       : {fold_metrics['f1_score'][i]:.4f}\")\n",
    "    print(f\"  Test AUC       : {fold_metrics['test_auc'][i]:.4f}\")\n",
    "\n",
    "best_model.save(\"results/best_model.keras\")\n",
    "print(f\"\\nğŸ’¾ Best model (Fold {best_fold}) saved as 'results/best_model.keras'\")\n",
    "print(f\"\\nğŸ“ Model Architecture: 5 CNN Blocks + LSTM + Dense Layers\")\n",
    "print(f\"   CNN: 48 â†’ 96 â†’ 128 â†’ 160 â†’ 192 filters\")\n",
    "print(f\"   LSTM: 64 units\")\n",
    "print(f\"   Dense: 64 â†’ 32 â†’ 1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
