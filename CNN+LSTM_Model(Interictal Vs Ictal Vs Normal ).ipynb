{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, LSTM, Dense, Dropout,\n",
    "    LayerNormalization, Add, Bidirectional,\n",
    "    GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Load Data --------------------\n",
    "X = np.load(r\"preprocessed\\ALL_X.npy\")\n",
    "y = np.load(r\"preprocessed\\ALL_y.npy\")\n",
    "\n",
    "# Convert to binary: ICTAL = 0, ALL OTHERS = 1\n",
    "y_encoded = np.where(y == 'ICTAL', 0, 1)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Labels distribution:\", np.unique(y_encoded, return_counts=True))\n",
    "\n",
    "# -------------------- Prepare Cross Validation --------------------\n",
    "random_state = np.random.randint(0, 10000)\n",
    "print(f\"ðŸŽ² Random state used for this run: {random_state}\")\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "fold_indices = [(train_idx, test_idx) for train_idx, test_idx in kfold.split(X, y_encoded)]\n",
    "\n",
    "os.makedirs(\"results(Interictal_VS_Normal_VS_Ictal)\", exist_ok=True)\n",
    "np.save(\"results(Interictal_VS_Normal_VS_Ictal)/fold_indices.npy\", np.array(fold_indices, dtype=object), allow_pickle=True)\n",
    "\n",
    "def hybrid_focal_loss(alpha=0.25, gamma=2.0, bce_weight=0.5):\n",
    "    \"\"\"\n",
    "    Hybrid = BCE * bce_weight + FocalLoss * (1 - bce_weight)\n",
    "    \"\"\"\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # --- Binary Cross Entropy ---\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # --- Focal Loss ---\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        focal = -alpha * (1 - y_pred) ** gamma * y_true * K.log(y_pred) \\\n",
    "                - (1 - alpha) * y_pred ** gamma * (1 - y_true) * K.log(1 - y_pred)\n",
    "\n",
    "        focal = K.mean(focal, axis=-1)\n",
    "\n",
    "        # --- Combine Both ---\n",
    "        return bce_weight * bce + (1 - bce_weight) * focal\n",
    "\n",
    "    return loss\n",
    "\n",
    "# -------------------- CNN + LSTM Model --------------------\n",
    "def build_cnn_lstm(input_length):\n",
    "    model = Sequential([\n",
    "        # --- CNN Layers ---\n",
    "        Conv1D(32, kernel_size=7, activation='relu', input_shape=(input_length, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # --- LSTM ---\n",
    "        LSTM(64, return_sequences=False),\n",
    "\n",
    "        # --- Dense Layers ---\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "    optimizer=Adam(1e-4),\n",
    "    loss=hybrid_focal_loss(alpha=0.25, gamma=2.0, bce_weight=0.5),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------------------- Data Augmentation for 1D EEG (Optimized Version) --------------------\n",
    "def augment_signal(signal):\n",
    "    # 1) Very Light Noise\n",
    "    noise = np.random.normal(0, 0.005, signal.shape)\n",
    "    signal_noisy = signal + noise\n",
    "\n",
    "    # 2) Small Time Shift\n",
    "    shift = np.random.randint(-5, 5)\n",
    "    signal_shifted = np.roll(signal_noisy, shift)\n",
    "\n",
    "    # 3) Gentle Scaling\n",
    "    scale = np.random.uniform(0.97, 1.03)\n",
    "    signal_scaled = signal_shifted * scale\n",
    "\n",
    "    return signal_scaled\n",
    "\n",
    "\n",
    "\n",
    "def augment_batch(X, y):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        X_aug.append(X[i])\n",
    "        y_aug.append(y[i])\n",
    "\n",
    "        # Generate **1 weakly augmented version**\n",
    "        X_aug.append(augment_signal(X[i]))\n",
    "        y_aug.append(y[i])\n",
    "\n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Training --------------------\n",
    "acc_per_fold = []\n",
    "conf_matrices = []\n",
    "\n",
    "for fold_no, (train_val_idx, test_idx) in enumerate(fold_indices, start=1):\n",
    "    print(f\"\\nðŸ”¹ Fold {fold_no}\")\n",
    "\n",
    "    # Split into train/val/test\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y_encoded[train_val_idx], y_encoded[test_idx]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.1765, stratify=y_train_val, random_state=42\n",
    "    )\n",
    "\n",
    "    # ------------ APPLY DATA AUGMENTATION ------------\n",
    "    X_train, y_train = augment_batch(X_train, y_train)\n",
    "\n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "    # Build new model for each fold\n",
    "    model = build_cnn_lstm(X_train.shape[1])\n",
    "    model.summary()\n",
    "\n",
    "    # Handle class imbalance\n",
    "    cw = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = {0: cw[0], 1: cw[1]}\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=40,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    print(f\"Fold {fold_no} - Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Save weights\n",
    "    weight_path = f\"results(Interictal_VS_Normal_VS_Ictal)/cnn_lstm_fold{fold_no}.weights.h5\"\n",
    "    model.save_weights(weight_path)\n",
    "    print(f\"âœ… Weights saved to {weight_path}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\").flatten()\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=['Normal', 'Abnormal'], \n",
    "        yticklabels=['Normal', 'Abnormal']\n",
    "    )\n",
    "    plt.title(f\"Fold {fold_no} Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results(Interictal_VS_Normal_VS_Ictal)/cnn_lstm_conf_fold{fold_no}.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nðŸ“Š Mean Accuracy:\", np.mean(acc_per_fold))\n",
    "\n",
    "# -------------------- Overall Confusion Matrix --------------------\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(\n",
    "    total_cm, annot=True, fmt='d', cmap='Greens',\n",
    "    xticklabels=['Normal', 'Abnormal'], \n",
    "    yticklabels=['Normal', 'Abnormal']\n",
    ")\n",
    "plt.title(\"Overall Confusion Matrix (All Folds)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results(Interictal_VS_Normal_VS_Ictal)/cnn_lstm_conf_overall.png\")\n",
    "\n",
    "tn, fp, fn, tp = total_cm.ravel()\n",
    "\n",
    "overall_accuracy = (tp + tn) / np.sum(total_cm)\n",
    "overall_precision = tp / (tp + fp)\n",
    "overall_recall = tp / (tp + fn)\n",
    "overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall)\n",
    "\n",
    "print(\"\\nðŸ“Š Overall Metrics:\")\n",
    "print(f\"  Accuracy : {overall_accuracy:.4f}\")\n",
    "print(f\"  Precision: {overall_precision:.4f}\")\n",
    "print(f\"  Recall   : {overall_recall:.4f}\")\n",
    "print(f\"  F1-score : {overall_f1:.4f}\")\n",
    "\n",
    "print(\"âœ… CNN+LSTM Training Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
