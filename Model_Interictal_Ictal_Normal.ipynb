{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e41618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, \n",
    "    GlobalAveragePooling1D, Input, Activation, SpatialDropout1D, \n",
    "    LSTM, Bidirectional, Multiply, Reshape, LeakyReLU\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# ===================== DATA AUGMENTATION =====================\n",
    "def augment_signal(segment, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply random augmentation to EEG segment\n",
    "    \n",
    "    Args:\n",
    "        segment: Input EEG segment (1D array)\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented segment\n",
    "    \"\"\"\n",
    "    if np.random.random() > augmentation_prob:\n",
    "        return segment  # No augmentation\n",
    "    \n",
    "    # Choose augmentation type\n",
    "    aug_type = np.random.choice(['noise', 'scale', 'shift', 'time_shift'], p=[0.3, 0.3, 0.2, 0.2])\n",
    "    \n",
    "    if aug_type == 'noise':\n",
    "        # Add Gaussian noise\n",
    "        noise_level = np.random.uniform(0.01, 0.05)\n",
    "        noise = np.random.normal(0, noise_level, segment.shape)\n",
    "        return segment + noise\n",
    "    \n",
    "    elif aug_type == 'scale':\n",
    "        # Random amplitude scaling\n",
    "        scale = np.random.uniform(0.9, 1.1)\n",
    "        return segment * scale\n",
    "    \n",
    "    elif aug_type == 'shift':\n",
    "        # Random DC shift\n",
    "        shift = np.random.uniform(-0.1, 0.1)\n",
    "        return segment + shift\n",
    "    \n",
    "    elif aug_type == 'time_shift':\n",
    "        # Random time shift (circular shift)\n",
    "        shift_amount = np.random.randint(-20, 20)\n",
    "        return np.roll(segment, shift_amount)\n",
    "    \n",
    "    return segment\n",
    "\n",
    "\n",
    "def augment_batch(X_batch, y_batch, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply augmentation to a batch of data\n",
    "    \n",
    "    Args:\n",
    "        X_batch: Batch of EEG segments (batch_size, time_steps, channels)\n",
    "        y_batch: Batch of labels (class indices)\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented batch\n",
    "    \"\"\"\n",
    "    X_augmented = np.zeros_like(X_batch)\n",
    "    \n",
    "    for i in range(len(X_batch)):\n",
    "        # Convert one-hot to class index if needed\n",
    "        if len(y_batch.shape) > 1:\n",
    "            class_idx = np.argmax(y_batch[i])\n",
    "        else:\n",
    "            class_idx = y_batch[i]\n",
    "        \n",
    "        # Augment minority classes (ICTAL and INTERICTAL) more aggressively\n",
    "        if class_idx == 1:  # ICTAL (most minority)\n",
    "            prob = augmentation_prob * 2.0\n",
    "        elif class_idx == 2:  # INTERICTAL\n",
    "            prob = augmentation_prob * 1.5\n",
    "        else:  # NORMAL\n",
    "            prob = augmentation_prob\n",
    "        \n",
    "        X_augmented[i, :, 0] = augment_signal(X_batch[i, :, 0], prob)\n",
    "    \n",
    "    return X_augmented\n",
    "\n",
    "\n",
    "# ===================== CUSTOM DATA GENERATOR =====================\n",
    "class AugmentedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator with real-time augmentation for multi-class\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, augmentation_prob=0.5, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.X))\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        \n",
    "        # Get batch data\n",
    "        X_batch = self.X[batch_indices].copy()\n",
    "        y_batch = self.y[batch_indices]\n",
    "        \n",
    "        # Apply augmentation\n",
    "        X_batch = augment_batch(X_batch, y_batch, self.augmentation_prob)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "# ===================== SE BLOCK IMPLEMENTATION =====================\n",
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block for channel attention\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction=8, **kwargs):\n",
    "        super(SEBlock, self).__init__(**kwargs)\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.squeeze = GlobalAveragePooling1D()\n",
    "        \n",
    "        # Excitation network\n",
    "        self.fc1 = Dense(\n",
    "            channels // self.reduction, \n",
    "            activation='relu', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.fc2 = Dense(\n",
    "            channels, \n",
    "            activation='sigmoid', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        \n",
    "        super(SEBlock, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Squeeze: Global average pooling\n",
    "        squeeze = self.squeeze(inputs)\n",
    "        \n",
    "        # Excitation: Learn channel importance\n",
    "        excitation = self.fc1(squeeze)\n",
    "        excitation = self.fc2(excitation)\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        excitation = tf.reshape(excitation, [-1, 1, tf.shape(inputs)[-1]])\n",
    "        \n",
    "        # Scale: Multiply input with learned weights\n",
    "        return Multiply()([inputs, excitation])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SEBlock, self).get_config()\n",
    "        config.update({\"reduction\": self.reduction})\n",
    "        return config\n",
    "\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "data_dir = os.path.join(\"preprocessed\")\n",
    "X = np.load(os.path.join(data_dir, \"ALL_X.npy\"))\n",
    "y = np.load(os.path.join(data_dir, \"ALL_y.npy\"))\n",
    "file_ids = np.load(os.path.join(data_dir, \"ALL_file_ids.npy\"))\n",
    "\n",
    "# Encode labels: NORMAL=0, ICTAL=1, INTERICTAL=2\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"3-CLASS CLASSIFICATION SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original classes: {label_encoder.classes_}\")\n",
    "print(f\"Encoded as: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "print(f\"Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Total unique files: {len(np.unique(file_ids))}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare Data\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "\n",
    "# Convert to one-hot encoding for multi-class classification\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_categorical = to_categorical(y_encoded, num_classes=num_classes)\n",
    "print(f\"One-hot encoded labels shape: {y_categorical.shape}\")\n",
    "\n",
    "# Compute Class Weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# ===================== PREPARE CROSS VALIDATION (FILE-LEVEL) =====================\n",
    "random_state = np.random.randint(0, 10000)\n",
    "print(f\"ðŸŽ² Random state used for this run: {random_state}\")\n",
    "\n",
    "# Split by unique file IDs, not by segments\n",
    "unique_file_ids = np.unique(file_ids)\n",
    "print(f\"Total unique files for splitting: {len(unique_file_ids)}\")\n",
    "\n",
    "# Create a mapping from file_id to its label\n",
    "file_id_to_label = {}\n",
    "for fid in unique_file_ids:\n",
    "    mask = file_ids == fid\n",
    "    labels_in_file = y_encoded[mask]\n",
    "    # Use the most common label\n",
    "    file_id_to_label[fid] = np.bincount(labels_in_file).argmax()\n",
    "\n",
    "file_labels = np.array([file_id_to_label[fid] for fid in unique_file_ids])\n",
    "\n",
    "# Stratified K-Fold on FILE level\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "file_fold_indices = [(train_val_files, test_files) for train_val_files, test_files \n",
    "                     in kfold.split(unique_file_ids, file_labels)]\n",
    "\n",
    "# Convert file-level indices to segment-level indices\n",
    "fold_indices = []\n",
    "for train_val_files_idx, test_files_idx in file_fold_indices:\n",
    "    train_val_files = unique_file_ids[train_val_files_idx]\n",
    "    test_files = unique_file_ids[test_files_idx]\n",
    "    \n",
    "    # Get all segments from these files\n",
    "    train_val_mask = np.isin(file_ids, train_val_files)\n",
    "    test_mask = np.isin(file_ids, test_files)\n",
    "    \n",
    "    train_val_idx = np.where(train_val_mask)[0]\n",
    "    test_idx = np.where(test_mask)[0]\n",
    "    \n",
    "    fold_indices.append((train_val_idx, test_idx))\n",
    "    \n",
    "    print(f\"Fold: Train/Val files: {len(train_val_files)}, Test files: {len(test_files)}\")\n",
    "    print(f\"      Train/Val segments: {len(train_val_idx)}, Test segments: {len(test_idx)}\")\n",
    "\n",
    "# Save indices for reproducibility\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "np.save(os.path.join(\"results\", \"fold_indices_3class.npy\"), np.array(fold_indices, dtype=object), allow_pickle=True)\n",
    "np.save(os.path.join(\"results\", \"file_fold_indices_3class.npy\"), np.array(file_fold_indices, dtype=object), allow_pickle=True)\n",
    "\n",
    "\n",
    "# ===================== LOSS FUNCTIONS =====================\n",
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi-class classification\n",
    "    \n",
    "    Args:\n",
    "        alpha: Weighting factor in range (0,1)\n",
    "        gamma: Focusing parameter for modulating loss\n",
    "    \n",
    "    Returns:\n",
    "        Loss function\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Cross entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        \n",
    "        # Focal term: (1-p_t)^gamma\n",
    "        focal_term = K.pow(1 - y_pred, gamma)\n",
    "        \n",
    "        # Combine\n",
    "        focal_loss = alpha * focal_term * cross_entropy\n",
    "        \n",
    "        return K.mean(K.sum(focal_loss, axis=-1))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def hybrid_focal_loss(alpha=0.25, gamma=2.0, focal_weight=0.6):\n",
    "    \"\"\"\n",
    "    Hybrid: Focal Loss + Categorical Cross-Entropy for multi-class\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Categorical cross-entropy\n",
    "        cce = -y_true * K.log(y_pred)\n",
    "        \n",
    "        # Focal term\n",
    "        focal_term = K.pow(1 - y_pred, gamma)\n",
    "        focal = alpha * focal_term * cce\n",
    "        \n",
    "        # Combine\n",
    "        combined = focal_weight * focal + (1 - focal_weight) * cce\n",
    "        \n",
    "        return K.mean(K.sum(combined, axis=-1))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# ===================== MODEL BUILDING =====================\n",
    "def build_model(input_shape, num_classes=3):\n",
    "    \"\"\"\n",
    "    3-Class Classification Model: NORMAL vs ICTAL vs INTERICTAL\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # ========== Block 1: Local patterns ==========\n",
    "    x = Conv1D(48, kernel_size=7, padding='same', kernel_regularizer=l2(0.002))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.28)(x)\n",
    "    \n",
    "    # ========== Block 2: Mid-level features ==========\n",
    "    x = Conv1D(96, kernel_size=5, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.32)(x)\n",
    "    \n",
    "    # ========== Block 3: High-level features ==========\n",
    "    x = Conv1D(128, kernel_size=3, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.38)(x)\n",
    "    \n",
    "    # ========== Block 4: Deep features ==========\n",
    "    x = Conv1D(160, kernel_size=3, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.38)(x)\n",
    "    \n",
    "    # ========== LSTM Temporal Modeling ==========\n",
    "    x = LSTM(64, return_sequences=False, kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # ========== Dense Classification ==========\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.48)(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.48)(x)\n",
    "    \n",
    "    # Output layer with softmax for multi-class\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ===================== TRAINING =====================\n",
    "USE_DATA_AUGMENTATION = True\n",
    "\n",
    "acc_per_fold = []\n",
    "conf_matrices = []\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Track metrics for best model selection\n",
    "fold_metrics = {\n",
    "    'fold_no': [],\n",
    "    'test_acc': [],\n",
    "    'test_loss': [],\n",
    "    'val_acc': [],\n",
    "    'train_acc': [],\n",
    "    'f1_score': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "best_model = None\n",
    "best_fold = None\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "for fold_no, (train_val_idx, test_idx) in enumerate(fold_indices, start=1):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" FOLD {fold_no}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Split into train/val/test\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y_categorical[train_val_idx], y_categorical[test_idx]\n",
    "    y_train_val_encoded = y_encoded[train_val_idx]\n",
    "    file_ids_train_val = file_ids[train_val_idx]\n",
    "    \n",
    "    # Split train/val by files, not segments\n",
    "    unique_train_val_files = np.unique(file_ids_train_val)\n",
    "    train_val_file_labels = np.array([file_id_to_label[fid] for fid in unique_train_val_files])\n",
    "    \n",
    "    train_files, val_files = train_test_split(\n",
    "        unique_train_val_files, \n",
    "        test_size=0.15, \n",
    "        stratify=train_val_file_labels, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_mask = np.isin(file_ids_train_val, train_files)\n",
    "    val_mask = np.isin(file_ids_train_val, val_files)\n",
    "    \n",
    "    X_train = X_train_val[train_mask]\n",
    "    y_train = y_train_val[train_mask]\n",
    "    X_val = X_train_val[val_mask]\n",
    "    y_val = y_train_val[val_mask]\n",
    "\n",
    "    print(f\"Train files: {len(train_files)}, Val files: {len(val_files)}, Test files: {len(np.unique(file_ids[test_idx]))}\")\n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Print test set distribution\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    test_dist = dict(zip(*np.unique(y_test_labels, return_counts=True)))\n",
    "    test_dist_named = {class_names[k]: v for k, v in test_dist.items()}\n",
    "    print(f\"Test set distribution: {test_dist_named}\")\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(input_shape=(X.shape[1], 1), num_classes=num_classes)\n",
    "    \n",
    "    # Print model summary only for first fold\n",
    "    if fold_no == 1:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL ARCHITECTURE (3-CLASS)\")\n",
    "        print(\"=\"*60)\n",
    "        model.summary()\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=5e-5),\n",
    "        loss=hybrid_focal_loss(alpha=0.25, gamma=2.0, focal_weight=0.6),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=25, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1, \n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.6,\n",
    "        patience=8, \n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        os.path.join(\"results\", f\"model_3class_fold{fold_no}.weights.h5\"),\n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    callbacks = [early_stop, reduce_lr, checkpoint]\n",
    "    \n",
    "    # Train Model\n",
    "    print(f\"\\nðŸš€ Training Fold {fold_no}...\")\n",
    "    \n",
    "    if USE_DATA_AUGMENTATION:\n",
    "        print(\"ðŸ“Š Using data augmentation with real-time generator\")\n",
    "        \n",
    "        # Create data generator\n",
    "        train_generator = AugmentedDataGenerator(\n",
    "            X_train, y_train, \n",
    "            batch_size=12, \n",
    "            augmentation_prob=0.5,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=200,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        print(\"ðŸ“Š Training without data augmentation\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=150,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_weights(os.path.join(\"results\", f\"model_3class_fold{fold_no}.weights.h5\"))\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    print(f\"\\nâœ… Fold {fold_no} Results:\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_labels, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "    \n",
    "    # Classification Metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test_labels, y_pred, average='weighted')\n",
    "    \n",
    "    # Store metrics\n",
    "    fold_metrics['fold_no'].append(fold_no)\n",
    "    fold_metrics['test_acc'].append(test_acc)\n",
    "    fold_metrics['test_loss'].append(test_loss)\n",
    "    fold_metrics['val_acc'].append(max(history.history['val_accuracy']))\n",
    "    fold_metrics['train_acc'].append(max(history.history['accuracy']))\n",
    "    fold_metrics['f1_score'].append(f1)\n",
    "    fold_metrics['precision'].append(precision)\n",
    "    fold_metrics['recall'].append(recall)\n",
    "    \n",
    "    # Check if this is the best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_fold = fold_no\n",
    "        best_model = model\n",
    "        best_cm = cm\n",
    "        best_y_test = y_test_labels\n",
    "        best_y_pred = y_pred\n",
    "        best_y_pred_prob = y_pred_prob\n",
    "        print(f\"ðŸŒŸ New best model! Fold {fold_no} with accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_no} Classification Report:\")\n",
    "    print(classification_report(y_test_labels, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                annot_kws={'size': 14})\n",
    "    plt.title(f\"Fold {fold_no} Confusion Matrix (3-Class)\\nAcc: {test_acc:.4f}\", \n",
    "              fontsize=14)\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"results\", f\"confusion_3class_fold{fold_no}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Training History\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    plt.axhline(y=test_acc, color='r', linestyle='--', label=f'Test Acc: {test_acc:.4f}')\n",
    "    plt.title(f'Fold {fold_no} - Model Accuracy', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Accuracy', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    plt.title(f'Fold {fold_no} - Model Loss', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Loss', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"results\", f\"training_history_3class_fold{fold_no}.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CROSS-VALIDATION SUMMARY (3-CLASS)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Mean Test Accuracy across folds: {np.mean(acc_per_fold):.4f} (Â±{np.std(acc_per_fold):.4f})\")\n",
    "\n",
    "# Combine confusion matrices\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(total_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            annot_kws={'size': 14})\n",
    "plt.title(\"Overall Confusion Matrix - All Folds (3-Class)\", fontsize=14)\n",
    "plt.xlabel(\"Predicted\", fontsize=12)\n",
    "plt.ylabel(\"True\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"results\", \"confusion_3class_overall.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Print Fold Metrics\n",
    "print(\"\\nðŸ“‹ Fold-wise Metrics Summary:\")\n",
    "for i in range(len(fold_metrics['fold_no'])):\n",
    "    print(f\"\\nðŸ”¸ Fold {fold_metrics['fold_no'][i]} Metrics:\")\n",
    "    print(f\"  Train Accuracy : {fold_metrics['train_acc'][i]:.4f}\")\n",
    "    print(f\"  Val Accuracy   : {fold_metrics['val_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Accuracy  : {fold_metrics['test_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Loss      : {fold_metrics['test_loss'][i]:.4f}\")\n",
    "    print(f\"  Precision      : {fold_metrics['precision'][i]:.4f}\")\n",
    "    print(f\"  Recall         : {fold_metrics['recall'][i]:.4f}\")\n",
    "    print(f\"  F1 Score       : {fold_metrics['f1_score'][i]:.4f}\")\n",
    "\n",
    "# Per-class metrics from best model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" BEST MODEL DETAILED METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best model from Fold {best_fold} with accuracy: {best_acc:.4f}\\n\")\n",
    "print(\"Per-class Performance:\")\n",
    "print(classification_report(best_y_test, best_y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# Save best model\n",
    "best_model.save(os.path.join(\"results\", \"best_model_3class.keras\"))\n",
    "print(f\"\\nðŸ’¾ Best model (Fold {best_fold}) saved as 'results/best_model_3class.keras'\")\n",
    "\n",
    "# Save label encoder for future use\n",
    "np.save(os.path.join(\"results\", \"label_encoder_classes_3class.npy\"), label_encoder.classes_)\n",
    "print(f\"ðŸ’¾ Label encoder classes saved as 'results/label_encoder_classes_3class.npy'\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
