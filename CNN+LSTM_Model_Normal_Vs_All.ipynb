{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc9a736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 1/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([1280, 1920]))\n",
      "Binary labels - Test: (array([0, 1]), array([320, 480]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">431</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">431</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">427</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">427</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">213</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">213</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">211</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">211</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m431\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m431\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m427\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚        \u001b[38;5;34m10,304\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m427\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m213\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m213\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m211\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m24,704\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m211\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m49,408\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m4,160\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m65\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,793</span> (350.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m89,793\u001b[0m (350.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,345</span> (349.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m89,345\u001b[0m (349.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int64(0): np.float64(1.25), np.int64(1): np.float64(0.8333333333333334)}\n",
      "\n",
      "ğŸš€ Training Fold 1...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 145ms/step - accuracy: 0.7152 - loss: 0.3184 - val_accuracy: 0.6000 - val_loss: 0.4308\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.8588 - loss: 0.1931 - val_accuracy: 0.6637 - val_loss: 0.3734\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.8901 - loss: 0.1585 - val_accuracy: 0.9009 - val_loss: 0.1594\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 164ms/step - accuracy: 0.9142 - loss: 0.1308 - val_accuracy: 0.9257 - val_loss: 0.1043\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 165ms/step - accuracy: 0.9195 - loss: 0.1212 - val_accuracy: 0.9398 - val_loss: 0.0896\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 160ms/step - accuracy: 0.9195 - loss: 0.1153 - val_accuracy: 0.9363 - val_loss: 0.0848\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 124ms/step - accuracy: 0.9283 - loss: 0.1013 - val_accuracy: 0.9522 - val_loss: 0.0730\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9359 - loss: 0.0949 - val_accuracy: 0.9504 - val_loss: 0.0724\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9343 - loss: 0.0949 - val_accuracy: 0.9575 - val_loss: 0.0635\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9404 - loss: 0.0866 - val_accuracy: 0.9575 - val_loss: 0.0637\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.9279 - loss: 0.0965 - val_accuracy: 0.9575 - val_loss: 0.0588\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9408 - loss: 0.0839 - val_accuracy: 0.9558 - val_loss: 0.0580\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 117ms/step - accuracy: 0.9417 - loss: 0.0785 - val_accuracy: 0.9593 - val_loss: 0.0582\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9457 - loss: 0.0769 - val_accuracy: 0.9628 - val_loss: 0.0537\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 122ms/step - accuracy: 0.9452 - loss: 0.0794 - val_accuracy: 0.9646 - val_loss: 0.0542\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9472 - loss: 0.0752 - val_accuracy: 0.9646 - val_loss: 0.0523\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9484 - loss: 0.0720 - val_accuracy: 0.9628 - val_loss: 0.0508\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 92ms/step - accuracy: 0.9482 - loss: 0.0692 - val_accuracy: 0.9699 - val_loss: 0.0594\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9497 - loss: 0.0712 - val_accuracy: 0.9717 - val_loss: 0.0498\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9516 - loss: 0.0668 - val_accuracy: 0.9681 - val_loss: 0.0548\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9535 - loss: 0.0646 - val_accuracy: 0.9717 - val_loss: 0.0498\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9573 - loss: 0.0621 - val_accuracy: 0.9699 - val_loss: 0.0501\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9522 - loss: 0.0664 - val_accuracy: 0.9752 - val_loss: 0.0463\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.9558 - loss: 0.0630 - val_accuracy: 0.9735 - val_loss: 0.0480\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 110ms/step - accuracy: 0.9577 - loss: 0.0558 - val_accuracy: 0.9788 - val_loss: 0.0476\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9571 - loss: 0.0597 - val_accuracy: 0.9770 - val_loss: 0.0518\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9586 - loss: 0.0544 - val_accuracy: 0.9752 - val_loss: 0.0441\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9569 - loss: 0.0559 - val_accuracy: 0.9788 - val_loss: 0.0490\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.9577 - loss: 0.0550 - val_accuracy: 0.9788 - val_loss: 0.0431\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.9584 - loss: 0.0544 - val_accuracy: 0.9788 - val_loss: 0.0449\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 123ms/step - accuracy: 0.9643 - loss: 0.0491 - val_accuracy: 0.9752 - val_loss: 0.0552\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.9664 - loss: 0.0459 - val_accuracy: 0.9752 - val_loss: 0.0543\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9626 - loss: 0.0500 - val_accuracy: 0.9735 - val_loss: 0.0488\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9639 - loss: 0.0522 - val_accuracy: 0.9752 - val_loss: 0.0462\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.9653 - loss: 0.0476 - val_accuracy: 0.9735 - val_loss: 0.0611\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9651 - loss: 0.0480 - val_accuracy: 0.9788 - val_loss: 0.0466\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9655 - loss: 0.0468 - val_accuracy: 0.9717 - val_loss: 0.0513\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9643 - loss: 0.0484 - val_accuracy: 0.9770 - val_loss: 0.0449\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9691 - loss: 0.0441 - val_accuracy: 0.9823 - val_loss: 0.0398\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9676 - loss: 0.0444 - val_accuracy: 0.9788 - val_loss: 0.0491\n",
      "\n",
      "âœ… Fold 1 - Test Accuracy: 0.9800\n",
      "ğŸ’¾ Weights saved to results(Normal_VS_All)\\cnn_lstm_fold1.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 2/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([1280, 1920]))\n",
      "Binary labels - Test: (array([0, 1]), array([320, 480]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int64(0): np.float64(1.25), np.int64(1): np.float64(0.8333333333333334)}\n",
      "\n",
      "ğŸš€ Training Fold 2...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 146ms/step - accuracy: 0.6894 - loss: 0.3355 - val_accuracy: 0.6088 - val_loss: 0.3665\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 145ms/step - accuracy: 0.8493 - loss: 0.2131 - val_accuracy: 0.7504 - val_loss: 0.2900\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.8888 - loss: 0.1673 - val_accuracy: 0.9133 - val_loss: 0.1297\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9157 - loss: 0.1357 - val_accuracy: 0.9469 - val_loss: 0.0861\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 140ms/step - accuracy: 0.9342 - loss: 0.1182 - val_accuracy: 0.9434 - val_loss: 0.0945\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 135ms/step - accuracy: 0.9328 - loss: 0.1088 - val_accuracy: 0.9451 - val_loss: 0.0885\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.9336 - loss: 0.1089 - val_accuracy: 0.9469 - val_loss: 0.0822\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.9385 - loss: 0.1031 - val_accuracy: 0.9469 - val_loss: 0.0816\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9378 - loss: 0.0882 - val_accuracy: 0.9504 - val_loss: 0.0732\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9355 - loss: 0.0941 - val_accuracy: 0.9540 - val_loss: 0.0596\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9435 - loss: 0.0849 - val_accuracy: 0.9593 - val_loss: 0.0589\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 136ms/step - accuracy: 0.9380 - loss: 0.0835 - val_accuracy: 0.9593 - val_loss: 0.0548\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 135ms/step - accuracy: 0.9467 - loss: 0.0780 - val_accuracy: 0.9611 - val_loss: 0.0536\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 133ms/step - accuracy: 0.9459 - loss: 0.0801 - val_accuracy: 0.9611 - val_loss: 0.0566\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.9457 - loss: 0.0751 - val_accuracy: 0.9575 - val_loss: 0.0541\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9495 - loss: 0.0723 - val_accuracy: 0.9699 - val_loss: 0.0461\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9482 - loss: 0.0701 - val_accuracy: 0.9681 - val_loss: 0.0469\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 138ms/step - accuracy: 0.9471 - loss: 0.0703 - val_accuracy: 0.9735 - val_loss: 0.0458\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9493 - loss: 0.0703 - val_accuracy: 0.9717 - val_loss: 0.0453\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 136ms/step - accuracy: 0.9499 - loss: 0.0706 - val_accuracy: 0.9735 - val_loss: 0.0470\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 98ms/step - accuracy: 0.9486 - loss: 0.0664 - val_accuracy: 0.9735 - val_loss: 0.0446\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9569 - loss: 0.0603 - val_accuracy: 0.9717 - val_loss: 0.0423\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.9558 - loss: 0.0604 - val_accuracy: 0.9788 - val_loss: 0.0376\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 109ms/step - accuracy: 0.9560 - loss: 0.0639 - val_accuracy: 0.9735 - val_loss: 0.0470\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.9550 - loss: 0.0622 - val_accuracy: 0.9752 - val_loss: 0.0468\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.9607 - loss: 0.0567 - val_accuracy: 0.9752 - val_loss: 0.0469\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 114ms/step - accuracy: 0.9588 - loss: 0.0586 - val_accuracy: 0.9752 - val_loss: 0.0459\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.9605 - loss: 0.0531 - val_accuracy: 0.9770 - val_loss: 0.0463\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.9609 - loss: 0.0547 - val_accuracy: 0.9646 - val_loss: 0.0454\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 92ms/step - accuracy: 0.9602 - loss: 0.0561 - val_accuracy: 0.9770 - val_loss: 0.0463\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - accuracy: 0.9571 - loss: 0.0582 - val_accuracy: 0.9681 - val_loss: 0.0447\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 80ms/step - accuracy: 0.9639 - loss: 0.0518 - val_accuracy: 0.9628 - val_loss: 0.0475\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.9611 - loss: 0.0574 - val_accuracy: 0.9717 - val_loss: 0.0435\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.9617 - loss: 0.0531 - val_accuracy: 0.9664 - val_loss: 0.0442\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9615 - loss: 0.0552 - val_accuracy: 0.9699 - val_loss: 0.0436\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.9683 - loss: 0.0470 - val_accuracy: 0.9646 - val_loss: 0.0438\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 100ms/step - accuracy: 0.9632 - loss: 0.0524 - val_accuracy: 0.9664 - val_loss: 0.0417\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 107ms/step - accuracy: 0.9645 - loss: 0.0495 - val_accuracy: 0.9788 - val_loss: 0.0386\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.9666 - loss: 0.0466 - val_accuracy: 0.9770 - val_loss: 0.0408\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9643 - loss: 0.0457 - val_accuracy: 0.9699 - val_loss: 0.0451\n",
      "\n",
      "âœ… Fold 2 - Test Accuracy: 0.9525\n",
      "ğŸ’¾ Weights saved to results(Normal_VS_All)\\cnn_lstm_fold2.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 3/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([1280, 1920]))\n",
      "Binary labels - Test: (array([0, 1]), array([320, 480]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int64(0): np.float64(1.25), np.int64(1): np.float64(0.8333333333333334)}\n",
      "\n",
      "ğŸš€ Training Fold 3...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 166ms/step - accuracy: 0.6968 - loss: 0.3169 - val_accuracy: 0.6000 - val_loss: 0.4326\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.8704 - loss: 0.1964 - val_accuracy: 0.6389 - val_loss: 0.4126\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9028 - loss: 0.1516 - val_accuracy: 0.8814 - val_loss: 0.1839\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9228 - loss: 0.1338 - val_accuracy: 0.9434 - val_loss: 0.0986\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9349 - loss: 0.1164 - val_accuracy: 0.9451 - val_loss: 0.0940\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 129ms/step - accuracy: 0.9417 - loss: 0.1036 - val_accuracy: 0.9451 - val_loss: 0.0874\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.9474 - loss: 0.0950 - val_accuracy: 0.9451 - val_loss: 0.0848\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.9416 - loss: 0.0947 - val_accuracy: 0.9522 - val_loss: 0.0793\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 0.9503 - loss: 0.0853 - val_accuracy: 0.9593 - val_loss: 0.0737\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9463 - loss: 0.0838 - val_accuracy: 0.9575 - val_loss: 0.0678\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9516 - loss: 0.0786 - val_accuracy: 0.9593 - val_loss: 0.0647\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.9518 - loss: 0.0769 - val_accuracy: 0.9611 - val_loss: 0.0626\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.9503 - loss: 0.0757 - val_accuracy: 0.9646 - val_loss: 0.0600\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 124ms/step - accuracy: 0.9546 - loss: 0.0712 - val_accuracy: 0.9717 - val_loss: 0.0629\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9556 - loss: 0.0662 - val_accuracy: 0.9681 - val_loss: 0.0688\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 123ms/step - accuracy: 0.9571 - loss: 0.0688 - val_accuracy: 0.9681 - val_loss: 0.0595\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9571 - loss: 0.0660 - val_accuracy: 0.9717 - val_loss: 0.0622\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9586 - loss: 0.0610 - val_accuracy: 0.9735 - val_loss: 0.0664\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9596 - loss: 0.0640 - val_accuracy: 0.9735 - val_loss: 0.0591\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9630 - loss: 0.0545 - val_accuracy: 0.9752 - val_loss: 0.0586\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9636 - loss: 0.0570 - val_accuracy: 0.9717 - val_loss: 0.0579\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9638 - loss: 0.0543 - val_accuracy: 0.9717 - val_loss: 0.0463\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9590 - loss: 0.0546 - val_accuracy: 0.9770 - val_loss: 0.0505\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9632 - loss: 0.0536 - val_accuracy: 0.9805 - val_loss: 0.0504\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9649 - loss: 0.0524 - val_accuracy: 0.9735 - val_loss: 0.0499\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9664 - loss: 0.0489 - val_accuracy: 0.9752 - val_loss: 0.0513\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 121ms/step - accuracy: 0.9668 - loss: 0.0484 - val_accuracy: 0.9752 - val_loss: 0.0517\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9643 - loss: 0.0498 - val_accuracy: 0.9788 - val_loss: 0.0494\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9702 - loss: 0.0423 - val_accuracy: 0.9735 - val_loss: 0.0510\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9677 - loss: 0.0492 - val_accuracy: 0.9805 - val_loss: 0.0448\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 99ms/step - accuracy: 0.9712 - loss: 0.0445 - val_accuracy: 0.9699 - val_loss: 0.0610\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 80ms/step - accuracy: 0.9683 - loss: 0.0472 - val_accuracy: 0.9788 - val_loss: 0.0436\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 81ms/step - accuracy: 0.9725 - loss: 0.0432 - val_accuracy: 0.9805 - val_loss: 0.0391\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9713 - loss: 0.0430 - val_accuracy: 0.9788 - val_loss: 0.0453\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.9713 - loss: 0.0413 - val_accuracy: 0.9752 - val_loss: 0.0448\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 117ms/step - accuracy: 0.9719 - loss: 0.0431 - val_accuracy: 0.9841 - val_loss: 0.0415\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 118ms/step - accuracy: 0.9715 - loss: 0.0400 - val_accuracy: 0.9823 - val_loss: 0.0333\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9651 - loss: 0.0483 - val_accuracy: 0.9805 - val_loss: 0.0347\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 121ms/step - accuracy: 0.9691 - loss: 0.0428 - val_accuracy: 0.9788 - val_loss: 0.0437\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9710 - loss: 0.0416 - val_accuracy: 0.9805 - val_loss: 0.0418\n",
      "\n",
      "âœ… Fold 3 - Test Accuracy: 0.9663\n",
      "ğŸ’¾ Weights saved to results(Normal_VS_All)\\cnn_lstm_fold3.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 4/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([1280, 1920]))\n",
      "Binary labels - Test: (array([0, 1]), array([320, 480]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int64(0): np.float64(1.25), np.int64(1): np.float64(0.8333333333333334)}\n",
      "\n",
      "ğŸš€ Training Fold 4...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 138ms/step - accuracy: 0.7091 - loss: 0.3243 - val_accuracy: 0.6000 - val_loss: 0.3778\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.8455 - loss: 0.2157 - val_accuracy: 0.6619 - val_loss: 0.3480\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.8880 - loss: 0.1698 - val_accuracy: 0.8726 - val_loss: 0.1846\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9101 - loss: 0.1396 - val_accuracy: 0.9009 - val_loss: 0.1458\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 97ms/step - accuracy: 0.9298 - loss: 0.1154 - val_accuracy: 0.9133 - val_loss: 0.1256\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 120ms/step - accuracy: 0.9279 - loss: 0.1131 - val_accuracy: 0.9150 - val_loss: 0.1239\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9340 - loss: 0.1004 - val_accuracy: 0.9150 - val_loss: 0.1202\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9412 - loss: 0.0966 - val_accuracy: 0.9274 - val_loss: 0.1188\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9454 - loss: 0.0858 - val_accuracy: 0.9487 - val_loss: 0.0909\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9412 - loss: 0.0921 - val_accuracy: 0.9540 - val_loss: 0.0875\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.9412 - loss: 0.0856 - val_accuracy: 0.9558 - val_loss: 0.0837\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 85ms/step - accuracy: 0.9397 - loss: 0.0858 - val_accuracy: 0.9504 - val_loss: 0.0882\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9467 - loss: 0.0836 - val_accuracy: 0.9504 - val_loss: 0.0874\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9509 - loss: 0.0751 - val_accuracy: 0.9593 - val_loss: 0.0817\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9533 - loss: 0.0712 - val_accuracy: 0.9575 - val_loss: 0.0771\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9509 - loss: 0.0721 - val_accuracy: 0.9575 - val_loss: 0.0830\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9520 - loss: 0.0719 - val_accuracy: 0.9611 - val_loss: 0.0736\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 107ms/step - accuracy: 0.9545 - loss: 0.0641 - val_accuracy: 0.9593 - val_loss: 0.0830\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 84ms/step - accuracy: 0.9558 - loss: 0.0637 - val_accuracy: 0.9593 - val_loss: 0.0797\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 118ms/step - accuracy: 0.9510 - loss: 0.0656 - val_accuracy: 0.9575 - val_loss: 0.0804\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.9588 - loss: 0.0637 - val_accuracy: 0.9593 - val_loss: 0.0772\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9592 - loss: 0.0577 - val_accuracy: 0.9593 - val_loss: 0.0843\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9592 - loss: 0.0602 - val_accuracy: 0.9575 - val_loss: 0.0855\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - accuracy: 0.9581 - loss: 0.0612 - val_accuracy: 0.9611 - val_loss: 0.0755\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9581 - loss: 0.0604 - val_accuracy: 0.9646 - val_loss: 0.0773\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9620 - loss: 0.0553 - val_accuracy: 0.9664 - val_loss: 0.0727\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9605 - loss: 0.0567 - val_accuracy: 0.9664 - val_loss: 0.0712\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9611 - loss: 0.0544 - val_accuracy: 0.9699 - val_loss: 0.0709\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9613 - loss: 0.0534 - val_accuracy: 0.9699 - val_loss: 0.0687\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9638 - loss: 0.0492 - val_accuracy: 0.9646 - val_loss: 0.0675\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9630 - loss: 0.0523 - val_accuracy: 0.9664 - val_loss: 0.0760\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 98ms/step - accuracy: 0.9619 - loss: 0.0507 - val_accuracy: 0.9611 - val_loss: 0.0679\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 80ms/step - accuracy: 0.9617 - loss: 0.0499 - val_accuracy: 0.9681 - val_loss: 0.0724\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 99ms/step - accuracy: 0.9636 - loss: 0.0505 - val_accuracy: 0.9664 - val_loss: 0.0738\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.9696 - loss: 0.0480 - val_accuracy: 0.9646 - val_loss: 0.0735\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.9677 - loss: 0.0479 - val_accuracy: 0.9681 - val_loss: 0.0603\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9679 - loss: 0.0436 - val_accuracy: 0.9664 - val_loss: 0.0618\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9679 - loss: 0.0447 - val_accuracy: 0.9664 - val_loss: 0.0628\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9725 - loss: 0.0442 - val_accuracy: 0.9699 - val_loss: 0.0613\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9672 - loss: 0.0465 - val_accuracy: 0.9699 - val_loss: 0.0571\n",
      "\n",
      "âœ… Fold 4 - Test Accuracy: 0.9850\n",
      "ğŸ’¾ Weights saved to results(Normal_VS_All)\\cnn_lstm_fold4.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 5/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([1280, 1920]))\n",
      "Binary labels - Test: (array([0, 1]), array([320, 480]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int64(0): np.float64(1.25), np.int64(1): np.float64(0.8333333333333334)}\n",
      "\n",
      "ğŸš€ Training Fold 5...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 119ms/step - accuracy: 0.7281 - loss: 0.3070 - val_accuracy: 0.6177 - val_loss: 0.3382\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.8632 - loss: 0.1924 - val_accuracy: 0.8372 - val_loss: 0.2380\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.8970 - loss: 0.1510 - val_accuracy: 0.9080 - val_loss: 0.1472\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9300 - loss: 0.1134 - val_accuracy: 0.8867 - val_loss: 0.1363\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9497 - loss: 0.0889 - val_accuracy: 0.9080 - val_loss: 0.1335\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9505 - loss: 0.0926 - val_accuracy: 0.8973 - val_loss: 0.1273\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9478 - loss: 0.0871 - val_accuracy: 0.9080 - val_loss: 0.0986\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9505 - loss: 0.0777 - val_accuracy: 0.9381 - val_loss: 0.0783\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9590 - loss: 0.0700 - val_accuracy: 0.9310 - val_loss: 0.0829\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9569 - loss: 0.0695 - val_accuracy: 0.9416 - val_loss: 0.0731\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9583 - loss: 0.0696 - val_accuracy: 0.9363 - val_loss: 0.0757\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9567 - loss: 0.0616 - val_accuracy: 0.9540 - val_loss: 0.0608\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9581 - loss: 0.0618 - val_accuracy: 0.9398 - val_loss: 0.0695\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9586 - loss: 0.0617 - val_accuracy: 0.9611 - val_loss: 0.0467\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9615 - loss: 0.0571 - val_accuracy: 0.9664 - val_loss: 0.0442\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9634 - loss: 0.0536 - val_accuracy: 0.9699 - val_loss: 0.0392\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9632 - loss: 0.0508 - val_accuracy: 0.9681 - val_loss: 0.0406\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9655 - loss: 0.0519 - val_accuracy: 0.9681 - val_loss: 0.0444\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.9670 - loss: 0.0499 - val_accuracy: 0.9699 - val_loss: 0.0390\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.9674 - loss: 0.0479 - val_accuracy: 0.9770 - val_loss: 0.0314\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9626 - loss: 0.0486 - val_accuracy: 0.9788 - val_loss: 0.0284\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9676 - loss: 0.0432 - val_accuracy: 0.9823 - val_loss: 0.0273\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9708 - loss: 0.0437 - val_accuracy: 0.9752 - val_loss: 0.0343\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9698 - loss: 0.0424 - val_accuracy: 0.9841 - val_loss: 0.0251\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9674 - loss: 0.0429 - val_accuracy: 0.9805 - val_loss: 0.0282\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9721 - loss: 0.0392 - val_accuracy: 0.9841 - val_loss: 0.0244\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9694 - loss: 0.0433 - val_accuracy: 0.9876 - val_loss: 0.0248\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9727 - loss: 0.0379 - val_accuracy: 0.9858 - val_loss: 0.0228\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.9719 - loss: 0.0412 - val_accuracy: 0.9894 - val_loss: 0.0223\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9658 - loss: 0.0465 - val_accuracy: 0.9858 - val_loss: 0.0247\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9751 - loss: 0.0366 - val_accuracy: 0.9894 - val_loss: 0.0202\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9744 - loss: 0.0381 - val_accuracy: 0.9876 - val_loss: 0.0239\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9725 - loss: 0.0387 - val_accuracy: 0.9858 - val_loss: 0.0238\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9750 - loss: 0.0362 - val_accuracy: 0.9912 - val_loss: 0.0207\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 80ms/step - accuracy: 0.9757 - loss: 0.0340 - val_accuracy: 0.9929 - val_loss: 0.0215\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9805 - loss: 0.0318 - val_accuracy: 0.9805 - val_loss: 0.0308\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9776 - loss: 0.0300 - val_accuracy: 0.9912 - val_loss: 0.0210\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9719 - loss: 0.0403 - val_accuracy: 0.9912 - val_loss: 0.0200\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 84ms/step - accuracy: 0.9780 - loss: 0.0325 - val_accuracy: 0.9894 - val_loss: 0.0183\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.9786 - loss: 0.0296 - val_accuracy: 0.9876 - val_loss: 0.0234\n",
      "\n",
      "âœ… Fold 5 - Test Accuracy: 0.9337\n",
      "ğŸ’¾ Weights saved to results(Normal_VS_All)\\cnn_lstm_fold5.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š FINAL RESULTS\n",
      "============================================================\n",
      "Mean Accuracy across all folds: 0.9635 Â± 0.0187\n",
      "Accuracy per fold: ['0.9800', '0.9525', '0.9663', '0.9850', '0.9337']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Overall Metrics (All Folds Combined):\n",
      "  Accuracy : 0.9635\n",
      "  Precision: 0.9649\n",
      "  Recall   : 0.9746\n",
      "  F1-score : 0.9697\n",
      "\n",
      "âœ… CNN+LSTM Training Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "PREPROCESSING_DIR = r\"Preprocessing_Updated_Kfold\"  # Path to your preprocessing output\n",
    "N_FOLDS = 5\n",
    "RESULTS_DIR = \"results(Normal_VS_All)\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- Hybrid Focal Loss --------------------\n",
    "def hybrid_focal_loss(alpha=0.25, gamma=2.0, bce_weight=0.5):\n",
    "    \"\"\"\n",
    "    Hybrid = BCE * bce_weight + FocalLoss * (1 - bce_weight)\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        # --- Binary Cross Entropy ---\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # --- Focal Loss ---\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        focal = -alpha * (1 - y_pred) ** gamma * y_true * K.log(y_pred) \\\n",
    "                - (1 - alpha) * y_pred ** gamma * (1 - y_true) * K.log(1 - y_pred)\n",
    "\n",
    "        focal = K.mean(focal, axis=-1)\n",
    "\n",
    "        # --- Combine Both ---\n",
    "        return bce_weight * bce + (1 - bce_weight) * focal\n",
    "\n",
    "    return loss\n",
    "\n",
    "# -------------------- CNN + LSTM Model --------------------\n",
    "def build_cnn_lstm(input_length):\n",
    "    model = Sequential([\n",
    "        # --- CNN Layers ---\n",
    "        Conv1D(32, kernel_size=7, activation='relu', input_shape=(input_length, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # --- LSTM ---\n",
    "        LSTM(64, return_sequences=False),\n",
    "\n",
    "        # --- Dense Layers ---\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-4),\n",
    "        loss=hybrid_focal_loss(alpha=0.25, gamma=2.0, bce_weight=0.5),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -------------------- Data Augmentation --------------------\n",
    "def augment_signal(signal):\n",
    "    # 1) Very Light Noise\n",
    "    noise = np.random.normal(0, 0.005, signal.shape)\n",
    "    signal_noisy = signal + noise\n",
    "\n",
    "    # 2) Small Time Shift\n",
    "    shift = np.random.randint(-5, 5)\n",
    "    signal_shifted = np.roll(signal_noisy, shift)\n",
    "\n",
    "    # 3) Gentle Scaling\n",
    "    scale = np.random.uniform(0.97, 1.03)\n",
    "    signal_scaled = signal_shifted * scale\n",
    "\n",
    "    return signal_scaled\n",
    "\n",
    "def augment_batch(X, y):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        X_aug.append(X[i])\n",
    "        y_aug.append(y[i])\n",
    "\n",
    "        # Generate 1 weakly augmented version\n",
    "        X_aug.append(augment_signal(X[i]))\n",
    "        y_aug.append(y[i])\n",
    "\n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "# -------------------- Training Loop --------------------\n",
    "acc_per_fold = []\n",
    "conf_matrices = []\n",
    "\n",
    "for fold_no in range(N_FOLDS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ”¹ Fold {fold_no + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # -------------------- Load Preprocessed Data --------------------\n",
    "    X_train_full = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_X_train.npy\"), allow_pickle=True)\n",
    "    y_train_full = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_y_train.npy\"), allow_pickle=True)\n",
    "    X_test = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_X_test.npy\"), allow_pickle=True)\n",
    "    y_test = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_y_test.npy\"), allow_pickle=True)\n",
    "    \n",
    "    # Convert to proper numpy arrays if they're object dtype\n",
    "    if X_train_full.dtype == object:\n",
    "        X_train_full = np.vstack(X_train_full).astype(np.float32)\n",
    "    if X_test.dtype == object:\n",
    "        X_test = np.vstack(X_test).astype(np.float32)\n",
    "    \n",
    "    X_train_full = X_train_full.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_train_full = y_train_full.astype(np.int32)\n",
    "    y_test = y_test.astype(np.int32)\n",
    "\n",
    "    # -------------------- Convert to Binary Classification --------------------\n",
    "    # NORMAL = 0 (class 0 in preprocessing), ALL OTHERS = 1 (classes 1 and 2)\n",
    "    y_train_full_binary = np.where(y_train_full == 0, 0, 1)\n",
    "    y_test_binary = np.where(y_test == 0, 0, 1)\n",
    "\n",
    "    print(f\"Original labels - Train: {np.unique(y_train_full, return_counts=True)}\")\n",
    "    print(f\"Binary labels - Train: {np.unique(y_train_full_binary, return_counts=True)}\")\n",
    "    print(f\"Binary labels - Test: {np.unique(y_test_binary, return_counts=True)}\")\n",
    "\n",
    "    # -------------------- Split Train into Train/Val --------------------\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full_binary, \n",
    "        test_size=0.1765, \n",
    "        stratify=y_train_full_binary, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # -------------------- Apply Data Augmentation --------------------\n",
    "    X_train, y_train = augment_batch(X_train, y_train)\n",
    "\n",
    "    # -------------------- Reshape for CNN --------------------\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    print(f\"\\nğŸ“Š Data shapes:\")\n",
    "    print(f\"  Train: {X_train.shape}\")\n",
    "    print(f\"  Val:   {X_val.shape}\")\n",
    "    print(f\"  Test:  {X_test.shape}\")\n",
    "\n",
    "    # -------------------- Build Model --------------------\n",
    "    model = build_cnn_lstm(X_train.shape[1])\n",
    "    \n",
    "    if fold_no == 0:  # Show summary only for first fold\n",
    "        model.summary()\n",
    "\n",
    "    # -------------------- Class Weights --------------------\n",
    "    unique_classes = np.unique(y_train)\n",
    "    cw = compute_class_weight('balanced', classes=unique_classes, y=y_train)\n",
    "    class_weights = {cls: weight for cls, weight in zip(unique_classes, cw)}\n",
    "    print(f\"\\nâš–ï¸ Class weights: {class_weights}\")\n",
    "\n",
    "    # -------------------- Train Model --------------------\n",
    "    print(f\"\\nğŸš€ Training Fold {fold_no + 1}...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=40,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # -------------------- Evaluate --------------------\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_binary, verbose=0)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    print(f\"\\nâœ… Fold {fold_no + 1} - Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # -------------------- Save Weights --------------------\n",
    "    weight_path = os.path.join(RESULTS_DIR, f\"cnn_lstm_fold{fold_no + 1}.weights.h5\")\n",
    "    model.save_weights(weight_path)\n",
    "    print(f\"ğŸ’¾ Weights saved to {weight_path}\")\n",
    "\n",
    "    # -------------------- Confusion Matrix --------------------\n",
    "    y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(\"int32\").flatten()\n",
    "    cm = confusion_matrix(y_test_binary, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=['NORMAL', 'ABNORMAL'], \n",
    "        yticklabels=['NORMAL', 'ABNORMAL']\n",
    "    )\n",
    "    plt.title(f\"Fold {fold_no + 1} Confusion Matrix\\nAccuracy: {test_acc:.4f}\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f\"cnn_lstm_conf_fold{fold_no + 1}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# -------------------- Overall Results --------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Accuracy across all folds: {np.mean(acc_per_fold):.4f} Â± {np.std(acc_per_fold):.4f}\")\n",
    "print(f\"Accuracy per fold: {[f'{acc:.4f}' for acc in acc_per_fold]}\")\n",
    "\n",
    "# -------------------- Overall Confusion Matrix --------------------\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    total_cm, annot=True, fmt='d', cmap='Greens',\n",
    "    xticklabels=['NORMAL', 'ABNORMAL'], \n",
    "    yticklabels=['NORMAL', 'ABNORMAL']\n",
    ")\n",
    "plt.title(\"Overall Confusion Matrix (All Folds)\")\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"cnn_lstm_conf_overall.png\"))\n",
    "plt.show()\n",
    "\n",
    "# -------------------- Calculate Overall Metrics --------------------\n",
    "tn, fp, fn, tp = total_cm.ravel()\n",
    "\n",
    "overall_accuracy = (tp + tn) / np.sum(total_cm)\n",
    "overall_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "overall_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "\n",
    "print(\"\\nğŸ“Š Overall Metrics (All Folds Combined):\")\n",
    "print(f\"  Accuracy : {overall_accuracy:.4f}\")\n",
    "print(f\"  Precision: {overall_precision:.4f}\")\n",
    "print(f\"  Recall   : {overall_recall:.4f}\")\n",
    "print(f\"  F1-score : {overall_f1:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… CNN+LSTM Training Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
