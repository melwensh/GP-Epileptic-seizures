{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f767d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes: ['ICTAL' 'INTERICTAL' 'NORMAL']\n",
      "Binary classes: 0 (NORMAL) vs 1 (INTERICTAL + ICTAL)\n",
      "Binary labels distribution: (array([0, 1]), array([4400, 6600]))\n",
      "Total samples: 11000\n",
      "Total unique files: 500\n",
      "Dataset shape: (11000, 347, 1)\n",
      "Class weights: {0: np.float64(1.25), 1: np.float64(0.8333333333333334)}\n",
      "ðŸŽ² Random state used for this run: 9540\n",
      "Total unique files for splitting: 500\n",
      "Fold: Train/Val files: 400, Test files: 100\n",
      "      Train/Val segments: 8800, Test segments: 2200\n",
      "Fold: Train/Val files: 400, Test files: 100\n",
      "      Train/Val segments: 8800, Test segments: 2200\n",
      "Fold: Train/Val files: 400, Test files: 100\n",
      "      Train/Val segments: 8800, Test segments: 2200\n",
      "Fold: Train/Val files: 400, Test files: 100\n",
      "      Train/Val segments: 8800, Test segments: 2200\n",
      "Fold: Train/Val files: 400, Test files: 100\n",
      "      Train/Val segments: 8800, Test segments: 2200\n",
      "\n",
      "============================================================\n",
      " FOLD 1\n",
      "============================================================\n",
      "\n",
      "Train files: 340, Val files: 60, Test files: 100\n",
      "Train: 7480, Val: 1320, Test: 2200\n",
      "Test set distribution: (array([0, 1]), array([ 880, 1320]))\n",
      "WARNING:tensorflow:From c:\\Users\\Mostafa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mostafa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training Fold 1...\n",
      "ðŸ“Š Using data augmentation with real-time generator\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mostafa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 39ms/step - accuracy: 0.5496 - loss: 1.5174 - val_accuracy: 0.6000 - val_loss: 1.4300 - learning_rate: 5.0000e-05\n",
      "Epoch 2/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.6778 - loss: 1.3727 - val_accuracy: 0.7045 - val_loss: 1.2773 - learning_rate: 5.0000e-05\n",
      "Epoch 3/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.7710 - loss: 1.2319 - val_accuracy: 0.7614 - val_loss: 1.1960 - learning_rate: 5.0000e-05\n",
      "Epoch 4/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.7992 - loss: 1.1331 - val_accuracy: 0.7871 - val_loss: 1.1257 - learning_rate: 5.0000e-05\n",
      "Epoch 5/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.8206 - loss: 1.0518 - val_accuracy: 0.8364 - val_loss: 1.0094 - learning_rate: 5.0000e-05\n",
      "Epoch 6/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.8293 - loss: 0.9807 - val_accuracy: 0.8500 - val_loss: 0.9286 - learning_rate: 5.0000e-05\n",
      "Epoch 7/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.8413 - loss: 0.9173 - val_accuracy: 0.8530 - val_loss: 0.8669 - learning_rate: 5.0000e-05\n",
      "Epoch 8/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.8405 - loss: 0.8689 - val_accuracy: 0.8652 - val_loss: 0.7926 - learning_rate: 5.0000e-05\n",
      "Epoch 9/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.8536 - loss: 0.8082 - val_accuracy: 0.8894 - val_loss: 0.7228 - learning_rate: 5.0000e-05\n",
      "Epoch 10/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.8523 - loss: 0.7582 - val_accuracy: 0.9083 - val_loss: 0.6640 - learning_rate: 5.0000e-05\n",
      "Epoch 11/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.8674 - loss: 0.7101 - val_accuracy: 0.9015 - val_loss: 0.6280 - learning_rate: 5.0000e-05\n",
      "Epoch 12/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.8757 - loss: 0.6691 - val_accuracy: 0.9379 - val_loss: 0.5628 - learning_rate: 5.0000e-05\n",
      "Epoch 13/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.8753 - loss: 0.6318 - val_accuracy: 0.9333 - val_loss: 0.5356 - learning_rate: 5.0000e-05\n",
      "Epoch 14/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.8809 - loss: 0.5984 - val_accuracy: 0.9250 - val_loss: 0.5178 - learning_rate: 5.0000e-05\n",
      "Epoch 15/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.8850 - loss: 0.5727 - val_accuracy: 0.9470 - val_loss: 0.4686 - learning_rate: 5.0000e-05\n",
      "Epoch 16/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.8868 - loss: 0.5448 - val_accuracy: 0.9371 - val_loss: 0.4522 - learning_rate: 5.0000e-05\n",
      "Epoch 17/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.8880 - loss: 0.5210 - val_accuracy: 0.9492 - val_loss: 0.4209 - learning_rate: 5.0000e-05\n",
      "Epoch 18/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.8908 - loss: 0.4972 - val_accuracy: 0.9712 - val_loss: 0.3829 - learning_rate: 5.0000e-05\n",
      "Epoch 19/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9013 - loss: 0.4712 - val_accuracy: 0.9311 - val_loss: 0.3946 - learning_rate: 5.0000e-05\n",
      "Epoch 20/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.8985 - loss: 0.4528 - val_accuracy: 0.9720 - val_loss: 0.3464 - learning_rate: 5.0000e-05\n",
      "Epoch 21/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9033 - loss: 0.4378 - val_accuracy: 0.9674 - val_loss: 0.3381 - learning_rate: 5.0000e-05\n",
      "Epoch 22/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 38ms/step - accuracy: 0.9011 - loss: 0.4254 - val_accuracy: 0.9652 - val_loss: 0.3252 - learning_rate: 5.0000e-05\n",
      "Epoch 23/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9116 - loss: 0.3978 - val_accuracy: 0.9636 - val_loss: 0.3115 - learning_rate: 5.0000e-05\n",
      "Epoch 24/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 38ms/step - accuracy: 0.9094 - loss: 0.3906 - val_accuracy: 0.9439 - val_loss: 0.3121 - learning_rate: 5.0000e-05\n",
      "Epoch 25/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9104 - loss: 0.3765 - val_accuracy: 0.9742 - val_loss: 0.2758 - learning_rate: 5.0000e-05\n",
      "Epoch 26/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9148 - loss: 0.3651 - val_accuracy: 0.9333 - val_loss: 0.3006 - learning_rate: 5.0000e-05\n",
      "Epoch 27/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.9135 - loss: 0.3552 - val_accuracy: 0.9712 - val_loss: 0.2657 - learning_rate: 5.0000e-05\n",
      "Epoch 28/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9148 - loss: 0.3425 - val_accuracy: 0.9705 - val_loss: 0.2549 - learning_rate: 5.0000e-05\n",
      "Epoch 29/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9225 - loss: 0.3238 - val_accuracy: 0.9697 - val_loss: 0.2464 - learning_rate: 5.0000e-05\n",
      "Epoch 30/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9170 - loss: 0.3162 - val_accuracy: 0.9727 - val_loss: 0.2356 - learning_rate: 5.0000e-05\n",
      "Epoch 31/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.9242 - loss: 0.3092 - val_accuracy: 0.9545 - val_loss: 0.2457 - learning_rate: 5.0000e-05\n",
      "Epoch 32/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9241 - loss: 0.3051 - val_accuracy: 0.9712 - val_loss: 0.2235 - learning_rate: 5.0000e-05\n",
      "Epoch 33/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9242 - loss: 0.2943 - val_accuracy: 0.9750 - val_loss: 0.2123 - learning_rate: 5.0000e-05\n",
      "Epoch 34/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9229 - loss: 0.2905 - val_accuracy: 0.9508 - val_loss: 0.2254 - learning_rate: 5.0000e-05\n",
      "Epoch 35/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9251 - loss: 0.2832 - val_accuracy: 0.9758 - val_loss: 0.2018 - learning_rate: 5.0000e-05\n",
      "Epoch 36/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9210 - loss: 0.2817 - val_accuracy: 0.9659 - val_loss: 0.2054 - learning_rate: 5.0000e-05\n",
      "Epoch 37/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9302 - loss: 0.2685 - val_accuracy: 0.9614 - val_loss: 0.2130 - learning_rate: 5.0000e-05\n",
      "Epoch 38/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9309 - loss: 0.2594 - val_accuracy: 0.9750 - val_loss: 0.1845 - learning_rate: 5.0000e-05\n",
      "Epoch 39/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9330 - loss: 0.2539 - val_accuracy: 0.9811 - val_loss: 0.1710 - learning_rate: 5.0000e-05\n",
      "Epoch 40/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9298 - loss: 0.2492 - val_accuracy: 0.9712 - val_loss: 0.1821 - learning_rate: 5.0000e-05\n",
      "Epoch 41/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9290 - loss: 0.2483 - val_accuracy: 0.9765 - val_loss: 0.1772 - learning_rate: 5.0000e-05\n",
      "Epoch 42/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9325 - loss: 0.2387 - val_accuracy: 0.9689 - val_loss: 0.1808 - learning_rate: 5.0000e-05\n",
      "Epoch 43/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.9306 - loss: 0.2416 - val_accuracy: 0.9712 - val_loss: 0.1728 - learning_rate: 5.0000e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9336 - loss: 0.2314 - val_accuracy: 0.9780 - val_loss: 0.1638 - learning_rate: 5.0000e-05\n",
      "Epoch 45/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9297 - loss: 0.2336 - val_accuracy: 0.9758 - val_loss: 0.1602 - learning_rate: 5.0000e-05\n",
      "Epoch 46/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9350 - loss: 0.2270 - val_accuracy: 0.9811 - val_loss: 0.1552 - learning_rate: 5.0000e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.9357 - loss: 0.2174 - val_accuracy: 0.9833 - val_loss: 0.1453 - learning_rate: 5.0000e-05\n",
      "Epoch 48/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9340 - loss: 0.2175 - val_accuracy: 0.9803 - val_loss: 0.1471 - learning_rate: 5.0000e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9337 - loss: 0.2178 - val_accuracy: 0.9841 - val_loss: 0.1401 - learning_rate: 5.0000e-05\n",
      "Epoch 50/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9356 - loss: 0.2149 - val_accuracy: 0.9795 - val_loss: 0.1351 - learning_rate: 5.0000e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9369 - loss: 0.2053 - val_accuracy: 0.9795 - val_loss: 0.1422 - learning_rate: 5.0000e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9400 - loss: 0.2034 - val_accuracy: 0.9818 - val_loss: 0.1341 - learning_rate: 5.0000e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9373 - loss: 0.2015 - val_accuracy: 0.9750 - val_loss: 0.1371 - learning_rate: 5.0000e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9382 - loss: 0.1968 - val_accuracy: 0.9795 - val_loss: 0.1372 - learning_rate: 5.0000e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9430 - loss: 0.1927 - val_accuracy: 0.9811 - val_loss: 0.1232 - learning_rate: 5.0000e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.9397 - loss: 0.1988 - val_accuracy: 0.9659 - val_loss: 0.1447 - learning_rate: 5.0000e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9376 - loss: 0.1944 - val_accuracy: 0.9826 - val_loss: 0.1240 - learning_rate: 5.0000e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9389 - loss: 0.1913 - val_accuracy: 0.9803 - val_loss: 0.1225 - learning_rate: 5.0000e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9451 - loss: 0.1822 - val_accuracy: 0.9826 - val_loss: 0.1156 - learning_rate: 5.0000e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9413 - loss: 0.1845 - val_accuracy: 0.9826 - val_loss: 0.1155 - learning_rate: 5.0000e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.9414 - loss: 0.1812 - val_accuracy: 0.9811 - val_loss: 0.1199 - learning_rate: 5.0000e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9449 - loss: 0.1738 - val_accuracy: 0.9803 - val_loss: 0.1125 - learning_rate: 5.0000e-05\n",
      "Epoch 63/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9418 - loss: 0.1805 - val_accuracy: 0.9826 - val_loss: 0.1143 - learning_rate: 5.0000e-05\n",
      "Epoch 64/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9441 - loss: 0.1714 - val_accuracy: 0.9826 - val_loss: 0.1121 - learning_rate: 5.0000e-05\n",
      "Epoch 65/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9410 - loss: 0.1759 - val_accuracy: 0.9856 - val_loss: 0.1077 - learning_rate: 5.0000e-05\n",
      "Epoch 66/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.9457 - loss: 0.1705 - val_accuracy: 0.9841 - val_loss: 0.1080 - learning_rate: 5.0000e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9460 - loss: 0.1669 - val_accuracy: 0.9856 - val_loss: 0.1055 - learning_rate: 5.0000e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.9477 - loss: 0.1651 - val_accuracy: 0.9833 - val_loss: 0.1058 - learning_rate: 5.0000e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9456 - loss: 0.1698 - val_accuracy: 0.9750 - val_loss: 0.1131 - learning_rate: 5.0000e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9464 - loss: 0.1642 - val_accuracy: 0.9841 - val_loss: 0.1045 - learning_rate: 5.0000e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9464 - loss: 0.1643 - val_accuracy: 0.9818 - val_loss: 0.0983 - learning_rate: 5.0000e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m624/624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.9476 - loss: 0.1606 - val_accuracy: 0.9788 - val_loss: 0.1014 - learning_rate: 5.0000e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m133/624\u001b[0m \u001b[32mâ”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.9436 - loss: 0.1649"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, \n",
    "    GlobalAveragePooling1D, Input, Activation, SpatialDropout1D, \n",
    "    LSTM, Bidirectional, Multiply, Reshape, LeakyReLU\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# ===================== DATA AUGMENTATION =====================\n",
    "def augment_signal(segment, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply random augmentation to EEG segment\n",
    "    \n",
    "    Args:\n",
    "        segment: Input EEG segment (1D array)\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented segment\n",
    "    \"\"\"\n",
    "    if np.random.random() > augmentation_prob:\n",
    "        return segment  # No augmentation\n",
    "    \n",
    "    # Choose augmentation type\n",
    "    aug_type = np.random.choice(['noise', 'scale', 'shift', 'time_shift'], p=[0.3, 0.3, 0.2, 0.2])\n",
    "    \n",
    "    if aug_type == 'noise':\n",
    "        # Add Gaussian noise\n",
    "        noise_level = np.random.uniform(0.01, 0.05)\n",
    "        noise = np.random.normal(0, noise_level, segment.shape)\n",
    "        return segment + noise\n",
    "    \n",
    "    elif aug_type == 'scale':\n",
    "        # Random amplitude scaling\n",
    "        scale = np.random.uniform(0.9, 1.1)\n",
    "        return segment * scale\n",
    "    \n",
    "    elif aug_type == 'shift':\n",
    "        # Random DC shift\n",
    "        shift = np.random.uniform(-0.1, 0.1)\n",
    "        return segment + shift\n",
    "    \n",
    "    elif aug_type == 'time_shift':\n",
    "        # Random time shift (circular shift)\n",
    "        shift_amount = np.random.randint(-20, 20)\n",
    "        return np.roll(segment, shift_amount)\n",
    "    \n",
    "    return segment\n",
    "\n",
    "\n",
    "def augment_batch(X_batch, y_batch, augmentation_prob=0.5):\n",
    "    \"\"\"\n",
    "    Apply augmentation to a batch of data\n",
    "    \n",
    "    Args:\n",
    "        X_batch: Batch of EEG segments (batch_size, time_steps, channels)\n",
    "        y_batch: Batch of labels\n",
    "        augmentation_prob: Probability of applying augmentation\n",
    "    \n",
    "    Returns:\n",
    "        Augmented batch\n",
    "    \"\"\"\n",
    "    X_augmented = np.zeros_like(X_batch)\n",
    "    \n",
    "    for i in range(len(X_batch)):\n",
    "        # Only augment positive class (seizure) more aggressively\n",
    "        if y_batch[i] == 1:\n",
    "            prob = augmentation_prob * 1.5  # Higher probability for minority class\n",
    "        else:\n",
    "            prob = augmentation_prob\n",
    "        \n",
    "        X_augmented[i, :, 0] = augment_signal(X_batch[i, :, 0], prob)\n",
    "    \n",
    "    return X_augmented\n",
    "\n",
    "\n",
    "class DataAugmentationCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback to apply data augmentation during training\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, augmentation_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.X_train_original = X_train.copy()\n",
    "        self.y_train = y_train\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Apply augmentation at the start of each epoch\n",
    "        if epoch > 0:  # Skip first epoch to see baseline performance\n",
    "            X_augmented = augment_batch(\n",
    "                self.X_train_original, \n",
    "                self.y_train, \n",
    "                self.augmentation_prob\n",
    "            )\n",
    "            # Update model's training data\n",
    "            self.model.stop_training = False\n",
    "\n",
    "\n",
    "# ===================== CUSTOM DATA GENERATOR =====================\n",
    "class AugmentedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator with real-time augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, augmentation_prob=0.5, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.X))\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        \n",
    "        # Get batch data\n",
    "        X_batch = self.X[batch_indices].copy()\n",
    "        y_batch = self.y[batch_indices]\n",
    "        \n",
    "        # Apply augmentation\n",
    "        X_batch = augment_batch(X_batch, y_batch, self.augmentation_prob)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "# ===================== SE BLOCK IMPLEMENTATION =====================\n",
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block for channel attention\n",
    "    Paper: \"Interpretable classification of epileptic EEG signals...\"\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction=8, **kwargs):\n",
    "        super(SEBlock, self).__init__(**kwargs)\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.squeeze = GlobalAveragePooling1D()\n",
    "        \n",
    "        # Excitation network\n",
    "        self.fc1 = Dense(\n",
    "            channels // self.reduction, \n",
    "            activation='relu', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.fc2 = Dense(\n",
    "            channels, \n",
    "            activation='sigmoid', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        \n",
    "        super(SEBlock, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Squeeze: Global average pooling\n",
    "        squeeze = self.squeeze(inputs)\n",
    "        \n",
    "        # Excitation: Learn channel importance\n",
    "        excitation = self.fc1(squeeze)\n",
    "        excitation = self.fc2(excitation)\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        excitation = tf.reshape(excitation, [-1, 1, tf.shape(inputs)[-1]])\n",
    "        \n",
    "        # Scale: Multiply input with learned weights\n",
    "        return Multiply()([inputs, excitation])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SEBlock, self).get_config()\n",
    "        config.update({\"reduction\": self.reduction})\n",
    "        return config\n",
    "\n",
    "\n",
    "# ===================== LOAD DATA =====================\n",
    "X = np.load(r\"preprocessed\\ALL_X.npy\")\n",
    "y = np.load(r\"preprocessed\\ALL_y.npy\")\n",
    "file_ids = np.load(r\"preprocessed\\ALL_file_ids.npy\")  # âœ… Load file IDs\n",
    "\n",
    "# Convert to Binary Classification \n",
    "y_encoded = np.where(y == 'NORMAL', 0, 1)\n",
    "\n",
    "print(\"Original classes:\", np.unique(y))\n",
    "print(\"Binary classes: 0 (NORMAL) vs 1 (INTERICTAL + ICTAL)\")\n",
    "print(\"Binary labels distribution:\", np.unique(y_encoded, return_counts=True))\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Total unique files: {len(np.unique(file_ids))}\")\n",
    "\n",
    "# Prepare Data\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "# Compute Class Weights \n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# ===================== PREPARE CROSS VALIDATION (FILE-LEVEL) =====================\n",
    "random_state = np.random.randint(0, 10000)\n",
    "print(f\"ðŸŽ² Random state used for this run: {random_state}\")\n",
    "\n",
    "# âœ… KEY FIX: Split by unique file IDs, not by segments\n",
    "unique_file_ids = np.unique(file_ids)\n",
    "print(f\"Total unique files for splitting: {len(unique_file_ids)}\")\n",
    "\n",
    "# Create a mapping from file_id to its label (use majority vote if needed)\n",
    "file_id_to_label = {}\n",
    "for fid in unique_file_ids:\n",
    "    mask = file_ids == fid\n",
    "    labels_in_file = y_encoded[mask]\n",
    "    # Use the most common label (should be the same for all segments from one file)\n",
    "    file_id_to_label[fid] = np.bincount(labels_in_file).argmax()\n",
    "\n",
    "file_labels = np.array([file_id_to_label[fid] for fid in unique_file_ids])\n",
    "\n",
    "# âœ… Stratified K-Fold on FILE level\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "file_fold_indices = [(train_val_files, test_files) for train_val_files, test_files \n",
    "                     in kfold.split(unique_file_ids, file_labels)]\n",
    "\n",
    "# Convert file-level indices to segment-level indices\n",
    "fold_indices = []\n",
    "for train_val_files_idx, test_files_idx in file_fold_indices:\n",
    "    train_val_files = unique_file_ids[train_val_files_idx]\n",
    "    test_files = unique_file_ids[test_files_idx]\n",
    "    \n",
    "    # Get all segments from these files\n",
    "    train_val_mask = np.isin(file_ids, train_val_files)\n",
    "    test_mask = np.isin(file_ids, test_files)\n",
    "    \n",
    "    train_val_idx = np.where(train_val_mask)[0]\n",
    "    test_idx = np.where(test_mask)[0]\n",
    "    \n",
    "    fold_indices.append((train_val_idx, test_idx))\n",
    "    \n",
    "    print(f\"Fold: Train/Val files: {len(train_val_files)}, Test files: {len(test_files)}\")\n",
    "    print(f\"      Train/Val segments: {len(train_val_idx)}, Test segments: {len(test_idx)}\")\n",
    "\n",
    "# Save indices for reproducibility\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "np.save(\"results/fold_indices.npy\", np.array(fold_indices, dtype=object), allow_pickle=True)\n",
    "np.save(\"results/file_fold_indices.npy\", np.array(file_fold_indices, dtype=object), allow_pickle=True)\n",
    "\n",
    "\n",
    "# ===================== LOSS FUNCTIONS =====================\n",
    "def focal_loss(alpha=0.75, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        \n",
    "        return K.mean(alpha_t * focal_term * cross_entropy)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def hybrid_focal_loss(alpha=0.75, gamma=1.7, focal_weight=0.55):\n",
    "    \"\"\"Hybrid: Focal + BCE\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        bce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        \n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = K.pow(1 - p_t, gamma)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal = alpha_t * focal_term * bce\n",
    "        \n",
    "        combined = focal_weight * focal + (1 - focal_weight) * bce\n",
    "        return K.mean(combined)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# ===================== MODEL BUILDING =====================\n",
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    MODIFIED MODEL:\n",
    "    - Added 4th CNN block\n",
    "    - Changed Bidirectional LSTM to regular LSTM\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # ========== Block 1: Local patterns ==========\n",
    "    x = Conv1D(48, kernel_size=7, padding='same', kernel_regularizer=l2(0.002))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.28)(x)\n",
    "    \n",
    "    # ========== Block 2: Mid-level features ==========\n",
    "    x = Conv1D(96, kernel_size=5, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.32)(x)\n",
    "    \n",
    "    # ========== Block 3: High-level features ==========\n",
    "    x = Conv1D(128, kernel_size=3, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.38)(x)\n",
    "    \n",
    "    # ========== Block 4: Deep features (NEW LAYER) ==========\n",
    "    x = Conv1D(160, kernel_size=3, padding='same', kernel_regularizer=l2(0.002))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SEBlock(reduction=8)(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = SpatialDropout1D(0.38)(x)\n",
    "    \n",
    "    # ========== LSTM Temporal Modeling (Changed from Bidirectional to regular LSTM) ==========\n",
    "    x = LSTM(64, return_sequences=False, kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # ========== Dense Classification ==========\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.48)(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu', kernel_regularizer=l2(0.003))(x)\n",
    "    x = Dropout(0.48)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# ===================== TRAINING =====================\n",
    "USE_DATA_AUGMENTATION = True  \n",
    "\n",
    "acc_per_fold = []\n",
    "auc_per_fold = []\n",
    "conf_matrices = []\n",
    "class_names = ['NORMAL', 'ALL (INTERICTAL+ICTAL)']\n",
    "\n",
    "# Track metrics for best model selection\n",
    "fold_metrics = {\n",
    "    'fold_no': [],\n",
    "    'test_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_auc': [],\n",
    "    'val_acc': [],\n",
    "    'train_acc': [],\n",
    "    'f1_score': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "best_model = None\n",
    "best_fold = None\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "for fold_no, (train_val_idx, test_idx) in enumerate(fold_indices, start=1):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" FOLD {fold_no}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Split into train/val/test\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y_encoded[train_val_idx], y_encoded[test_idx]\n",
    "    file_ids_train_val = file_ids[train_val_idx]\n",
    "    \n",
    "    # âœ… KEY FIX: Split train/val by files, not segments\n",
    "    unique_train_val_files = np.unique(file_ids_train_val)\n",
    "    train_val_file_labels = np.array([file_id_to_label[fid] for fid in unique_train_val_files])\n",
    "    \n",
    "    train_files, val_files = train_test_split(\n",
    "        unique_train_val_files, \n",
    "        test_size=0.15, \n",
    "        stratify=train_val_file_labels, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_mask = np.isin(file_ids_train_val, train_files)\n",
    "    val_mask = np.isin(file_ids_train_val, val_files)\n",
    "    \n",
    "    X_train = X_train_val[train_mask]\n",
    "    y_train = y_train_val[train_mask]\n",
    "    X_val = X_train_val[val_mask]\n",
    "    y_val = y_train_val[val_mask]\n",
    "\n",
    "    print(f\"Train files: {len(train_files)}, Val files: {len(val_files)}, Test files: {len(np.unique(file_ids[test_idx]))}\")\n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    print(f\"Test set distribution: {np.unique(y_test, return_counts=True)}\")\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(input_shape=(X.shape[1], 1))\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=5e-5),\n",
    "        loss=hybrid_focal_loss(alpha=0.7, gamma=1.5, focal_weight=0.5),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=25, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1, \n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.6,\n",
    "        patience=8, \n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"results/model_fold{fold_no}.weights.h5\", \n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True,\n",
    "        verbose=0,  \n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    callbacks = [early_stop, reduce_lr, checkpoint]\n",
    "    \n",
    "    # Train Model\n",
    "    print(f\"\\nðŸš€ Training Fold {fold_no}...\")\n",
    "    \n",
    "    if USE_DATA_AUGMENTATION:\n",
    "        print(\"ðŸ“Š Using data augmentation with real-time generator\")\n",
    "        \n",
    "        # Create data generators\n",
    "        train_generator = AugmentedDataGenerator(\n",
    "            X_train, y_train, \n",
    "            batch_size=12, \n",
    "            augmentation_prob=0.55,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Note: Validation data is NOT augmented\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=200,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        print(\"ðŸ“Š Training without data augmentation\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=150,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_weights(f\"results/model_fold{fold_no}.weights.h5\")\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # AUC Score\n",
    "    test_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    auc_per_fold.append(test_auc)\n",
    "    \n",
    "    print(f\"\\nâœ… Fold {fold_no} Results:\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "    \n",
    "    # Classification Metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store metrics\n",
    "    fold_metrics['fold_no'].append(fold_no)\n",
    "    fold_metrics['test_acc'].append(test_acc)\n",
    "    fold_metrics['test_loss'].append(test_loss)\n",
    "    fold_metrics['test_auc'].append(test_auc)\n",
    "    fold_metrics['val_acc'].append(max(history.history['val_accuracy']))\n",
    "    fold_metrics['train_acc'].append(max(history.history['accuracy']))\n",
    "    fold_metrics['f1_score'].append(f1)\n",
    "    fold_metrics['precision'].append(precision)\n",
    "    fold_metrics['recall'].append(recall)\n",
    "    \n",
    "    # Check if this is the best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_fold = fold_no\n",
    "        best_model = model\n",
    "        best_cm = cm\n",
    "        best_y_test = y_test\n",
    "        best_y_pred = y_pred\n",
    "        best_y_pred_prob = y_pred_prob\n",
    "        print(f\"ðŸŒŸ New best model! Fold {fold_no} with accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_no} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                annot_kws={'size': 14})\n",
    "    plt.title(f\"Fold {fold_no} Confusion Matrix\\nAcc: {test_acc:.4f} | AUC: {test_auc:.4f}\", \n",
    "              fontsize=14)\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/confusion_fold{fold_no}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Training History \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    plt.axhline(y=test_acc, color='r', linestyle='--', label=f'Test Acc: {test_acc:.4f}')\n",
    "    plt.title(f'Fold {fold_no} - Model Accuracy', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Accuracy', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    plt.title(f'Fold {fold_no} - Model Loss', fontsize=13)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.ylabel('Loss', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/training_history_fold{fold_no}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===================== SUMMARY =====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Mean Test Accuracy across folds: {np.mean(acc_per_fold):.4f} (Â±{np.std(acc_per_fold):.4f})\")\n",
    "print(f\"ðŸ“Š Mean Test AUC across folds: {np.mean(auc_per_fold):.4f} (Â±{np.std(auc_per_fold):.4f})\")\n",
    "\n",
    "# Combine confusion matrices\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(total_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            annot_kws={'size': 14})\n",
    "plt.title(\"Overall Confusion Matrix (All Folds)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/confusion_overall.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Print Fold Metrics\n",
    "print(\"\\nðŸ“‹ Fold-wise Metrics Summary:\")\n",
    "for i in range(len(fold_metrics['fold_no'])):\n",
    "    print(f\"\\nðŸ”¸ Fold {fold_metrics['fold_no'][i]} Metrics:\")\n",
    "    print(f\"  Train Accuracy : {fold_metrics['train_acc'][i]:.4f}\")\n",
    "    print(f\"  Val Accuracy   : {fold_metrics['val_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Accuracy  : {fold_metrics['test_acc'][i]:.4f}\")\n",
    "    print(f\"  Test Loss      : {fold_metrics['test_loss'][i]:.4f}\")\n",
    "    print(f\"  Precision      : {fold_metrics['precision'][i]:.4f}\")\n",
    "    print(f\"  Recall         : {fold_metrics['recall'][i]:.4f}\")\n",
    "    print(f\"  F1 Score       : {fold_metrics['f1_score'][i]:.4f}\")\n",
    "    print(f\"  Test AUC       : {fold_metrics['test_auc'][i]:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "best_model.save(\"results/best_model.keras\")\n",
    "print(f\"\\nðŸ’¾ Best model (Fold {best_fold}) saved as 'results/best_model.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
