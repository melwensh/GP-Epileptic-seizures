{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37df3167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 1/5\n",
      "============================================================\n",
      "Labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Labels - Test: (array([0, 1, 2], dtype=int32), array([320, 320, 160]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">431</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">431</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">427</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">427</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">213</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">213</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">211</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">211</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m431\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m431\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m427\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚        \u001b[38;5;34m10,304\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m427\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m213\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m213\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m211\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m24,704\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m211\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m49,408\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m4,160\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚           \u001b[38;5;34m195\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,923</span> (351.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m89,923\u001b[0m (351.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,475</span> (349.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m89,475\u001b[0m (349.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int32(0): np.float64(0.8333333333333334), np.int32(1): np.float64(0.8333333333333334), np.int32(2): np.float64(1.6666666666666667)}\n",
      "\n",
      "ğŸš€ Training Fold 1...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 127ms/step - accuracy: 0.5626 - loss: 0.9840 - val_accuracy: 0.4018 - val_loss: 1.3121\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.7063 - loss: 0.8011 - val_accuracy: 0.4619 - val_loss: 1.2319\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.7509 - loss: 0.6754 - val_accuracy: 0.7469 - val_loss: 0.6755\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.8180 - loss: 0.5269 - val_accuracy: 0.8673 - val_loss: 0.4082\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.8935 - loss: 0.3428 - val_accuracy: 0.9150 - val_loss: 0.2690\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9152 - loss: 0.2661 - val_accuracy: 0.9327 - val_loss: 0.2166\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9209 - loss: 0.2513 - val_accuracy: 0.9310 - val_loss: 0.2084\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9239 - loss: 0.2252 - val_accuracy: 0.9310 - val_loss: 0.2376\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 119ms/step - accuracy: 0.9315 - loss: 0.2054 - val_accuracy: 0.9469 - val_loss: 0.1814\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9324 - loss: 0.1858 - val_accuracy: 0.9416 - val_loss: 0.2015\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9366 - loss: 0.1726 - val_accuracy: 0.9310 - val_loss: 0.2263\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9366 - loss: 0.1760 - val_accuracy: 0.9345 - val_loss: 0.2477\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 102ms/step - accuracy: 0.9332 - loss: 0.1791 - val_accuracy: 0.9434 - val_loss: 0.2006\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 0.9385 - loss: 0.1646 - val_accuracy: 0.9540 - val_loss: 0.1766\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9374 - loss: 0.1630 - val_accuracy: 0.9487 - val_loss: 0.1978\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9446 - loss: 0.1506 - val_accuracy: 0.9469 - val_loss: 0.2002\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 149ms/step - accuracy: 0.9454 - loss: 0.1417 - val_accuracy: 0.9504 - val_loss: 0.1864\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 163ms/step - accuracy: 0.9425 - loss: 0.1422 - val_accuracy: 0.9504 - val_loss: 0.1797\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 165ms/step - accuracy: 0.9408 - loss: 0.1442 - val_accuracy: 0.9522 - val_loss: 0.1679\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 148ms/step - accuracy: 0.9499 - loss: 0.1313 - val_accuracy: 0.9593 - val_loss: 0.1725\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9440 - loss: 0.1408 - val_accuracy: 0.9434 - val_loss: 0.2192\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9467 - loss: 0.1244 - val_accuracy: 0.9504 - val_loss: 0.1917\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9425 - loss: 0.1410 - val_accuracy: 0.9558 - val_loss: 0.1391\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9488 - loss: 0.1201 - val_accuracy: 0.9434 - val_loss: 0.2371\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 127ms/step - accuracy: 0.9495 - loss: 0.1359 - val_accuracy: 0.9451 - val_loss: 0.1844\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9512 - loss: 0.1257 - val_accuracy: 0.9611 - val_loss: 0.1392\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9514 - loss: 0.1163 - val_accuracy: 0.9469 - val_loss: 0.2072\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9516 - loss: 0.1234 - val_accuracy: 0.9487 - val_loss: 0.2121\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 128ms/step - accuracy: 0.9528 - loss: 0.1124 - val_accuracy: 0.9522 - val_loss: 0.1914\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9505 - loss: 0.1184 - val_accuracy: 0.9611 - val_loss: 0.1433\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.9522 - loss: 0.1057 - val_accuracy: 0.9664 - val_loss: 0.1303\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9514 - loss: 0.1158 - val_accuracy: 0.9558 - val_loss: 0.1711\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9598 - loss: 0.0990 - val_accuracy: 0.9593 - val_loss: 0.1655\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 129ms/step - accuracy: 0.9596 - loss: 0.0984 - val_accuracy: 0.9558 - val_loss: 0.1815\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9565 - loss: 0.1033 - val_accuracy: 0.9487 - val_loss: 0.1781\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 137ms/step - accuracy: 0.9590 - loss: 0.1012 - val_accuracy: 0.9575 - val_loss: 0.1724\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.9598 - loss: 0.0981 - val_accuracy: 0.9664 - val_loss: 0.1129\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 99ms/step - accuracy: 0.9581 - loss: 0.0990 - val_accuracy: 0.9628 - val_loss: 0.1487\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.9573 - loss: 0.1043 - val_accuracy: 0.9628 - val_loss: 0.1298\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9674 - loss: 0.0798 - val_accuracy: 0.9593 - val_loss: 0.1577\n",
      "\n",
      "âœ… Fold 1 - Test Accuracy: 0.9413\n",
      "ğŸ’¾ Weights saved to results_3class\\cnn_lstm_fold1.weights.h5\n",
      "\n",
      "ğŸ“‹ Classification Report - Fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL     1.0000    0.9219    0.9593       320\n",
      "  INTERICTAL     0.8719    1.0000    0.9316       320\n",
      "       ICTAL     1.0000    0.8625    0.9262       160\n",
      "\n",
      "    accuracy                         0.9413       800\n",
      "   macro avg     0.9573    0.9281    0.9390       800\n",
      "weighted avg     0.9488    0.9413    0.9416       800\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 2/5\n",
      "============================================================\n",
      "Labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Labels - Test: (array([0, 1, 2], dtype=int32), array([320, 320, 160]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int32(0): np.float64(0.8333333333333334), np.int32(1): np.float64(0.8333333333333334), np.int32(2): np.float64(1.6666666666666667)}\n",
      "\n",
      "ğŸš€ Training Fold 2...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 136ms/step - accuracy: 0.5342 - loss: 0.9910 - val_accuracy: 0.6142 - val_loss: 0.9903\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 124ms/step - accuracy: 0.7099 - loss: 0.7756 - val_accuracy: 0.6531 - val_loss: 0.8047\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.7600 - loss: 0.6448 - val_accuracy: 0.7451 - val_loss: 0.6199\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.8416 - loss: 0.4721 - val_accuracy: 0.8053 - val_loss: 0.5250\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.8884 - loss: 0.3426 - val_accuracy: 0.8442 - val_loss: 0.4675\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9201 - loss: 0.2513 - val_accuracy: 0.8708 - val_loss: 0.3908\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9205 - loss: 0.2370 - val_accuracy: 0.8850 - val_loss: 0.3482\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.9368 - loss: 0.1955 - val_accuracy: 0.8991 - val_loss: 0.3228\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9366 - loss: 0.1893 - val_accuracy: 0.9009 - val_loss: 0.3160\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 139ms/step - accuracy: 0.9427 - loss: 0.1682 - val_accuracy: 0.8796 - val_loss: 0.4129\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9398 - loss: 0.1799 - val_accuracy: 0.8920 - val_loss: 0.3763\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9467 - loss: 0.1523 - val_accuracy: 0.8850 - val_loss: 0.4124\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 123ms/step - accuracy: 0.9389 - loss: 0.1589 - val_accuracy: 0.8885 - val_loss: 0.4170\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9444 - loss: 0.1501 - val_accuracy: 0.8903 - val_loss: 0.3807\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 134ms/step - accuracy: 0.9454 - loss: 0.1447 - val_accuracy: 0.8920 - val_loss: 0.3722\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 126ms/step - accuracy: 0.9495 - loss: 0.1409 - val_accuracy: 0.8956 - val_loss: 0.4038\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9495 - loss: 0.1389 - val_accuracy: 0.8761 - val_loss: 0.4886\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9520 - loss: 0.1412 - val_accuracy: 0.8779 - val_loss: 0.4167\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 138ms/step - accuracy: 0.9467 - loss: 0.1482 - val_accuracy: 0.8920 - val_loss: 0.3170\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 138ms/step - accuracy: 0.9529 - loss: 0.1260 - val_accuracy: 0.8956 - val_loss: 0.3390\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 136ms/step - accuracy: 0.9499 - loss: 0.1284 - val_accuracy: 0.8920 - val_loss: 0.3532\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.9556 - loss: 0.1124 - val_accuracy: 0.9044 - val_loss: 0.3723\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 137ms/step - accuracy: 0.9543 - loss: 0.1173 - val_accuracy: 0.8973 - val_loss: 0.3381\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 140ms/step - accuracy: 0.9594 - loss: 0.1076 - val_accuracy: 0.8920 - val_loss: 0.3552\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9590 - loss: 0.1098 - val_accuracy: 0.9027 - val_loss: 0.3199\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 138ms/step - accuracy: 0.9584 - loss: 0.1050 - val_accuracy: 0.8956 - val_loss: 0.3693\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9583 - loss: 0.1084 - val_accuracy: 0.8956 - val_loss: 0.3122\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9603 - loss: 0.0938 - val_accuracy: 0.9062 - val_loss: 0.3017\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.9619 - loss: 0.1006 - val_accuracy: 0.8814 - val_loss: 0.4189\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 113ms/step - accuracy: 0.9581 - loss: 0.1087 - val_accuracy: 0.8832 - val_loss: 0.3866\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 110ms/step - accuracy: 0.9596 - loss: 0.0930 - val_accuracy: 0.9168 - val_loss: 0.2971\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.9598 - loss: 0.0986 - val_accuracy: 0.9097 - val_loss: 0.3254\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 0.9605 - loss: 0.0996 - val_accuracy: 0.8938 - val_loss: 0.3703\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 114ms/step - accuracy: 0.9647 - loss: 0.0841 - val_accuracy: 0.8938 - val_loss: 0.3503\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 110ms/step - accuracy: 0.9592 - loss: 0.1010 - val_accuracy: 0.9027 - val_loss: 0.3586\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.9647 - loss: 0.0844 - val_accuracy: 0.9292 - val_loss: 0.2126\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 81ms/step - accuracy: 0.9634 - loss: 0.0950 - val_accuracy: 0.9009 - val_loss: 0.3367\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9645 - loss: 0.0841 - val_accuracy: 0.8938 - val_loss: 0.3769\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 106ms/step - accuracy: 0.9620 - loss: 0.0899 - val_accuracy: 0.9150 - val_loss: 0.2266\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.9679 - loss: 0.0755 - val_accuracy: 0.9062 - val_loss: 0.3490\n",
      "\n",
      "âœ… Fold 2 - Test Accuracy: 0.8925\n",
      "ğŸ’¾ Weights saved to results_3class\\cnn_lstm_fold2.weights.h5\n",
      "\n",
      "ğŸ“‹ Classification Report - Fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL     0.9962    0.8281    0.9044       320\n",
      "  INTERICTAL     0.7901    1.0000    0.8828       320\n",
      "       ICTAL     1.0000    0.8063    0.8927       160\n",
      "\n",
      "    accuracy                         0.8925       800\n",
      "   macro avg     0.9288    0.8781    0.8933       800\n",
      "weighted avg     0.9145    0.8925    0.8934       800\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 3/5\n",
      "============================================================\n",
      "Labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Labels - Test: (array([0, 1, 2], dtype=int32), array([320, 320, 160]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int32(0): np.float64(0.8333333333333334), np.int32(1): np.float64(0.8333333333333334), np.int32(2): np.float64(1.6666666666666667)}\n",
      "\n",
      "ğŸš€ Training Fold 3...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 130ms/step - accuracy: 0.5598 - loss: 1.0113 - val_accuracy: 0.4053 - val_loss: 1.0528\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.7072 - loss: 0.8184 - val_accuracy: 0.5664 - val_loss: 0.9096\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.7761 - loss: 0.6637 - val_accuracy: 0.7363 - val_loss: 0.6812\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 101ms/step - accuracy: 0.8509 - loss: 0.4714 - val_accuracy: 0.8566 - val_loss: 0.4530\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9076 - loss: 0.3152 - val_accuracy: 0.9221 - val_loss: 0.2975\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9252 - loss: 0.2580 - val_accuracy: 0.9416 - val_loss: 0.2057\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.9307 - loss: 0.2198 - val_accuracy: 0.9451 - val_loss: 0.2140\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 135ms/step - accuracy: 0.9380 - loss: 0.2020 - val_accuracy: 0.9469 - val_loss: 0.1926\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9400 - loss: 0.1942 - val_accuracy: 0.9416 - val_loss: 0.2262\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9361 - loss: 0.1997 - val_accuracy: 0.9540 - val_loss: 0.1759\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9444 - loss: 0.1676 - val_accuracy: 0.9451 - val_loss: 0.2048\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9436 - loss: 0.1735 - val_accuracy: 0.9310 - val_loss: 0.2499\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.9497 - loss: 0.1530 - val_accuracy: 0.9186 - val_loss: 0.2676\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 0.9421 - loss: 0.1632 - val_accuracy: 0.9239 - val_loss: 0.2745\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 129ms/step - accuracy: 0.9512 - loss: 0.1453 - val_accuracy: 0.9398 - val_loss: 0.2257\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9505 - loss: 0.1388 - val_accuracy: 0.9398 - val_loss: 0.2204\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.9531 - loss: 0.1394 - val_accuracy: 0.9363 - val_loss: 0.2368\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9539 - loss: 0.1300 - val_accuracy: 0.9168 - val_loss: 0.3103\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.9484 - loss: 0.1397 - val_accuracy: 0.9540 - val_loss: 0.1944\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 106ms/step - accuracy: 0.9501 - loss: 0.1277 - val_accuracy: 0.9451 - val_loss: 0.2104\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 107ms/step - accuracy: 0.9514 - loss: 0.1270 - val_accuracy: 0.9310 - val_loss: 0.2776\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.9528 - loss: 0.1224 - val_accuracy: 0.9434 - val_loss: 0.2277\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9550 - loss: 0.1135 - val_accuracy: 0.9416 - val_loss: 0.2380\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9565 - loss: 0.1118 - val_accuracy: 0.9363 - val_loss: 0.2569\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9535 - loss: 0.1133 - val_accuracy: 0.9150 - val_loss: 0.3657\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9565 - loss: 0.1082 - val_accuracy: 0.9327 - val_loss: 0.2614\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9571 - loss: 0.1111 - val_accuracy: 0.9257 - val_loss: 0.2861\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9560 - loss: 0.1178 - val_accuracy: 0.9469 - val_loss: 0.2152\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9590 - loss: 0.1009 - val_accuracy: 0.9363 - val_loss: 0.2502\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.9603 - loss: 0.0970 - val_accuracy: 0.9239 - val_loss: 0.2886\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9638 - loss: 0.0906 - val_accuracy: 0.9274 - val_loss: 0.2934\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9581 - loss: 0.1050 - val_accuracy: 0.9434 - val_loss: 0.2141\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9600 - loss: 0.0978 - val_accuracy: 0.9540 - val_loss: 0.2061\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9651 - loss: 0.0905 - val_accuracy: 0.9310 - val_loss: 0.2702\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9676 - loss: 0.0831 - val_accuracy: 0.9487 - val_loss: 0.2230\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9624 - loss: 0.0866 - val_accuracy: 0.9434 - val_loss: 0.2407\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9647 - loss: 0.0891 - val_accuracy: 0.9381 - val_loss: 0.2499\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9624 - loss: 0.0906 - val_accuracy: 0.9398 - val_loss: 0.2178\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.9685 - loss: 0.0758 - val_accuracy: 0.9363 - val_loss: 0.2334\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.9655 - loss: 0.0859 - val_accuracy: 0.9363 - val_loss: 0.2335\n",
      "\n",
      "âœ… Fold 3 - Test Accuracy: 0.9525\n",
      "ğŸ’¾ Weights saved to results_3class\\cnn_lstm_fold3.weights.h5\n",
      "\n",
      "ğŸ“‹ Classification Report - Fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL     0.9834    0.9281    0.9550       320\n",
      "  INTERICTAL     0.9052    0.9844    0.9431       320\n",
      "       ICTAL     1.0000    0.9375    0.9677       160\n",
      "\n",
      "    accuracy                         0.9525       800\n",
      "   macro avg     0.9629    0.9500    0.9553       800\n",
      "weighted avg     0.9554    0.9525    0.9528       800\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 4/5\n",
      "============================================================\n",
      "Labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Labels - Test: (array([0, 1, 2], dtype=int32), array([320, 320, 160]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int32(0): np.float64(0.8333333333333334), np.int32(1): np.float64(0.8333333333333334), np.int32(2): np.float64(1.6666666666666667)}\n",
      "\n",
      "ğŸš€ Training Fold 4...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 99ms/step - accuracy: 0.5797 - loss: 0.9908 - val_accuracy: 0.4000 - val_loss: 1.2560\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.7017 - loss: 0.8097 - val_accuracy: 0.4726 - val_loss: 1.1699\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.7361 - loss: 0.7183 - val_accuracy: 0.6832 - val_loss: 0.7304\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 135ms/step - accuracy: 0.7951 - loss: 0.5974 - val_accuracy: 0.7947 - val_loss: 0.5448\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 133ms/step - accuracy: 0.8702 - loss: 0.4271 - val_accuracy: 0.8531 - val_loss: 0.4489\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 133ms/step - accuracy: 0.9087 - loss: 0.3133 - val_accuracy: 0.8973 - val_loss: 0.3378\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9182 - loss: 0.2722 - val_accuracy: 0.9062 - val_loss: 0.3038\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9156 - loss: 0.2545 - val_accuracy: 0.8956 - val_loss: 0.3553\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 130ms/step - accuracy: 0.9317 - loss: 0.2165 - val_accuracy: 0.8832 - val_loss: 0.3866\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 130ms/step - accuracy: 0.9324 - loss: 0.2156 - val_accuracy: 0.8920 - val_loss: 0.3759\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9370 - loss: 0.1931 - val_accuracy: 0.8973 - val_loss: 0.3492\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 148ms/step - accuracy: 0.9362 - loss: 0.1933 - val_accuracy: 0.9080 - val_loss: 0.3401\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 134ms/step - accuracy: 0.9366 - loss: 0.1840 - val_accuracy: 0.9097 - val_loss: 0.3326\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 135ms/step - accuracy: 0.9351 - loss: 0.1778 - val_accuracy: 0.9044 - val_loss: 0.3224\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 136ms/step - accuracy: 0.9378 - loss: 0.1673 - val_accuracy: 0.8850 - val_loss: 0.4164\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 137ms/step - accuracy: 0.9342 - loss: 0.1752 - val_accuracy: 0.9062 - val_loss: 0.3316\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 142ms/step - accuracy: 0.9431 - loss: 0.1527 - val_accuracy: 0.8973 - val_loss: 0.3639\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 134ms/step - accuracy: 0.9440 - loss: 0.1459 - val_accuracy: 0.8991 - val_loss: 0.3631\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 137ms/step - accuracy: 0.9402 - loss: 0.1524 - val_accuracy: 0.9080 - val_loss: 0.3352\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 134ms/step - accuracy: 0.9423 - loss: 0.1453 - val_accuracy: 0.9080 - val_loss: 0.3279\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 134ms/step - accuracy: 0.9461 - loss: 0.1462 - val_accuracy: 0.9097 - val_loss: 0.3135\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 137ms/step - accuracy: 0.9482 - loss: 0.1279 - val_accuracy: 0.9221 - val_loss: 0.2875\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 134ms/step - accuracy: 0.9471 - loss: 0.1263 - val_accuracy: 0.8938 - val_loss: 0.3746\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 141ms/step - accuracy: 0.9452 - loss: 0.1293 - val_accuracy: 0.9239 - val_loss: 0.2834\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 140ms/step - accuracy: 0.9472 - loss: 0.1297 - val_accuracy: 0.9150 - val_loss: 0.3233\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 137ms/step - accuracy: 0.9499 - loss: 0.1226 - val_accuracy: 0.8920 - val_loss: 0.3892\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 136ms/step - accuracy: 0.9545 - loss: 0.1098 - val_accuracy: 0.9115 - val_loss: 0.3089\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9522 - loss: 0.1186 - val_accuracy: 0.9150 - val_loss: 0.3002\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 137ms/step - accuracy: 0.9564 - loss: 0.1130 - val_accuracy: 0.8973 - val_loss: 0.3661\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 133ms/step - accuracy: 0.9493 - loss: 0.1124 - val_accuracy: 0.9027 - val_loss: 0.3480\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 133ms/step - accuracy: 0.9556 - loss: 0.1130 - val_accuracy: 0.9257 - val_loss: 0.2684\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9552 - loss: 0.1032 - val_accuracy: 0.9062 - val_loss: 0.3295\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9556 - loss: 0.1119 - val_accuracy: 0.9257 - val_loss: 0.2988\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 107ms/step - accuracy: 0.9543 - loss: 0.1114 - val_accuracy: 0.9044 - val_loss: 0.3657\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9556 - loss: 0.1053 - val_accuracy: 0.9257 - val_loss: 0.2711\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9488 - loss: 0.1188 - val_accuracy: 0.9062 - val_loss: 0.3727\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 135ms/step - accuracy: 0.9579 - loss: 0.0990 - val_accuracy: 0.9186 - val_loss: 0.2882\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 135ms/step - accuracy: 0.9613 - loss: 0.1068 - val_accuracy: 0.9469 - val_loss: 0.1789\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 135ms/step - accuracy: 0.9607 - loss: 0.0966 - val_accuracy: 0.9363 - val_loss: 0.1985\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.9594 - loss: 0.0913 - val_accuracy: 0.9451 - val_loss: 0.2084\n",
      "\n",
      "âœ… Fold 4 - Test Accuracy: 0.9375\n",
      "ğŸ’¾ Weights saved to results_3class\\cnn_lstm_fold4.weights.h5\n",
      "\n",
      "ğŸ“‹ Classification Report - Fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL     0.9781    0.9781    0.9781       320\n",
      "  INTERICTAL     0.8799    0.9844    0.9292       320\n",
      "       ICTAL     1.0000    0.7625    0.8652       160\n",
      "\n",
      "    accuracy                         0.9375       800\n",
      "   macro avg     0.9527    0.9083    0.9242       800\n",
      "weighted avg     0.9432    0.9375    0.9360       800\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 5/5\n",
      "============================================================\n",
      "Labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Labels - Test: (array([0, 1, 2], dtype=int32), array([320, 320, 160]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {np.int32(0): np.float64(0.8333333333333334), np.int32(1): np.float64(0.8333333333333334), np.int32(2): np.float64(1.6666666666666667)}\n",
      "\n",
      "ğŸš€ Training Fold 5...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 152ms/step - accuracy: 0.5723 - loss: 0.9862 - val_accuracy: 0.4000 - val_loss: 1.0760\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.7129 - loss: 0.7752 - val_accuracy: 0.4938 - val_loss: 0.9768\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.7740 - loss: 0.6384 - val_accuracy: 0.7274 - val_loss: 0.6470\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.8404 - loss: 0.4720 - val_accuracy: 0.8265 - val_loss: 0.5015\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.8960 - loss: 0.3314 - val_accuracy: 0.8991 - val_loss: 0.3356\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9290 - loss: 0.2313 - val_accuracy: 0.9044 - val_loss: 0.3182\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9347 - loss: 0.2044 - val_accuracy: 0.9186 - val_loss: 0.2679\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9408 - loss: 0.1747 - val_accuracy: 0.9133 - val_loss: 0.2828\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9478 - loss: 0.1522 - val_accuracy: 0.9186 - val_loss: 0.2766\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9484 - loss: 0.1501 - val_accuracy: 0.9062 - val_loss: 0.2870\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9531 - loss: 0.1301 - val_accuracy: 0.9221 - val_loss: 0.2622\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9550 - loss: 0.1317 - val_accuracy: 0.9062 - val_loss: 0.2890\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.9550 - loss: 0.1226 - val_accuracy: 0.9097 - val_loss: 0.3020\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9575 - loss: 0.1233 - val_accuracy: 0.9027 - val_loss: 0.3133\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 133ms/step - accuracy: 0.9564 - loss: 0.1143 - val_accuracy: 0.9097 - val_loss: 0.2860\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9546 - loss: 0.1132 - val_accuracy: 0.9204 - val_loss: 0.2510\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9554 - loss: 0.1111 - val_accuracy: 0.9133 - val_loss: 0.2680\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9617 - loss: 0.0889 - val_accuracy: 0.9257 - val_loss: 0.2389\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9639 - loss: 0.1014 - val_accuracy: 0.9221 - val_loss: 0.2430\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9653 - loss: 0.0918 - val_accuracy: 0.9186 - val_loss: 0.2943\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9676 - loss: 0.0842 - val_accuracy: 0.9150 - val_loss: 0.2979\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9655 - loss: 0.0854 - val_accuracy: 0.9044 - val_loss: 0.3601\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9636 - loss: 0.0912 - val_accuracy: 0.9204 - val_loss: 0.2501\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9712 - loss: 0.0793 - val_accuracy: 0.9310 - val_loss: 0.2710\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9708 - loss: 0.0765 - val_accuracy: 0.9097 - val_loss: 0.3101\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9715 - loss: 0.0778 - val_accuracy: 0.9150 - val_loss: 0.2980\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9700 - loss: 0.0754 - val_accuracy: 0.9150 - val_loss: 0.2958\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 120ms/step - accuracy: 0.9702 - loss: 0.0703 - val_accuracy: 0.9133 - val_loss: 0.3121\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9744 - loss: 0.0667 - val_accuracy: 0.9257 - val_loss: 0.2868\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 104ms/step - accuracy: 0.9727 - loss: 0.0710 - val_accuracy: 0.9027 - val_loss: 0.3457\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9710 - loss: 0.0750 - val_accuracy: 0.9292 - val_loss: 0.2780\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9763 - loss: 0.0674 - val_accuracy: 0.9345 - val_loss: 0.2613\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9748 - loss: 0.0599 - val_accuracy: 0.9310 - val_loss: 0.2553\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9727 - loss: 0.0738 - val_accuracy: 0.9133 - val_loss: 0.2964\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.9729 - loss: 0.0741 - val_accuracy: 0.9168 - val_loss: 0.2872\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9763 - loss: 0.0573 - val_accuracy: 0.9080 - val_loss: 0.3258\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.9740 - loss: 0.0694 - val_accuracy: 0.9115 - val_loss: 0.3174\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.9750 - loss: 0.0594 - val_accuracy: 0.9168 - val_loss: 0.3078\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.9770 - loss: 0.0558 - val_accuracy: 0.8956 - val_loss: 0.3368\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - accuracy: 0.9750 - loss: 0.0649 - val_accuracy: 0.8956 - val_loss: 0.3665\n",
      "\n",
      "âœ… Fold 5 - Test Accuracy: 0.8712\n",
      "ğŸ’¾ Weights saved to results_3class\\cnn_lstm_fold5.weights.h5\n",
      "\n",
      "ğŸ“‹ Classification Report - Fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL     0.9396    0.7781    0.8513       320\n",
      "  INTERICTAL     0.7866    0.9906    0.8769       320\n",
      "       ICTAL     0.9924    0.8187    0.8973       160\n",
      "\n",
      "    accuracy                         0.8712       800\n",
      "   macro avg     0.9062    0.8625    0.8751       800\n",
      "weighted avg     0.8890    0.8712    0.8707       800\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š FINAL RESULTS\n",
      "============================================================\n",
      "Mean Accuracy across all folds: 0.9190 Â± 0.0314\n",
      "Accuracy per fold: ['0.9413', '0.8925', '0.9525', '0.9375', '0.8712']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAu8AAAJOCAYAAAAHw+kaAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZtpJREFUeJzt3Qd4FGX38OGTUEIzVOlVpXcB6U2QJk3gFaSqCIKAIEWISBVBUakvgo0ioiIKiKAgUgQBqSJKCaAgRXrvLftd53m/3f9usoEsu9nd2fxurzHZmdnZZyeTcObsmTNhNpvNJgAAAACCXnigBwAAAAAgYQjeAQAAAIsgeAcAAAAsguAdAAAAsAiCdwAAAMAiCN4BAAAAiyB4BwAAACyC4B0AAACwCIJ3AAAAwCII3oEQt3r1agkLCzNf7Z599lnJnz+/WMHs2bOlSJEikiJFCsmQIYPPtz98+HCzf/A/Bw8eNPtj5syZPt0lY8eONT/HmJiY+3q+u2NWx6k/v2DcN/fzO7Z06VJJly6dnDp1yotRAgh1BO/APezcuVPat28vuXLlkoiICMmZM6e0a9fOzE8qFixYIA0bNpQsWbJIypQpzT54+umnZeXKlYn6unv27DFB0MMPPywfffSRfPjhhxJKNBDU6YUXXnC7fPDgwY51Tp8+7fH2v//+e78Gt/G5ePGivP322zJw4EAJD4/7z8758+clVapU5n3u3r07UQJud1OlSpUkmDRo0EAeeeQRGTNmTKCHAiCIJQ/0AIBgNn/+fHnmmWckU6ZM0rlzZylQoIAJBj755BP5+uuv5csvv5SnnnpKQpXNZpPnn3/eZBrLli0rffv2lezZs8uxY8dMQF+nTh1Zt26dVKlSJVFeXz8t0EztxIkTTVCTGF5//XUZNGiQBIoGrd988428//775sTI2RdffGGWX79+/b62rcH7lClTPArg8+XLJ9euXTOfdPjK9OnT5fbt2+Z3yZ158+aZYFqPrTlz5sioUaPE1/S1GzVq5DLvwQcflGDz4osvSv/+/WXEiBHywAMPBHo4AIIQwTsQj7/++ks6dOggDz30kKxZs8blH/revXtL9erVzfIdO3aYdfzlypUrkjZtWr+81nvvvWcC9z59+si4ceNcyks0K6wlLcmTJ96fkZMnT5qviVEuY6fjT8z3kJBs66JFi+SHH36QZs2aOeavX79eDhw4IC1btjTBfWLT4FpPlPQEQk8YfGnGjBnStGnTeLf72WefmcBaTxw+//zzRAneH330UfMJWrDTn3evXr3MCY2eOANAbJTNAPF455135OrVq6ZUI3aGTstHPvjgAxNIay2v0ky8Brc///xznG3purrszz//dCkJadWqlcnqa1BTvnx5E8Q508DZvs2XXnpJsmbNKrlz5zbL/vnnHzOvcOHCkjp1asmcObP85z//MZ8M+IJmX/Xje61Tfvfdd93WhevJy2OPPeZ4/Pfff5sx6HtKkyaNKUtYsmSJ2xr8r776St58803zfvT9axZ///79jvW0XnjYsGHme93/zvXN8dU663O0zMbu1q1bJoNZsGBB8xq6j6pVqybLly+/a827BrJvvPGGKdfRUind7muvvSY3btyI83qNGzeWX375xewHfQ09kfv0008TvJ+1HKtGjRomaHWmGeiSJUtKiRIl4jxn7dq1Zj/nzZvXjC9PnjzyyiuvmJ+Zne4Hzbrb95d9ci4l0Z/rhAkTHO9z165dceq69QRK93+tWrXMJzF2+rPSk8jWrVvf9f3pCYie4NatW9ft8kOHDpn306ZNGzPp+nri4m8JOXbjs3DhQvNz0p+/ftVPpdzRT+rKlStnMuqRkZHm56ufKjnT3/FSpUrJt99+65P3BSD0kHkH4vHdd9+Z4Ewz7O5owKXL7f/AP/nkk+ZiMw1Ka9as6bLu3LlzpXjx4o5ATOvlq1atagI3LdnQIEif17x5c5NljV2Ko0G6BlBDhw41Jwxq8+bNJsjRgEcDYA26pk6daoIsDcI0APGGBqRnz541WfdkyZLdc/0TJ06Y8hk94Xn55ZdNoDxr1iyTcdUTm9jv6a233jL1z1oicOHCBXMSpNcSbNy40SzXoFKDYA2E9H3pvtWgxhMamOsJiNaUa3CttddbtmyRbdu2yRNPPBHv83R9HbueXPXr18+MSbej9dixAzMNYnU9Lavq1KmTKRHRwFmDNP2ZJ0Tbtm3NpzmXL18271NPHjTzqmVK7kpmdJnu5+7du5v9vGnTJpk8ebIcOXLELLOXX/z777/mREU/IYkvI67b79q1qwneNXCNfUGpBpO6/zWw1dfQn62uo+9Rg1At97kbeyCumW93tDRIj389CdKTUD2R0BMXX5di6f6Kfd1A+vTpTXmQp8eusx9//NFky4sVK2aOkTNnzshzzz3nOMm205+Dlu7oSarW/ys9nrTsTH/2zvTY0RMCAHDLBiCO8+fPa4rR1qxZs7vunaZNm5r1Ll68aB4/88wztqxZs9pu377tWOfYsWO28PBw28iRIx3z6tSpYytZsqTt+vXrjnkxMTG2KlWq2AoWLOiYN2PGDLP9atWquWxTXb16Nc54NmzYYNb/9NNPHfNWrVpl5ulXu06dOtny5ct31/c2ceJE87wFCxbYEqJPnz5m/bVr1zrmXbp0yVagQAFb/vz5bXfu3HEZT9GiRW03btyI83p//PGHY96wYcPMvFOnTrm8ls7TZbHpe9L3Zle6dGnbk08+eddx21/Dbvv27ebxCy+84LJe//79zfyVK1e6vJ7OW7NmjWPeyZMnbREREbZ+/frd9XXt76NHjx62s2fP2lKmTGmbPXu2mb9kyRJbWFiY7eDBg273gbuf/ZgxY8xz/vnnH8c83ba7P/MHDhww8yMjI8143S3TY8+ZHttp0qSx7d271/bOO++YdRYuXHjP9/j666+bdfVYcEd/D9q1a+d4/Nprr9myZMliu3Xrlst67o7Z+I4Dd+/H3WT/nUjosetu35QpU8aWI0cO8zfD7scffzTrOY+3d+/eZn/H/j12Z/To0eb5J06cuOe6AJIeymYANy5dumS+3uuCMftyzegqLSHQMgPntoyaudNMpb28QLPZ2qVFu7Xo62g2UCfN2NWvX1/27dsnR48edXmdLl26xMl+a5bSuTxEn68XdWp9uGaWvWV/Twm9aE4vjtTstpal2GkWWbO6+qmAfhrgTLOTzhdo2j/h0PIFX9F9oZ9y6D5NKH0fSrPezjQDr2KXUmjG1fnTGf2EREuZPHkfGTNmNLXvmoVWWkKjmWCtAXfH+Wevn8To8aPrazz722+/Jfh1NWOc0Is2//vf/5pMtX7KMGTIEFMy5VyjHx89LvWaAj0WYtNymj/++MPlQlb9Xt/PsmXLxJf0ONTst/NUunTp+zp27fTC7e3bt5tPXHTf2OmnOnpcxD4W9WflXLJ1t+NB3U+HIQChj+AdcMMesNqD+IQG+RqA6T/iWiZjp9+XKVNGChUq5Ciz0CBLAyANnJwne423/UJNO+1yE5vWN2sZjdY7a8mD1uHrNrTtnpaheEtrchOyD+y0Bl+D1tiKFi3qWO5M67XdBSznzp0TXxk5cqTZH7rvtb54wIABJmC8Gx2nlvPE7m6jnVA0ALvX+7C/F0/fh5bOaGCnNeBaMqGP46PraNmKlrlokKk/d3uplic/e3fHVXz0tSZNmmT2nx7j+r239EJVLZnR6wT090InrRvXcjQtnfElve5B6+6dJ/sx5+mxa2efr9uOLfb2tPRNj0NtuaolNXoxqvZ1d8d+bQH3HwDgDjXvgBsanOTIkeOegZ4u17p1e6CrQbTWrWtdtNYCay2t1rSOHj3a8Rx7TbHWemum3Z3YgaNzptVOO1JozbLWpFeuXNmMWf+x1xr4+70RjjO9UFVpZlTfk6/FV0fvfFGkp+7cuRPnugTtGqQX/2lt8scffyzjx4+XadOmxdtb3S6hgZOv3ofWV+vxo1lcvTBWP5mJ7z1qZlc/wdG+6fpz0gBYP63RgN6Tn7274+pu7NlwPTHR+vqEdAHS+nGt4deTQOdPcXT/6CcNmo2OnaW2n8DarwEIBXrtgGbpdR9qZyGd9Pe3Y8eOpr7emf3ET0/IASA2gncgHnoBnd4YSC/cdP443U47ZOhH6nphoDMtj9F/jFesWGEuSNMgxbkjh72tpF4oF18HjoTQchwN9LSdo51efKiZZl/Q96yZSQ2wtNPKvS5a1RKP6OjoOPO1q459ua/ouGK/z5s3b5oyBncZYy3R0UmDQQ3o9ULW+IJ3HacGwFpqY8+8Kj0R09f05fuIHUjrSZJmo+03xHJHT6b27t1rjjEN/OzclWP4MnOrWWI9+Xn11VdNVlyPPb2Q915tNu0ngdpFxvmCY+2gpCcA+umI8362B69asqKfQPijveP9Hrv2+e7KstxtT8vEmjRpYiY9xjQbr52o9FM45xN23Vf2T9IAIDbKZoB4aImFBlQanGvdrjPNenbr1s10dNH1nGlArgGjlsvopLW0zuUJmoHTjjD6j7a7YDOht0bXYDp2dle7gcTOPt8vfW+a2dUTEP3qLpOsgaZ2OlHap1u/37Bhg2O5ZlW11aaWQbjLrt4v7Uiivfed6evEfu+xf26axdUgKXbLR2f2G/lotxtn2ufe3lUoseinMVo6pcFcfOwnUc4/D/0+dstBZb8fgLcndPp8e8ce/RRJg3i9rsL5E6X46KdCSrv8uCuZ0d8fraN3nvQaDy1F8XXpTHzu99jVT+e0JE5PpJzLlfREKnadfOxjUUuz7CczsY/HrVu3OvYbAMRG5h2IhwYP+o+yti/UeunYd1jVi8k0K62BpDPNqLdo0cL0dNYAQHtpx6b9tzWzrdvVQEWz8ZrZ1eBBs5G///57gj4Z0BaAWi6jwYU+96effjJlCr6igZVe8KnZ/VWrVpnASmu/jx8/brKiGvDYWwFqy0vdH5o11nZ7egKj+0+ziNr+UoMVX9FAUk+e9IJLLSHR/aXlCLGz1bpf9ERJW+/peDSA1E8sevbsGe+29SJGzSpr4KZBq9aS6/vU96KZ8dq1a/vsfbh7bftFlHfLZOsxp4G+lspoyZbuX3c19vq+lf48tERLA38tq/KUtjLU4FOPL92GXtuhPwO9mZJetHq3MeuxrS1S9bn2mw5psKpj1p9dfDdu0jIiPSHR8hk94U1M3hy72h5ST+j091nfn57Y60m0tgnVT3rsdH/psscff9zUvGu9vK6nwb/zJw/6frUcr0ePHon6ngFYWKDb3QDBbseOHaZNnraDS5EihS179uzmsXNLw9iWL19uWr1p677Dhw+7Xeevv/6ydezY0WxPt5srVy5b48aNbV9//XWcVpGbN2+O8/xz587ZnnvuOdNWL126dLb69evb9uzZE6dd4v22inSmY6pXr54tU6ZMtuTJk5t90bp1a9vq1avjvKdWrVrZMmTIYEuVKpXtsccesy1evNhlHft45s2b5zLfXRu++FpFauu+gQMHmveu7Qv1ve/fvz/Oex81apQZg44nderUtiJFitjefPNN282bN+O8hjNtUzhixAjTKlB/Nnny5LFFRUW5tPZU+nruWlHWrFnTTAltFXk37vbBrl27bHXr1jU/d90HXbp0sf3+++9x9p+2JezVq5ftwQcfNMei/X3a97W2fIwt9s/h22+/NY/fe+89l/W0Paq+f23H6bw/3Rk3bpwZq73F5TfffGO2+cknn8T7HD22dB1tIeqLVpHu3qunx258bTT1/WjrU20RWqxYMdv8+fPjjNf+O6StZLUtaN68eW0vvviiaSXrbOrUqeaYtrefBYDYwvR/gT6BAACELi0p0Qy83ohLP8FC/MqWLWs+LdILqwHAHYJ3AECi07uKancVrQX3ZQlVKNGLgrU0Te8RkNilQgCsi+AdAAAAsAjSHwAAAIBFELwDAAAAFkHwDgAAAFgEwTsAAABgEQTvAAAAgEWE5B1Ww7r57jbsgFUdG78s0EMAAi5DSt/dcRiwqlTJ0kgwCXsid6K/hm35EQlVZN4BAAAAiwjJzDsAAACCVFhYoEdgaWTeAQAAAIsg8w4AAAD/IXXsFXYfAAAAYBFk3gEAAOA/1Lx7hcw7AAAAYBFk3gEAAOA/NJvxCpl3AAAAwCLIvAMAAMB/qHn3Cpl3AAAAwCLIvAMAAMB/SB17hd0HAAAAWASZdwAAAPgPNe9eIfMOAAAAWASZdwAAAPgPfd69QuYdAAAAsAgy7wAAAPCfcFLv3iDzDgAAAFgEmXcAAAD4D4l3r5B5BwAAACyCzDsAAAD8hz7vXiHzDgAAAFgEmXcAAAD4DzXvXiHzDgAAAFgEmXcAAAD4D33evULmHQAAALAIMu8AAADwH2revULmHQAAALAIMu8AAADwH/q8e4XMOwAAAGARZN4BAADgP3Sb8QqZdwAAAMAiyLwDAADAf+g24xUy7wAAAIBFkHkHAACA/9Btxitk3gEAAACLIPMOAAAA/6Hm3Stk3gEAAACLIPMOAAAA/6HPu1fIvAMAAAAWQeYdAAAA/kPNu1fIvAMAAAAWQeYdAAAA/kOfd6+QeQcAAAAsgsw7AAAA/IfUsVfYfQAAAIBFkHkHAACA/1Dz7hUy7wAAAIBFkHkHAACA/9Dn3Stk3gEAAACLIPMOAAAA/6Hm3Stk3gEAAACLIPMOAAAA/yF17BV2HwAAAJKsNWvWSJMmTSRnzpwSFhYmCxcujHfdbt26mXUmTJjgMv/s2bPSrl07iYyMlAwZMkjnzp3l8uXLLuvs2LFDqlevLqlSpZI8efLI2LFj72u8BO8AAADwb817Yk8euHLlipQuXVqmTJly1/UWLFggv/76qwnyY9PAfefOnbJ8+XJZvHixOSHo2rWrY/nFixelXr16ki9fPtm6dau88847Mnz4cPnwww8lpMpm9AylfPnycvPmzUAPBQAAACHYKrJhw4ZmupujR49Kr169ZNmyZfLkk0+6LNu9e7csXbpUNm/ebOJWNXnyZGnUqJG8++67JtifM2eOiWenT58uKVOmlOLFi8v27dtl3LhxLkG+5TPvNptN7ty5E+hhAAAAIImKiYmRDh06yIABA0zQHduGDRtMqYw9cFd169aV8PBw2bhxo2OdGjVqmMDdrn79+hIdHS3nzp0Lncw7AAAAQkx44qfeb9y4YSZnERERZvLU22+/LcmTJ5eXX37Z7fLjx49L1qxZXebp+pkyZTLL7OsUKFDAZZ1s2bI5lmXMmDE0Mu8AAACAp8aMGSPp06d3mXSep7Q+feLEiTJz5kxzoWowCGjmXYv37+bSpUt+GwsAAAD8wA9BcFRUlPTt29dl3v1k3deuXSsnT56UvHnzOuZpSXe/fv1Mx5mDBw9K9uzZzTrObt++bTrQ6DKlX0+cOOGyjv2xfR1LBO9aH3S3sxiteQ+WsxwAAABYQ8R9lsjEprXuWr/uTGvVdf5zzz1nHleuXFnOnz9vsvTlypUz81auXGlq5StWrOhYZ/DgwXLr1i1JkSKFmaedaQoXLuxRyUzAg/dVq1YF8uUBAADgb0GWl718+bLs37/f8fjAgQOmE4zWrGvGPXPmzC7ra/Ct2XINvFXRokWlQYMG0qVLF5k2bZoJ0Hv27Clt2rRxtJVs27atjBgxwvR/HzhwoPz555+mHGf8+PEejzegwXvNmjXvuY5+5AAAAAAkhi1btkjt2rUdj+3lNp06dTK17gmhrSA1YK9Tp47pMtOyZUuZNGmSY7nW3P/444/So0cPk53PkiWLDB061OM2kSrMprUpQUjf4McffyzfffedXLt2zaPnhnUrlmjjAqzi2PhlgR4CEHAZUrpmzICkKFWyNBJMwnuXSvTXiJm4Q0JVUHWb+eeff2TYsGGSP39++c9//mPOXD799NNADwsAAAAICgHv8653m5o/f77Jsq9bt85cFHDkyBH57bffpGTJkoEeHgAAAHyIZiQWzrzrbWa1kF8L9p966ikTtGuZjP5QkyVLFsihAQAAAEEnoJn3qVOnmituBw0aJA888EAghwIAAAA/oAu4hTPvs2fPlk2bNkmOHDmkdevWsnjxYtP4HgAAAECQBe/PPPOMaVD/xx9/SJEiRUz7HO2bqU3td+3aFcihAQAAIBGEh4Ul+hTKgqLbTIECBUzjer3F7GeffWZ6Y7Zv315y584tL7/8cqCHBwAAAASFgHebcaYXquotZ3XSmzNpm8gZM2YEelgAAADwEbrNhEDm3R29JW2fPn3k999/D/RQAAAAgKAQ0Mz7yJEjE3R2NmTIEL+MBwAAAImLzLuFg/fhw4ebPu9Zs2YVm83mdh2CdwAAACAIgveGDRvKypUrpXz58vL8889L48aNJTw8aCt5AAAA4CUy794JaKS8ZMkS+euvv6RixYoyYMAAyZUrl7lpU3R0dCCHBRGp/kg5WfTSFDn61mqxTdslzUrXiXe/TG07zKzT+/EOLvNfa/iirBswR65M2irnxv3q9rmPF65k1rk4YbMce3uNvPVUX0kWzt11EZx+37pDBr38urR4orXULFNX1q5c57L86tVrMmHMZGlVr408UbGRdGzxvHw77zuXdW7cuCnjR0+SJjWfkgaVG8uQfsPl7Jlzfn4nQOLaumWr9Hqpt9St+YSULlZWVv60il0O+EjA09xaNhMVFWUC9rlz58rJkyelQoUKUrVqVbl27Vqgh5dkpY1II78fiZYeX75x1/Wal6kjlQqUlqPnT8RZljJZCpm3bZlM/Xmu2+eWylVYvu85TZbu/EXKvtlSWn/cV5qWqm0CeCAYXbt2XR4p9JD0ierldvmUd6fKpvWbZfCbg+TT+dOlVdsWMvGtybJu9XrHOv99931Zv2aDjHhnqEz8ZJycPnVGhvQd7sd3ASS+a1evSeHChSRqSBS7G3FoG/bEnkJZULWK1KBde73rDZp+++03uXXrlqROnTrQw0qSlu5ca6a7yZkhq0xuPVjqT+oqS3pOjbN8+OL/mq+dKjd3+/zW5RvKjqPR8sb3/3vuX6cOyavz35OvuoyTEYunyOUbV33yXgBfqVTtMTPFZ+fvu6R+k3pStkIZ87hpq8by3TdLZPefe6RqrSpy+dJl+X7BUhky5jV59LGyZp1BIwZIx6eel507dknxUsX4YSEkVKtRzUwAQjDzrjZs2CBdunQxd1edPHmydOrUSf7991+JjIwM9NBwl3q12c++Je8sny67ju2/r/0UkTylXL9102XetVs3JHXKVFIuX3H2PSyneOliJst+6sRpcxH+ts3b5fA/R6RC5fJm+d7d++T27dtSruKjjufkK5BXsuXIagJ/AEgqMURiT6EsoJn3sWPHysyZM+X06dPSrl07Wbt2rZQqVSqQQ0ICDaz3gtyOuSOTVn523/ts2a5fpE+dDtKmfCP5autSyZ4+iwx9srtZliPyQX4WsJzeg3rKuyPHS6v6bSRZ8mQSHhYu/Ye+IqXL/e/v2pnTZyVFihTyQGQ6l+dlzJSRuncAQPAH74MGDZK8efPK008/bc6SNJB3Z9y4cfFu48aNG2ZycSdGJFlQfKgQkh7NW8xcnPro6JZebWf57vUy4Jt3ZVq7YTL7ubfkxu2b8sb306RGwfISY4vx2XgBf5n/xULZ9cduGT3xDcmeI5v8vm2HuYA1y4OZpXylcvwgAIBuM9YO3mvUqGGC9p07d8a7zr0++hgzZoyMGDHCdWa5LCLlydwmZiearA9kkkOjVzjmJU+WXN5r9ar0qdNRCgx+IsHbGr9ilplypH9Qzl29KPkz5zIXrP59+kgijR5IHDeu35CPJk+XUeOGS+Ualcy8hws9JPuj/5K5n84zwXvmLJnMtTyXLl52yb6fO3tOMmXOyI8GABDcwfvq1au93oZ2qunb17U7Sfp+8V9QBu/N3rhIftqzwWXespc/ktm/LpIZGxbc1zaPXThlvj5ToZEcOntMth2i/hfWorXsOoXFuleF3rsiJuZ/N6ErVLSgJE+eXLZt2iY169Yw8w4dPCwnjp009fIAkBSESWjXpCepbjPubNmyxdzEKT4RERFmckHJjE9aRT7yYF7H4wJZcknp3EXk7JULcvjcMfPV2a07t+X4xdOy98RBx7w8GXNIprTpJW/GHKZ3uz5f7T91SK78/04y/Z943nS1ibHZpEXZujKofhd5+qO+lM0gKGkf96OHjjoeHzt6TPbt2S+R6R+QbDmySZlypWTa+A8lIiKlZM+ZTbZv2SHLFi+XHv26mfXTPZBOGj3VQKa8N00eSB8padOmkYlv/dd0maHTDELJ1StX5dChw47HR48elT27oyV9+kjJkTNHQMcGWF2YTVsiBNjly5clWbJkLm0ht2/fLkOGDJHvv/9e7ty549H2wrqRwfJWzUIVZHXfWXHmz9ywQJ6bNTjO/ANvLpcJKz6ViStnO+bN6PSmPFv5qTjr1hrXSX7eu9l8v6LPdFNDr51ntK/8iCXv37NFJRLm2Phl7Cof+23zdunTpX+c+Q2a1JOoN141F6R+OOkT2bJhi1y8eMnUvTdu+aQ83b6lowRQb9L0/nvTZMXSVXLr5i2pUKW8vPLay6akBr6XIWVmdmsAbN60RV54tkuc+U2bN5E3Ro/kZ+JnqZKlCap9HhlVMdFf4+KYjRKqAhq8Hz582FysumnTJhO89+zZU0aNGiXdunUzN2x66qmn5JVXXjF3YPUEwTtA8A4ogneA4D3UBLRsZsCAAXL9+nWZOHGizJ8/33zVdpEarP/111+SO3fuQA4PAAAAPhbibdhDO3hfs2aNCdorVapkMvB6kybt996nT59ADgsAAAAISgEN3k+cOCEFChQw32fNmlXSpEkjDRs2DOSQAAAAkIjCSb17JeB3MtI2as7fp0yZMqDjAQAAAIJVQDPveq1soUKFHF0YtOtM2bJlXQJ6dfbs2QCNEAAAAL50rxtwIoiD9xkzZgTy5QEAAABLCWjw3qlTp0C+PAAAAPyMzHsI3GH12rVrsnz5ctm7d695XLhwYalbt67LTZsAAACApC7gwfuiRYvkhRdekNOnT7vMz5Ili3zyySfSpEmTgI0NAAAAvkXJu4W7zaxfv15atWolNWrUkHXr1pkLU3X65ZdfpHr16mbZr7/+GsghAgAAAEEjzKYtXwKkUaNGkidPHvnggw/cLn/xxRfl8OHD8v3333u03bBuxXw0QsC6jo1fFughAAGXIWXmQA8BCLhUydJIMHlwWNVEf41TI9ZJqApo5l2z6j179ox3eY8ePWTDhg1+HRMAAAAQrJIH+kLVyMjIeJenT59erl+/7tcxAQAAIPHQbcbCmfeCBQvKypUr412+YsUKsw4AAACAAAfvzz33nPTv399tTfuSJUvk1VdflWeffTYgYwMAAEDiZN4TewplAS2b6d27t+k407hxY9PbvWjRoqLXz+7evVv27dsnzZs3lz59+gRyiAAAAEDQCGjmPTw8XObNmydffPGFFCpUSPbs2SPR0dFSpEgRmTNnjnzzzTdmHQAAAIQGMu8Wv0mTat26tZkAAAAABGnwrln1e9Ul6fLbt2/7bUwAAABIPCFekh7awfuCBQviXab93SdNmiQxMTF+HRMAAAAQrAIavDdr1izOPK15HzRokHz33XfSrl07GTlyZEDGBgAAAN8L9W4wiS1orgb9999/pUuXLlKyZElTJrN9+3aZNWuW5MuXL9BDAwAAAIJCwC9YvXDhgowePVomT54sZcqUMTdmql69eqCHBQAAgERA5t3CwfvYsWPl7bffluzZs5t2ke7KaAAAAAAEQfCute2pU6eWRx55xJTI6OTO/Pnz/T42AAAA+F44Ne/WDd47duzIRycAAACAFYL3mTNnBvLlAQAA4Gck3kOk2wwAAACAIO82AwAAgKSDbjPeIfMOAAAAWATBOwAAAPwmzA//eWLNmjXSpEkTyZkzp/lUYOHChY5lt27dkoEDB5qbiKZNm9asow1X9Oaizs6ePSvt2rWTyMhIyZAhg3Tu3FkuX77sss6OHTvMvYxSpUolefLkMS3T7wfBOwAAAJKsK1euSOnSpWXKlClxll29elW2bdsmQ4YMMV+1fXl0dLQ0bdrUZT0N3Hfu3CnLly+XxYsXmxOCrl27OpZfvHhR6tWrJ/ny5ZOtW7fKO++8I8OHD5cPP/zQ4/GG2Ww2m4SYsG7FAj0EIOCOjV8W6CEAAZchZeZADwEIuFTJ0kgwKfB23UR/jQMDf7qv52nmfcGCBdK8efN419m8ebM89thj8s8//0jevHll9+7dUqxYMTO/fPnyZp2lS5dKo0aN5MiRIyZbP3XqVBk8eLAcP35cUqZM6bjfkWb59+zZ49EYybwDAADAbzRATuzpxo0bJtvtPOk8X7hw4YJ5DS2PURs2bDDf2wN3VbduXQkPD5eNGzc61qlRo4YjcFf169c3Wfxz58559PoE7wAAAAgpY8aMkfTp07tMOs9b169fNzXwzzzzjKlvV5pNz5o1q8t6yZMnl0yZMpll9nWyZcvmso79sX2dhKJVJAAAAELqJk1RUVHSt29fl3kRERFebVMvXn366adFK861DCZQCN4BAAAQUiIiIrwO1t0F7lrnvnLlSkfWXWXPnl1Onjzpsv7t27dNBxpdZl/nxIkTLuvYH9vXSSjKZgAAABBSNe++ZA/c9+3bJz/99JNkzux6IXzlypXl/PnzpouMnQb4MTExUrFiRcc62oFGt2WnnWkKFy4sGTNm9Gg8BO8AAABIsi5fvizbt283kzpw4ID5/tChQybYbtWqlWzZskXmzJkjd+7cMTXqOt28edOsX7RoUWnQoIF06dJFNm3aJOvWrZOePXtKmzZtTKcZ1bZtW3OxqvZ/15aSc+fOlYkTJ8Yp7UkIymYAAADgN77OjHtLA/PatWs7HtsD6k6dOple7IsWLTKPy5Qp4/K8VatWSa1atcz3GthrwF6nTh3TZaZly5YyadIkx7p6weyPP/4oPXr0kHLlykmWLFlk6NChLr3gE4rgHQAAAElWrVq1zEWo8UnILZG0s8znn39+13VKlSola9euFW8RvAMAACDJZt6thpp3AAAAwCLIvAMAAMBvSLx7h8w7AAAAYBFk3gEAAOA31Lx7h8w7AAAAYBFk3gEAAOA3ZN69Q+YdAAAAsAgy7wAAAPAbMu/eIfMOAAAAWASZdwAAAPgNfd69Q+YdAAAAsAgy7wAAAPAbat69Q+YdAAAAsAgy7wAAAPAbMu/eIfMOAAAAWASZdwAAAPgNmXfvkHkHAAAALILMOwAAAPyGPu/eIfMOAAAAWASZdwAAAPgNNe/eIfMOAAAAWASZdwAAAPgPRe9eIfMOAAAAWASZdwAAAPgNNe/eIfMOAAAAWASZdwAAAPgNJe/eIfMOAAAAWASZdwAAAPgNNe/eIfMOAAAAWASZdwAAAPgNmXfvkHkHAAAALILMOwAAAPyGzLt3yLwDAAAAFkHmHQAAAH5Dn3fvkHkHAAAALILMOwAAAPyGmnfvkHkHAAAALCIkM+9Xp2wO9BCAgEvToHCghwAE3LWlewM9BACxkHn3Dpl3AAAAwCJCMvMOAACA4ETm3Ttk3gEAAACLIPMOAAAAvyHz7h0y7wAAAIBFkHkHAACA33CHVe+QeQcAAAAsgsw7AAAA/Iaad++QeQcAAAAsgsw7AAAA/IbMu3fIvAMAAAAWQeYdAAAAfkPm3Ttk3gEAAACLIHgHAACAX/u8J/bkiTVr1kiTJk0kZ86c5lOBhQsXuiy32WwydOhQyZEjh6ROnVrq1q0r+/btc1nn7Nmz0q5dO4mMjJQMGTJI586d5fLlyy7r7NixQ6pXry6pUqWSPHnyyNixY+V+ELwDAAAgybpy5YqULl1apkyZ4na5BtmTJk2SadOmycaNGyVt2rRSv359uX79umMdDdx37twpy5cvl8WLF5sTgq5duzqWX7x4UerVqyf58uWTrVu3yjvvvCPDhw+XDz/80OPxhtn0dCLEXLtzJdBDAAIuTYPCgR4CEHDXlu4N9BCAgEuVLI0Ek1pz2yf6a6xu/dl9PU8z7wsWLJDmzZubxxoma0a+X79+0r9/fzPvwoULki1bNpk5c6a0adNGdu/eLcWKFZPNmzdL+fLlzTpLly6VRo0ayZEjR8zzp06dKoMHD5bjx49LypQpzTqDBg0yWf49e/Z4NEYy7wAAAIAbBw4cMAG3lsrYpU+fXipWrCgbNmwwj/WrlsrYA3el64eHh5tMvX2dGjVqOAJ3pdn76OhoOXfunHiCbjMAAADwH0+L0u/DjRs3zOQsIiLCTJ7QwF1ppt2ZPrYv069Zs2Z1WZ48eXLJlCmTyzoFChSIsw37sowZMyZ4TGTeAQAAEFLGjBljMuTOk84LBWTeAQAAEFJ93qOioqRv374u8zzNuqvs2bObrydOnDDdZuz0cZkyZRzrnDx50uV5t2/fNh1o7M/Xr/ocZ/bH9nUSisw7AAAA/CY8LPGniIgI07bRebqf4F1LXTS4XrFihUvnGK1lr1y5snmsX8+fP2+6yNitXLlSYmJiTG28fR3tQHPr1i3HOtqZpnDhwh6VzJj95/G7AAAAAELE5cuXZfv27WayX6Sq3x86dMh8StCnTx8ZNWqULFq0SP744w/p2LGj6SBj70hTtGhRadCggXTp0kU2bdok69atk549e5pONLqeatu2rblYVfu/a0vJuXPnysSJE+N8OpAQlM0AAAAgpMpmPLFlyxapXbu247E9oO7UqZNpB/nqq6+aXvDat10z7NWqVTOtIPVmS3Zz5swxAXudOnVMl5mWLVua3vB2WnP/448/So8ePaRcuXKSJUsWc+Mn517wCUWfdyBE0ecdoM87EIx93ut+0ynRX+OnlrMkVJF5BwAAgN+EB1nm3WqoeQcAAAAsgsw7AAAAkmzNu9WQeQcAAAAsgsw7AAAA/IbMsXfYfwAAAIBFkHkHAACA39Btxjtk3gEAAACLIPMOAAAAv6HbjHfIvAMAAAAWQeYdAAAAfkPNu3fIvAMAAAAWQeYdAAAAfkPNu3fIvAMAAAAWQeYdAAAAfkPm2DvsPwAAAMAiyLwDAADAb+g24x0y7wAAAIBFkHkHAACA39Btxjtk3gEAAACLIPMOAAAAv6Hm3Ttk3gEAAACLCOrg/eTJkzJ69OhADwMAAAA+EuaHKZQFdfB+7NgxGTJkSKCHAQAAAAQFat4BAADgN9S8+yF437FjR4I3WKpUKW/GAwAAAMCb4L1MmTKmJ6fNZnO73L5Mv965cychmwQAAEASRObdD8H7gQMHJDH07dv3rstPnTqVKK8LAAAAhGzwni9fvkR58d9+++2e69SoUSNRXhsAAAD+xx1WA3DB6uzZs2XatGkmI79hwwYT3E+YMEEKFCggzZo1S/B2Vq1adT8vDwAAACRJHreKnDp1qil3adSokZw/f95R454hQwYTwPvS7t27pX///j7dJgAAAAJb857YUyjzOHifPHmyfPTRRzJ48GBJliyZY3758uXljz/+8HpAV65ckU8++USqVKkixYsXl6VLl3q9TQAAACBJBu9aKlO2bNk48yMiIkzgfb/WrVsnzz//vGTLlk26du1qgvddu3bJn3/+ed/bBAAAQHDhDqt+Dt61rn379u1x5muGvGjRoh5t6+TJkzJ27FgpUqSItGrVypTerF69WsLDw00gr/MBAAAA3OcFq1rv3qNHD7l+/brp7b5p0yb54osvZMyYMfLxxx97tC290FWD9okTJ8oTTzxhgnYAAACErlCvSQ+64P2FF16Q1KlTy+uvvy5Xr16Vtm3bSs6cOU0A3qZNG4+D919++UXy5s1rvifTDgAAAPi4VWS7du3MpMH75cuXJWvWrPezGdmzZ4+pddcLVCtUqCCFChWS9u3bm2X0AAUAAAg9ZN69c991KlqvvnXrVomOjvbqTqhVq1aV6dOny7Fjx6Rbt24yb948037ypZdeMl1tuMsqAAAAcJ/B+6VLl6RDhw6mVKZmzZpm0u81Y37hwgWPtjVy5EiTvVfp0qWTLl26yPr162Xnzp1Srlw5U5qj2wYAAEBo0OqKxJ5CWfj91Lxv3LhRlixZYm7SpNPixYtly5Yt8uKLL3q0rREjRpiym9i0a827774rR48elblz53o6RAAAACAkeVzzroH6smXLpFq1ao559evXNyUuDRo08Ghb2q3mroNLnlxatGjh6RABAAAQpKh593PmPXPmzJI+ffo483VexowZPR5AqH+0AQAAAAQs86516Nrrffbs2ZI9e3Yz7/jx4zJgwAAZMmSIxwPQDjP3CuDPnj3r8XYBAAAQfEjb+iF4L1u2rEuAvW/fPtObXSd16NAhiYiIMJ1h7qfu3V0mHwAAAMB9BO/NmzeXxKI3drrfPvEAAACwFmre/RC8Dxs2TBID9e4AAABAIt9h1Vfu1W0GAAAAoYXMu5+7zejdT7UH+2OPPWYuWM2UKZPL5ImbN2+ai12vXbsWZ5nevGnHjh0SExPj6RABAACAkORx8K4XmI4bN05at25t7qiqnWe0F3t4eLgMHz7co2199tln8vzzz0vKlCnjLNN5uuzzzz/3dIgAAAAIUtxh1c/B+5w5c8wNmfr162duovTMM8/Ixx9/LEOHDpVff/3Vo23p8/r37y/JkiWLs0y3/eqrr8qHH37o6RABAACAkORx8K5lLiVLljTfp0uXzmTfVePGjWXJkiUebWvv3r1SqVKleJdXqFBBdu/e7ekQAQAAEMTBZ2JPoczj95c7d245duyY+f7hhx+WH3/80Xy/efNm0+vdE1euXJGLFy/Gu/zSpUum9h0AAADAfQTvTz31lKxYscJ836tXL3NX1YIFC0rHjh1Njbon9Hnr16+Pd/kvv/xi1gEAAEBooObdz8H7W2+9Ja+99pr5Xi9aXbt2rXTv3l2+/vprs8wTbdu2lddff910lYnt999/N3X0ug4AAACQGLSToiajCxQoIKlTpzaVJW+88YZLS3P9XuPSHDlymHXq1q0r+/btc9nO2bNnpV27dhIZGSkZMmSQzp07y+XLl30+Xq/LgrRmXTvOVKxYUUaPHu3Rc1955RVTP1+uXDlp2LCheayTfl++fHkpUaKEeYzgtHXLVnn5pd7yRM16UqbYo7Lyp1Uuy6f+d5o0f7KFVCpXRapXqikvPt9N/vj9j4CNF/BU9ZIVZdHIGXL0yy1iW35EmlWp77J8xoBxZr7z9MPoz1zWKZirgCwc8Ymc+nqHXFi4W9aOny+1SldxLO9U7z9xtmGfHsyQmR8aLOvLz+dKw7qNpEKZitKudQf5Y8efgR4SgqjPe2JPnnj77bdl6tSp8t///tdca6mPx44dK5MnT3aso48nTZok06ZNk40bN0ratGmlfv36cv36dcc6Grjv3LlTli9fLosXL5Y1a9ZI165dxdd8VtOvdfB61uKJFClSmJr5N9980zxfO8t88MEH5nudp8t0HQSna1evS6HChSRqyCC3y/PlzyeDBg+Urxd+JTNmT5ecuXJK9y495OzZc34fK3A/0qZKI7//vUt6TH493nV+2LRKsj9d1jE9M7qHy/LFo2ZJ8mTJ5fEBraVcj0Zme4vfmCnZMj5ols9d/Z3L83Vaunm1rP59g5w6f4YfHCxp6Q/L5N2335MXX3pRvvz6cylcpJB07/qSnDlzNtBDA+LQEu5mzZrJk08+Kfnz55dWrVpJvXr1ZNOmTY6s+4QJE0y1iK5XqlQp+fTTT+Xff/+VhQsXmnU06F+6dKnppKgJ7WrVqpng/8svvzTrhcwdVpUG59oSUidYS7UaVc0Un0aNG7o87jewryz4ZqHsi94rFStX9MMIAe8s3bzKTHdz49YNOXHulNtlmSMzSqHcD0nn9/rLHwf+1zlr0MdjpEfTZ6VE/sLmeddvXjeTXZb0meTxMlWk87gB/PhgWbNnfiYt/tNCmrdoZh6/PmywrPl5rSycv1A6d/Hs+jiEHn/cYfXGjRtmcqaNVdw1V6lSpYpJIGsXxEKFCpnSbb3uUu9rpA4cOGC6LWqpjF369OlNkL5hwwZp06aN+aqlMlo5Yqfr632QNFOv14z6Sqh300GQuHXzlnzz1XxJ90A6KVSkUKCHA/hMrdKV5cRX22XP9J/l/ZdHS6YHMjiWnbl4TvYc2i8dn2glaVKllmThyeTFJ9uboH3rPvclZLru1RvX5Os1nrXeBYLp7/3uXbulUqX/S9JoAFOpckXZsT3uNW5AYhgzZowJsJ0nnefOoEGDTABepEgRk1QuW7as9OnTx5TBKA3cVbZs2Vyep4/ty/Rr1qxZ49yzKFOmTI51QiLznjFjRnPF8b3oBQCwpjWr18jAflGmJizLg1lk2sdTzc8dCAVa3jL/lx/kwLHD8nDOfDL6+YGm5r1y76YSExNj1qk78BlZOOJjufRttMTYYuTk+dPSIKq9nL/8v3tkxNa5QRv5fOVCl2w8YCXnzp8zFwBmzpLJZX7mzJnlwN8HAzYuBI+ExH7eioqKMtdkOouvpflXX31lbkL6+eefS/HixWX79u0meM+ZM6d06tRJgk2Cg/fYOyC2U6fcf2x8N1o/lBgfi8Qkv+1xz3kkjgqPVZC587+Q8+fPy/x5C+TVvgPlsy8/lUyZXf+oA1Y0d/Uix/d/HtwjO/7eLX/PXm+y8St/W2fmT+k1Sk6ePyPV+7aQazeuywsNn5Hv3pgpFXo+KcfPnnTZXqWij0qxfIWkw9u9/f5eACCURMRTIuPOgAEDHNl3pc1U/vnnH5Op1+A9e/bsZv6JEydMtxk7fVymTBnzva5z8qTr3/Tbt2+bBLT9+X4P3n/77bd7rlOjRg2PXtwXZzO6Y0eMGOEy77UhUaa+DoGXOk1qyZsvr5lKlS4lTRo0M3XvnbtS84jQc+D4IXOR6SM585vg/fGyVaVxxbqSsUVxuXT1f+3CekweLE+UqyGdnviPvD13isvzX2jYVn7b/6dsi6ekBrCCjBkySrJkyeTMaddPzc+cOSNZstBBCVqznfiZd0/oDUG1tMuZHsP2T1C1haQG4HqfI3uwrjcZ1Vp2bZeuKleubBKVW7duNV0U1cqVK802tDY+IMH7qlV3v2grMdi7zmjrHk8+FtHMO4KTXrF98+bNQA8DSBS5suQwF6ke+/8Z9TQRqc1X+z8Advo4PDwsTmebp2s2lqjpnt0vAwg2KVKmkKLFisrGXzfK43VrO475jb9ukjZtWwd6eEgiZTOeaNKkiYk38+bNa8pmNGGtF6vabz6q49UymlGjRpmbh2owrx0WtaymefPmZp2iRYtKgwYNpEuXLqad5K1bt6Rnz54mm6/rhVS3Ge2HqScGKVOmlKefftpcqXv69GmzE/XNP/TQQx5/LHLtzpVEHjXU1StX5dChw46dcfToUdmzO1rSp//fzQk++uBjqfV4TcmSJYs5G537+Vdy8sRJeaL+E+xAWIIG1I/kyu94XCB7Hin9cDE5e/G8nL10XoZ16Cvf/PK9KX/RmvexLwyW/f8elGVbfjbrb9i1Vc5dviCzXp0gIz8bb8pmujRqZ7azZOP/7lRt17pWU9NS8rOf5vv9fQK+1uHZ9jIkaqgUL1FMSpQsIZ99+rlcu3ZNmj/1v+4zQDCZPHmyCcZfeuklU/qiwfaLL75obspkp10Rr1y5Yvq2a0yjrSC1NWSqVKkc62jdvAbsderUMZn8li1bmt7wvhZmc759lJ8tWrTI9NLUmiClgfpHH31kgnj9yEHPcvQsxlME7/6xedMW6fJs3JsPNGneRF4f9ppEDXjN3JTj/LnzkiFDeileori80O0FKVGyuJ9GmLSlaVA40EOwvJqlKsvq9+bFmT/zx6+k+8TXzIWoZR8uIRnSRcq/Z07Ij1vXyJCZ75iLUu3KFSolbz73qpQvVFpSJEsuO//ZKyM/mxCnBeW6CQvlwPHD0v6tXn55b0nFtaV7Az2EJOuLOV/KrOmz5PTpM1K4SGEZ+NqrUqp0yUAPK0lKlSyNBJOoDa8l+muMqezZjUOtJKDB+2OPPSZVq1Y1t6DVpvZa/qIfV0yfPl0qVKhw39sleAcI3gHz7wHBO0DwHmIC2uc9OjpaevToIenSpZNevXqZjxjGjx/vVeAOAACA4BXmh/9CWUCD90uXLklkZKTjqt7UqVPfs8YdAAAASKru64LVtWvXygcffCB//fWXfP3115IrVy6ZPXu2ufpWC/g9sWzZMnPXK/vV6NqG588//3RZp2nTpvczTAAAAASZYOs2E/LB+zfffCMdOnQwt4zVVjr2GyRduHBBRo8eLd9//71Xvd716t7YP2C9UxsAAACQ1HlcNqM9LrWFo3aFSZEihWO+Xni6bds2j7almfZ7TQTuAAAAoSM8LCzRp1AWfj8Xmbq7k6qWvmjfSwAAAABBErzr7WH3798fZ/4vv/zi8cWm2gz/8uX/3TJcffHFF6YBvp2eDDRq1MjTIQIAACBIhUl4ok+hzON3p7d97d27t2zcuNHUo//777/mjlL9+/eX7t27e7Qtvej16tWrLvXuJ06ccDzWenq9oBUAAADAfVywOmjQIFOLrrd+1cBbS2giIiJM8K692j0R+/5QAbxfFAAAAPwg1GvSgy5412z74MGDZcCAAaZ8RsteihUrZm60BAAAACDI+ryrlClTmqAdAAAASCj6vPs5eK9du/Zdd/rKlSs92t7QoUMlTZo05vubN2/Km2++6bhpk3M9PAAAAJDUeRy8lylTxuXxrVu3ZPv27eauqLFvuHQvWi+vrSftqlSpIn///XecdQAAABAawoSad78G7+PHj3c7f/jw4S5tHxNi9erVnr48AAAAkGT5rBFm+/btZfr06R49R/vCnzlzxldDAAAAQJDjDqtBErxv2LBBUqVK5dFzDh48KHfu3PHVEAAAAICQ5nHZTIsWLeL0Zj927Jhs2bJFhgwZ4suxAQAAIMTQbcbPwbu9E4xdeHi4FC5cWEaOHCn16tXzeAB6B9XY24ytadOmHm8XAAAASNLBu5a4PPfcc1KyZEnJmDGjTwZwrw41enZGaQ0AAEBoCPdd1XaS5NHeS5Ysmcmunz9/3mcDOH78uMTExMQ7EbgDAAAA/+PxqU+JEiXi9GK/X9Q8AQAAJC0a/yX2FMo8Dt5HjRol/fv3l8WLF5sLVS9evOgyeUIvdgUAAADg45p3vSC1X79+0qhRI8dFpM5nNhqIe1qfrvXuqVOnTvD6AAAAsLZQz4wHTfA+YsQI6datm6xatcpnLz5jxgyfbQsAAAAIdck9LXGpWbOmz15c20ze6+xLl9++fdtnrwkAAIDACRcy735rFenrjznmz58f7zb1jq2TJk0yHWcAAAAAeBi8FypU6J4B/NmzZxO8vebNm8eZFx0dLYMGDZLvvvtO2rVrZ2rtAQAAEBqoefdj8K517/e6G+r9+vfff2XYsGEya9YsqV+/vmzfvt20pQQAAABwH8F7mzZtJGvWrOJLFy5ckNGjR8vkyZOlTJkysmLFCqlevbpPXwMAAADBIZxuM/4J3hPjI46xY8fK22+/LdmzZ5cvvvhCmjVr5vPXAAAAAJJstxlf0tp27fP+yCOPmHIZneK7sBUAAADWF0a3Gf8E74nR9aVjx45ctAAAAAAkRs27r82cOTOQLw8AAAA/Cw8LZ59bNXhv0aJFgmrtv/nmG7+MBwAAAAhmAQ3eE6vtJAAAAIITfd4tHLzPmDEjkC8PAAAAWEpAg3cAAAAkLXSb8Q5XDAAAAAAWQeYdAAAAfsMdVr1D5h0AAACwCDLvAAAA8Btq3r1D5h0AAACwCDLvAAAA8Btq3r1D5h0AAACwCDLvAAAA8JuwMHLH3mDvAQAAABZB5h0AAAB+Q7cZ75B5BwAAACyCzDsAAAD8hm4z3iHzDgAAAFgEmXcAAAD4TVhYGHvbC2TeAQAAAIsgeAcAAIAfg8+wRJ88dfToUWnfvr1kzpxZUqdOLSVLlpQtW7Y4lttsNhk6dKjkyJHDLK9bt67s27fPZRtnz56Vdu3aSWRkpGTIkEE6d+4sly9fFl8jeAcAAECSde7cOalataqkSJFCfvjhB9m1a5e89957kjFjRsc6Y8eOlUmTJsm0adNk48aNkjZtWqlfv75cv37dsY4G7jt37pTly5fL4sWLZc2aNdK1a1efjzfMpqcSIebanSuBHgIQcGkaFA70EICAu7Z0b6CHAARcqmRpJJjMjP4w0V/j2cIJD5oHDRok69atk7Vr17pdrqFyzpw5pV+/ftK/f38z78KFC5ItWzaZOXOmtGnTRnbv3i3FihWTzZs3S/ny5c06S5culUaNGsmRI0fM832FzDsAAABCyo0bN+TixYsuk85zZ9GiRSbg/s9//iNZs2aVsmXLykcffeRYfuDAATl+/LgplbFLnz69VKxYUTZs2GAe61ctlbEH7krXDw8PN5l6XyJ4BwAAgN+EhYUn+jRmzBgTYDtPOs+dv//+W6ZOnSoFCxaUZcuWSffu3eXll1+WWbNmmeUauCvNtDvTx/Zl+lUDf2fJkyeXTJkyOdbxFVpFAgAAIKRERUVJ3759XeZFRES4XTcmJsZkzEePHm0ea+b9zz//NPXtnTp1kmBD5h0AAAAh1W0mIiLCdH1xnuIL3rWDjNarOytatKgcOnTIfJ89e3bz9cSJEy7r6GP7Mv168uRJl+W3b982HWjs6/hu/wEAAABJVNWqVSU6Otpl3t69eyVfvnzm+wIFCpgAfMWKFY7lWkOvteyVK1c2j/Xr+fPnZevWrY51Vq5cabL6WhvvS5TNAAAAIMneYfWVV16RKlWqmLKZp59+WjZt2iQffvihmezj7dOnj4waNcrUxWswP2TIENNBpnnz5o5MfYMGDaRLly6m3ObWrVvSs2dP04nGl51mFME7AAAAkqwKFSrIggULTJ38yJEjTXA+YcIE07fd7tVXX5UrV66Yvu2aYa9WrZppBZkqVSrHOnPmzDEBe506dUyXmZYtW5re8L5Gn3cgRNHnHaDPOxCMfd7n7JuR6K/RruBzEqrIvAMAACDJls1YDResAgAAABZB5h0AAAB+o60ccf/IvAMAAAAWQeYdAAAAfhMWRu7YG+w9AAAAwCLIvAMAAMBvwqh59wqZdwAAAMAiyLwDAADAb+jz7h0y7wAAAIBFkHkHAACA31Dz7h0y7wAAAIBFkHkHAACA31Dz7h0y7wAAAIBFkHkHAACA34TT590rZN4BAAAAiyDzDoSoM4u3BXoIQMB9sHNaoIcABFzvUn0lmFDz7h0y7wAAAIBFkHkHAACA34SRO/YKmXcAAADAIsi8AwAAwG+oefcOmXcAAADAIsi8AwAAwG/C6PPuFTLvAAAAgEWQeQcAAIDfhIeFsbe9QOYdAAAAsAgy7wAAAPAbat69Q+YdAAAAsAgy7wAAAPAb+rx7h8w7AAAAYBFk3gEAAOA3YeSOvULmHQAAALAIMu8AAADwG2revUPmHQAAALAIMu8AAADwm3DhDqveIPMOAAAAWASZdwAAAPgNNe/eIfMOAAAAWASZdwAAAPhNGDXvXiHzDgAAAFgEmXcAAAD4DTXv3iHzDgAAAFgEmXcAAAD4TRi5Y6+QeQcAAAAsgsw7AAAA/CY8jDuseoPMOwAAAGARZN4BAADgN/R59w6ZdwAAAMAiyLwDAADAb+jz7h0y7wAAAIBFkHkHAACA31Dz7h0y7wAAAIBFELwDAADArzXviT1546233jLb6NOnj2Pe9evXpUePHpI5c2ZJly6dtGzZUk6cOOHyvEOHDsmTTz4padKkkaxZs8qAAQPk9u3b4msE7wAAAICIbN68WT744AMpVaqUy/545ZVX5LvvvpN58+bJzz//LP/++6+0aNHCsfzOnTsmcL9586asX79eZs2aJTNnzpShQ4cSvAMAAMC6wv3w3/24fPmytGvXTj766CPJmDGjY/6FCxfkk08+kXHjxsnjjz8u5cqVkxkzZpgg/ddffzXr/Pjjj7Jr1y757LPPpEyZMtKwYUN54403ZMqUKSag9yUy7wAAAEjyevToYbLndevWddkXW7dulVu3brnML1KkiOTNm1c2bNhgHuvXkiVLSrZs2Rzr1K9fXy5evCg7d+706b6l2wwAAABCqs/7jRs3zOQsIiLCTO58+eWXsm3bNlM2E9vx48clZcqUkiFDBpf5GqjrMvs6zoG7fbl9mS+ReQcAAEBIGTNmjKRPn95l0nnuHD58WHr37i1z5syRVKlSSbAjeAcAAIBf+7wn9n9RUVGmVt150nnuaFnMyZMn5dFHH5XkyZObSS9KnTRpkvleM+hat37+/HmX52m3mezZs5vv9Wvs7jP2x/Z1fIXgHQAAACElIiJCIiMjXab4Smbq1Kkjf/zxh2zfvt0xlS9f3ly8av8+RYoUsmLFCsdzoqOjTWvIypUrm8f6VbehJwF2y5cvN69brFgxn743at4BAAAQUjXvnnjggQekRIkSLvPSpk1rerrb53fu3Fn69u0rmTJlMgF5r169TMBeqVIls7xevXomSO/QoYOMHTvW1Lm//vrr5iLY+E4a7hfBOwAAAHAX48ePl/DwcHNzJr0QVjvJvP/++47lyZIlk8WLF0v37t1NUK/Bf6dOnWTkyJHia2E2m80mIebanSuBHgIQcNdu83sAzI7+jJ2AJK93qb5BtQ82n/ol0V+jwoPVJFRR8w4AAABYBGUzAAAA8BvtBoP7R+YdAAAAsAgy7wAAAPCfIOs2YzUE7wAAAPAbyma8Q9kMAAAAYBFk3gEAAJBkb9JkNWTeAQAAAIsg8w4AAAC/oebdO2TeAQAAAIsg8w4AAAC/IfPuHTLvAAAAgEWQeQcAAIDf0G3GO2TeAQAAAIsg8w4AAAC/oebdO2TeAQAAAIsg8w4AAAC/IfPuHTLvAAAAgEWQeQcAAIDf0G3GO2TeAQAAAIsI6uD96tWrsn79+kAPAwAAAD6seU/s/0JZUAfv+/btk+rVqwd6GAAAAEBQoOYdAAAAfkPNewhn3gEAAAD8HzLvAAAA8JtQr0kP6eB90aJFd11+4MABv40FAAAACHYBDd6bN28eyJcHAACAn5F5t3DwHhMTE8iXBwAAACwlqC9Y1eB+8eLFgR4GAAAAfNhtJrGnUBaUF6zu379fpk+fLjNnzpRTp07JrVu3Aj0kAAAAIOCCJvN+7do1+fTTT6VGjRpSuHBhc2fVoUOHypEjRwI9NAAAAPgId1j1TsAz75s3b5aPP/5YvvzyS3n44YelXbt2JnB///33pVixYoEeHu5i65atMmv6p7J75245deq0jJv0njxet7bLOn//9bdMHDdJtm7eJrfv3JaHHn5I3pvwjuTImYN9C8v5bct2mTPzC4neHS2nT52Rtya8KTUfr+FYvvqnn2XBvG9lz65ouXjhosz6aroUKlLQ7bZsNpv0fWmA/LpuY5ztAMHu8pkrsmHOr3Lot8Ny+8ZtSZ89vTzeo5ZkffhBs/z9/3zg9nmV21eUss3KmO+vX7oua6evk4Nb/zFlDg9VLCDVn6sqKVKn8Ot7AawmoMF7qVKl5OLFi9K2bVsTsBcvXtzMHzRoUCCHhQS6dvW6FCpcSJq3aCZ9X+4fZ/nhQ4flufadpXnLZtK9RzdJmy6t/LX/b4mIiGAfw5KuX7suBQs/Io2felKiXhns9hPEUmVLSp16tWXMiLF33daXn30V8nWZCE3XL9+QBUMWSq7iOaXxa40kdWQqOX/8gkSkTelY59kPO7g855/th2TV1J/loUoPOeb9NGmlXDl3VZoOeVJibsfIyvdXy+oP1sgTfer49f3A/+g2Y+HgPTo6Wlq3bi21a9cmy25B1WpUNVN8/jtxiln+Sv8+jnl58ubx0+gA36tcvZKZ4tOwSQPz9djRY3fdzt49++SLWXNlxpcfSePHaZkLa/lt4XZJlzmdPN7j/z5pjcwW6bJOmoxpXB4f3PyPCfbT///1zh45J4e2H5ZWb7VwZOurP19VFo/5Qap0rCRpM6X1y3sBrCigNe9///23qW/v3r275M6dW/r37y+//fYb2agQoJ2C1v78i+TLn0+6d3lJalerI+1bd5SVP60K9NCAgGfvhw0aIf0HvyKZs2TmpwHLObjloDz48IOy7L3lMqPzLPlqwNey66fd8a5/9fxV+WfbISn6eBHHvBN7T5hMvT1wV7lL5Tb//p/YdzLR3wMCi24zFg7ec+XKJYMHDzbdZWbPni3Hjx+XqlWryu3bt02nmb179wZyePDC2TNn5erVqzL94xlSpVoVmfrR+6Yevl/v/rJl81b2LZKsCe9MlpKlS0iN2tUDPRTgvlw8eUl2/rhL0ueIlMavPynF6xUztet7Vke7XT/6572SIlUKU9PuHNCnjkztsl54snBJlS7CLAMQpGUza9askSpVqkjy5Mnl8ccfN9OFCxdkzpw5plXku+++KyVKlJAdO3bEu40bN26YyVlM8tvUVQdYjM1mvtZ6vJZ06NTefF+kaGH5ffvv8vXcr6V8hXIBHiHgf2tX/SJbN22TWV99wu6HZdlibCbzXqltRfP4wQJZ5OzhcyagL1KrcJz1d6+MlkLVH5HkKQPeIwNBg+t9LJt511r3s2fPusxLnz69vPTSS7JlyxbZtm2b1KpV667bGDNmjHmO8/TOW+8m8shxLxkzZDAnZQ8//H8XJ6kCDxWQY8eOswORJG3ZtE2OHj4q9ao2kmpla5lJvdZ3iLz0fK9ADw9IEK1nz5Q7o8u8jLkyyOXTl+Os++/uY3L+3/NStE5R121kSCPXLl5zmRdzJ8ZcDKvLAMQvoKfB2irtbsqUKSOTJk266zpRUVHSt2/fOJl3BFaKlCmkWIlicvDAQZf5/xw8RJtIJFkdO7eTpi0au8xr37KT9B7QS6rVrBKwcQGeyFE4uwnInZ0/dkHSPfhAnHV3r9gjDz6URbLkd72+I1uhbHLjyk05+dcpR937kT+PmrggW8Gs/EBCHJ22vJPc6j9AbTsYu/XgtTtXvBwVEuLqlaty6NBhx+OjR4/Knt3Rkj59pAnQn32+o7zad5A8Wv5RqfBYeVn/y3pZs3qNfDzzQ3YwLEmv4zhy6Kjj8b9Hj5nOMZHpIyV7jmxy4cJFOXHshJw+ddosP3TwkPmaOUsmc3GqfYotW46skjN3Tj++E+D+lWpcUha8/q1snb9NHqn8sJzYf9JcsFrrRdd7Fdy8elP++vVvqdKxcpxtaOY+b5k8pjVkzS7VTdZ97SfrpGCVR+g0A9xDmO1e6e9EFB4eLg0bNrxnffr8+fM92i7Bu39s3rRFujzbNc78Js2byBujR5jvF36zUD75aIacPHHyf51nenaT2nXuXgoF37h2m5NYX9u2+Tfp0fnlOPMbNW0gQ0YNliXffi+jhoyJs7xzt+fkhZeed7vNyqWqc5OmRDQ7+rPE3HySpTdW+nXOJrlw/II8kPUBKdO4lBSr61oas3P5Llk3c4N0+rC9RKSN+++8uUnTJ043aarETZoSS+9SrhUKgfb3JfcXN/vSQw/Evf4iVAQ8eH/66acldWrXK85jmzFjhkfbJXgHCN4BRfAOELyHmoCXzWhNe9as1LcBAAAkBdxh1cLdZrhgAQAAAAiRbjMAAAAILSRvLZx5X7VqlWTKlCmQQwAAAAAsI6DB+507d6RUqVJy8eLFOMv0TqvFixeXtWvXBmRsAAAASJya98T+L5QFNHifOHGidOnSRSIjI+Ms0zulvvjiizJu3LiAjA0AAAAINgEN3rdv3y4NGjSId3m9evVk69atfh0TAAAAEg+ZdwsH7ydOnJAUKVLEuzx58uRy6tQpv44JAAAACFYBDd5z5colf/75Z7zLd+zYITly5PDrmAAAAJC43WYSewplAQ3eGzVqJEOGDJHr16/HWXbt2jUZNmyYNG7cOCBjAwAAAIJNQPu8v/766zJ//nwpVKiQ9OzZUwoXLmzm79mzR6ZMmWK60QwePDiQQwQAAIAPhXo3mJDOvGfLlk3WrVsnJUqUkKioKHnqqafM9Nprr5l5v/zyi1kHAAAASAxjxoyRChUqyAMPPCBZs2aV5s2bS3R0tMs6WiXSo0cPyZw5s6RLl05atmxprt10dujQIXnyySclTZo0ZjsDBgyQ27dvh1bmXeXPn1++//57OXfunOzfv9/cdbVgwYKSMWPGQA8NAAAAPhZsNek///yzCcw1gNdgW5PI2vFw165dkjZtWrPOK6+8IkuWLJF58+aZduZaMdKiRQuThFZaLaKBe/bs2WX9+vVy7Ngx6dixo2nMMnr0aJ+ON8ym0XKA6JtOCC2t8cS1O1fuc0RA6Lh2m98DYHb0Z+wEJHm9S/UNqn1w9OrBRH+NXGny3/dztdOhZs41qK9Ro4a5ceiDDz4on3/+ubRq1cpR4l20aFHZsGGDVKpUSX744Qdznea///7rqBqZNm2aDBw40GwvZcqUoVE2o2cuCZkAAAAQGvzR5/3GjRty8eJFl0nnJYQG6ypTpkzmq95z6NatW1K3bl3HOkWKFJG8efOa4F3p15IlS7qUe9evX9+87s6dO0OnbGbGjBmBfHkAAACEaB37iBEjXOZpF8Phw4ff9XkxMTHSp08fqVq1qrn+Uh0/ftxkzjNkyOCyrgbqusy+TuzrNO2P7euETM07AAAAkpLEr3mPioqSvn1dy4UiIiLu+Tytfdd7EGnTlGBF8A4AAICQEhERkaBg3ZlehLp48WJZs2aN5M6d2zFfL0K9efOmnD9/3iX7rt1mdJl9nU2bNrlsz96Nxr5OSNS8AwAAIOnl3RN78oT2btHAfcGCBbJy5UopUKCAy/Jy5cqZrjErVqxwzNNWktoasnLlyuaxfv3jjz/k5MmTjnWWL18ukZGRUqxYMfElMu8AAABIsnr06GE6yXz77bem17u9Rl2bpqROndp87dy5synD0YtYNSDv1auXCdi104zS1pIapHfo0EHGjh1rtqE3I9Vte/oJwL0QvAMAACDJ9nmfOnWq+VqrVq04jVWeffZZ8/348eMlPDzc3JxJu9ZoJ5n333/fsW6yZMlMyU337t1NUK/94Tt16iQjR470+XgD2uc9sdDnHaDPO6Do8w5I0PV5P37tcKK/RvbUeSRUkXkHAACAHwVX5t1quGAVAAAAsAgy7wAAAPAb8u7eIfMOAAAAWASZdwAAAPgRuXdvELwDAAAgybaKtBrKZgAAAACLIHgHAAAALILgHQAAALAIat4BAADgN2FcsOoVMu8AAACARZB5BwAAgN+QefcOmXcAAADAIgjeAQAAAIsgeAcAAAAsgpp3AAAA+A13WPUOmXcAAADAIgjeAQAAAIsgeAcAAAAsgpp3AAAA+A193r1D5h0AAACwCDLvAAAA8KMw9rYXyLwDAAAAFkHmHQAAAH5D3t07ZN4BAAAAiyDzDgAAAL/hDqveIfMOAAAAWASZdwAAAPgRVe/eIPMOAAAAWASZdwAAAPgNeXfvkHkHAAAALILMOwAAAPyI3Ls3yLwDAAAAFkHmHQAAAH5Dn3fvkHkHAAAALILgHQAAALAIgncAAADAIqh5BwAAgN+E0W3GK2TeAQAAAIsg8w4AAAA/os+7N8i8AwAAABZB5h0AAAB+Q97dO2TeAQAAAIsg8w4AAAC/4Q6r3iHzDgAAAFgEmXcAAAD4EVXv3iDzDgAAAFgEmXcAAAD4DXl375B5BwAAACyCzDsAAAD8iNy7N8i8AwAAABZB5h0AAAB+Q59375B5BwAAQJI3ZcoUyZ8/v6RKlUoqVqwomzZtCsp9QvAOAACAJG3u3LnSt29fGTZsmGzbtk1Kly4t9evXl5MnT0qwIXgHAABAkjZu3Djp0qWLPPfcc1KsWDGZNm2apEmTRqZPny7BhuAdAAAAfhPmh/88cfPmTdm6davUrVvXMS88PNw83rBhgwQbLlgFAABASLlx44aZnEVERJgpttOnT8udO3ckW7ZsLvP18Z49eyTYhGTwnjpZ2kAPIUnTX5YxY8ZIVFSU218S+Ae/B4HF70Fw6F2qb6CHkKTxewB3UiVLk+g7Zvgbw2XEiBEu87Seffjw4Zb/oYTZbDZboAeB0HLx4kVJnz69XLhwQSIjIwM9HCAg+D0A+D2ANTLvN2/eNPXtX3/9tTRv3twxv1OnTnL+/Hn59ttvJZhQ8w4AAICQEhERYRKIzlN81QApU6aUcuXKyYoVKxzzYmJizOPKlStLsAnJshkAAAAgobRNpGbay5cvL4899phMmDBBrly5YrrPBBuCdwAAACRprVu3llOnTsnQoUPl+PHjUqZMGVm6dGmci1iDAcE7fE4/ltKLQrhYFUkZvwcAvwewlp49e5op2HHBKgAAAGARXLAKAAAAWATBOwAAAGARBO8AAACARRC8JwHPPvushIWFyVtvveUyf+HChWa+nd4aePz48VKyZElJlSqVZMyYURo2bCjr1q1zed7MmTPN83QKDw+XHDlymKu0Dx065LJerVq13L6uevLJJ80yd3c6++KLLyRZsmTSo0ePOMtWr15tnqc3TQCcj3H7jTUScrzb14lvyp8/v8sxHHvq1q2bY7vO87WPcIUKFeLc0EN/ZzJkyBDnpiBjx46V0qVLm5uDZMmSRapWrSozZsyQW7du3XV8sX93ihQpYi6Q1Q4Jsel76NOnDwcLvPq9Unp89erVSx566CFzvOXJk0eaNGliemHb/zbfbdJ11JEjR0xf7RIlSrh9XV1Xf18BuEfwnkRoMP7222/LuXPn3C7XG+22adNGRo4cKb1795bdu3ebP7T6x1n/8Y/9h1SDlGPHjsnRo0flm2++kejoaPnPf/4TZ7v6fA1cnOlz9I+9Bv3ufPLJJ/Lqq6+aIP769etevW8kTfc63idOnGiOX/ukNGi2P968ebNj3S5durisq5MG3c7sz92yZYsJwFu1aiV//PFHvOPTwL1+/frmBKNr166yfv162bRpkzlhnTx5suzcudPl9bTfsP13zj7179/fbOuXX36Ra9eumdecNWuWj/Yg4OrgwYPmJjYrV66Ud955xxzf2kavdu3a5ritUqWKy/H59NNPS4MGDVzm6TpK/03Q5XoX4o0bN7KrAQ/RKjKJqFu3ruzfv1/GjBkTJ/BQX331lbkt8KJFi0wmxe7DDz+UM2fOyAsvvCBPPPGEpE2b1pEZyZ49u/leg/DOnTvLyy+/bP4Ya5Bh17hxY7Ntzd5rUKM0wKhXr16cTL06cOCACWT0hGDVqlUyf/58adu2baLsEyTd4z19+vRmcqaZcfsx7Uyz4u7mu3uuTm+88YY5OdDjVz/FckeD8TVr1phgv2zZso75mtHUk2AN7u2/a/bxOv/OxT7Z1d+RmjVrmhPvgQMH3nWswP146aWXzDGoJ5nOx2bx4sXl+eefN5l05+MzderU5tb0sY9ZTRTpye77778vuXPnNsdvxYoV+aEAHiDznkRoGcro0aNNVk8/sozt888/l0KFCrkE7nb9+vUzAfzy5cvdbvvkyZOyYMEC8xo6OdM/6O3atTN/rO0066J/7N3R9bSkRoOV9u3bmz/sgK+P98Ry+/ZtxzGrx3585syZY04wnAN3uxQpUrgER3dz6dIlmTdvnvld0ZPrCxcuyNq1a714B0BcZ8+eNVl2zbC7OzZjl4TdjZ7UXr161Rz/etx++eWX5i6WABKO4D0Jeeqpp8wdw/QGSrHt3btXihYt6vZ59vm6jp0GCenSpTN/yPXuY/oHOb4/7Bqoa/Zd/0BrtlGfqxn52GJiYkxgr3/QlZbxaEmAZuMBXx7vntAMoR7rzpMG386eeeYZM1/rgF955RVTM69lAfHZt2+fqVP3lgY+BQsWNNlPPWHR3xlOeOFr+imWZsx9cczq8anHqR6vWvOunzbpCSiAhCN4T2K0DljLVrSmPTb945xQDzzwgGzfvt187P/ee+/Jo48+Km+++abbdfWCPA0wtCxn+vTp0qFDB0mePG7Flmb2NcBv1KiReawX8Gk2UZ8D+Pp4Tyj95EiPdeepadOmLuvohd46/4cffpBixYrJxx9/LJkyZYp3m578rt2N/m7YT3aVfq+BkGbkAV/x1fGqjQa0FDL2McsJJ+AZat6TmBo1apgL5aKiokwnATstmYkvwLHP13XstMvMI4884sjM//XXX9K9e3eZPXu2221o9n3KlCmya9cuUzPpjv4B149ntVbSORu/Y8cOGTFihHlNwBfHuye0hMt+rMdH63p1HZ209EtPQPVYz5o1q9v19Xdpz5494g3d/q+//mp+n5zr3LVrlGbk9UJbwBc0+aL17t4es1qeqU0InGvc9cRA/87rJ7vO/8YAiB/RUBKkHS6+++472bBhg2OefoypH+Xr/Ng0s545c2aTBY/PoEGDZO7cubJt2za3y/WCOu1OoB+TamYyNq2p1/Z6GnQ4Zzh/++030zHkxx9/vO/3i6TN3fGemB577DHTlSO+T6Lsvw8//fSTOb5j0zaRCakB1pNdPTn5/fffXX5n+vbtSyYTPqWfIulJsCZg3B2bCW3dq8esXkPlfLzq8Vu9enU+YQU8QOY9CdIOGFoKMGnSJJfgXT9u79Spk2kDVqdOHdM5Rv9YawcaXXa3i+i0JaTWGA8dOlQWL14cZ7n2jNdWYXoxnjuasdcTBK0Tdu49rzSLqX/0te2YnZ4IaOmOnT5Hy3OAhBzvntCL62L3T9fadj2m46N91fX3QVue5sqVy+3yJUuWmN8z7U5TrVo1czxrGZqW+ujxrvX68dEAX39ntLVr7F7Z2hlq3Lhxpt2k1sKrU6dOmUDJmXaJ0utVgITQfwu0Y5ienOpxV6pUKXOBtpY7Tp069Z6laXr8aXJHrxeJXTuv14zoNkeNGuUoqdRrnWIfs/oJQEIv5gZCGZn3JEr/UOpHlc7Br15U+tprr5n63cKFC5tsyD///GP6vTvfqCM+eqGeBiTxlcVoR4L4/vBq7a4GO7EDd9WyZUtzAnH69GnHPM04aqcO+6SZTiChx7snPvroIxPoOk8abNyNnmgWKFAg3uy7Bv8a9Ghw/8EHH0ilSpXMzZ30BENbrsZ38xo7/X3QT6v0dyY2LWPTybmOWMsVnH9fdNL3BSSUXliqwbf2ddfsuR6j+mms3rNDg/d70eNRP3V1d9GrHsfatez77793zNNPkGIfs+4+qQKSojCbr65EAQAAAJCoyLwDAAAAFkHwDgAAAFgEwTsAAABgEQTvAAAAgEUQvAMAAAAWQfAOAAAAWATBOwAAAGARBO8AAACARRC8A0hynn32WZe7BteqVUv69Onj93Ho3Yv1rsLnz5/323sN1nECABKG4B1AUNAgUwNEnVKmTCmPPPKIjBw5Um7fvp3orz1//nx54403gjKQzZ8/v0yYMMEvrwUACH7JAz0AALBr0KCBzJgxQ27cuCHff/+99OjRQ1KkSCFRUVFxdtLNmzdNkO8LmTJl4ocAALAEMu8AgkZERIRkz55d8uXLJ927d5e6devKokWLXMo/3nzzTcmZM6cULlzYzD98+LA8/fTTkiFDBhOEN2vWTA4ePOjY5p07d6Rv375meebMmeXVV18Vm83m8rqxy2b05GHgwIGSJ08eMyb9FOCTTz4x261du7ZZJ2PGjCYDr+NSMTExMmbMGClQoICkTp1aSpcuLV9//bXL6+gJSaFChcxy3Y7zOO+HvrfOnTs7XlP3ycSJE92uO2LECHnwwQclMjJSunXrZk5+7BIydgBAcCDzDiBoaSB55swZx+MVK1aY4HP58uXm8a1bt6R+/fpSuXJlWbt2rSRPnlxGjRplMvg7duwwmfn33ntPZs6cKdOnT5eiRYuaxwsWLJDHH3883tft2LGjbNiwQSZNmmQC2QMHDsjp06dNMP/NN99Iy5YtJTo62oxFx6g0+P3ss89k2rRpUrBgQVmzZo20b9/eBMw1a9Y0JxktWrQwnyZ07dpVtmzZIv369fNq/2jQnTt3bpk3b545MVm/fr3Zdo4cOcwJjfN+S5UqlSn50ROG5557zqyvJ0IJGTsAIIjYACAIdOrUydasWTPzfUxMjG358uW2iIgIW//+/R3Ls2XLZrtx44bjObNnz7YVLlzYrG+ny1OnTm1btmyZeZwjRw7b2LFjHctv3bply507t+O1VM2aNW29e/c230dHR2ta3ry+O6tWrTLLz50755h3/fp1W5o0aWzr1693Wbdz5862Z555xnwfFRVlK1asmMvygQMHxtlWbPny5bONHz/ellA9evSwtWzZ0vFY91umTJlsV65cccybOnWqLV26dLY7d+4kaOzu3jMAIDDIvAMIGosXL5Z06dKZjLpmldu2bSvDhw93LC9ZsqRLnfvvv/8u+/fvlwceeMBlO9evX5e//vpLLly4IMeOHZOKFSs6lml2vnz58nFKZ+y2b98uyZIl8yjjrGO4evWqPPHEEy7ztTSlbNmy5vvdu3e7jEPpJwbemjJlivlU4dChQ3Lt2jXzmmXKlHFZRz89SJMmjcvrXr582XwaoF/vNXYAQPAgeAcQNLQOfOrUqSZA17p2DbSdpU2b1uWxBp7lypWTOXPmxNmWlnzcD3sZjCd0HGrJkiWSK1cul2VaM59YvvzyS+nfv78pBdKAXE9i3nnnHdm4cWPQjx0AcH8I3gEEDQ3O9eLQhHr00Udl7ty5kjVrVlN/7o7Wf2swW6NGDfNYW09u3brVPNcdze5r1v/nn382F8zGZs/868WidsWKFTOBrma/48vYa729/eJbu19//VW8sW7dOqlSpYq89NJLjnn6iUNs+gmFZuXtJyb6uvoJh9bw60W+9xo7ACB40G0GgGW1a9dOsmTJYjrM6AWremGpXpT58ssvy5EjR8w6vXv3lrfeeksWLlwoe/bsMYHu3Xq0a1/1Tp06yfPPP2+eY9/mV199ZZZrJxztMqMlPqdOnTKZa814awb8lVdekVmzZpkAetu2bTJ58mTzWGmHl3379smAAQPMxa6ff/65uZA2IY4ePWrKeZync+fOmYtL9cLXZcuWyd69e2XIkCGyefPmOM/XEhjtSrNr1y7T8WbYsGHSs2dPCQ8PT9DYAQDBg+AdgGVpHbd2RsmbN6/p5KLZbQ1StebdnonXji4dOnQwAbm9tOSpp56663a1dKdVq1Ym0C9SpIh06dJFrly5YpZpaYm2XRw0aJBky5bNBMFKb/KkwbN2btFxaMcbLUXR9otKx6idavSEQGvQtbPL6NGjE/Q+3333XVN/7jzptl988UXzvlu3bm3q6bUzj3MW3q5OnTom0NdPH3Tdpk2bulxLcK+xAwCCR5hetRroQQAAAAC4NzLvAAAAgEUQvAMAAAAWQfAOAAAAWATBOwAAAGARBO8AAACARRC8AwAAABZB8A4AAABYBME7AAAAYBEE7wAAAIBFELwDAAAAFkHwDgAAAFgEwTsAAAAg1vD/AJrm5XwZBgCYAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Overall Metrics (All Folds Combined):\n",
      "  Overall Accuracy: 0.9190\n",
      "\n",
      "Per-Class Metrics:\n",
      "\n",
      "  NORMAL:\n",
      "    Precision: 0.9800\n",
      "    Recall:    0.8869\n",
      "    F1-score:  0.9311\n",
      "\n",
      "  INTERICTAL:\n",
      "    Precision: 0.8437\n",
      "    Recall:    0.9919\n",
      "    F1-score:  0.9118\n",
      "\n",
      "  ICTAL:\n",
      "    Precision: 0.9985\n",
      "    Recall:    0.8375\n",
      "    F1-score:  0.9109\n",
      "\n",
      "  Macro-Averaged:\n",
      "    Precision: 0.9407\n",
      "    Recall:    0.9054\n",
      "    F1-score:  0.9180\n",
      "\n",
      "âœ… CNN+LSTM 3-Class Training Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "PREPROCESSING_DIR = r\"Preprocessing_Updated_Kfold\"  # Path to your preprocessing output\n",
    "N_FOLDS = 5\n",
    "RESULTS_DIR = \"results_3class\"\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- CNN + LSTM Model --------------------\n",
    "def build_cnn_lstm_multiclass(input_length, num_classes=3):\n",
    "    model = Sequential([\n",
    "        Conv1D(32, kernel_size=7, activation='relu', input_shape=(input_length, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        LSTM(64, return_sequences=False),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')  # Multi-class output\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-4),\n",
    "        loss='categorical_crossentropy',  # Multi-class loss\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -------------------- Data Augmentation --------------------\n",
    "def augment_signal(signal):\n",
    "    noise = np.random.normal(0, 0.005, signal.shape)\n",
    "    signal_noisy = signal + noise\n",
    "    shift = np.random.randint(-5, 5)\n",
    "    signal_shifted = np.roll(signal_noisy, shift)\n",
    "    scale = np.random.uniform(0.97, 1.03)\n",
    "    signal_scaled = signal_shifted * scale\n",
    "    return signal_scaled\n",
    "\n",
    "def augment_batch(X, y):\n",
    "    X_aug, y_aug = [], []\n",
    "    for i in range(len(X)):\n",
    "        X_aug.append(X[i])\n",
    "        y_aug.append(y[i])\n",
    "        X_aug.append(augment_signal(X[i]))\n",
    "        y_aug.append(y[i])\n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "# -------------------- Training Loop --------------------\n",
    "acc_per_fold = []\n",
    "conf_matrices = []\n",
    "\n",
    "for fold_no in range(N_FOLDS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ”¹ Fold {fold_no + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # -------------------- Load Preprocessed Data --------------------\n",
    "    X_train_full = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_X_train.npy\"), allow_pickle=True)\n",
    "    y_train_full = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_y_train.npy\"), allow_pickle=True)\n",
    "    X_test = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_X_test.npy\"), allow_pickle=True)\n",
    "    y_test = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_y_test.npy\"), allow_pickle=True)\n",
    "    \n",
    "    # Convert to proper numpy arrays if they're object dtype\n",
    "    if X_train_full.dtype == object:\n",
    "        X_train_full = np.vstack(X_train_full).astype(np.float32)\n",
    "    if X_test.dtype == object:\n",
    "        X_test = np.vstack(X_test).astype(np.float32)\n",
    "    \n",
    "    X_train_full = X_train_full.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_train_full = y_train_full.astype(np.int32)\n",
    "    y_test = y_test.astype(np.int32)\n",
    "\n",
    "    # -------------------- Labels Info --------------------\n",
    "    # Preprocessing labels: NORMAL=0, INTERICTAL=1, ICTAL=2\n",
    "    print(f\"Labels - Train: {np.unique(y_train_full, return_counts=True)}\")\n",
    "    print(f\"Labels - Test: {np.unique(y_test, return_counts=True)}\")\n",
    "\n",
    "    # -------------------- Split Train into Train/Val --------------------\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, \n",
    "        test_size=0.1765, \n",
    "        stratify=y_train_full, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # -------------------- Apply Data Augmentation --------------------\n",
    "    X_train, y_train = augment_batch(X_train, y_train)\n",
    "\n",
    "    # -------------------- Reshape for CNN --------------------\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    print(f\"\\nğŸ“Š Data shapes:\")\n",
    "    print(f\"  Train: {X_train.shape}\")\n",
    "    print(f\"  Val:   {X_val.shape}\")\n",
    "    print(f\"  Test:  {X_test.shape}\")\n",
    "\n",
    "    # -------------------- Convert to One-Hot --------------------\n",
    "    y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "    y_val_onehot = tf.keras.utils.to_categorical(y_val, num_classes=NUM_CLASSES)\n",
    "    y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=NUM_CLASSES)\n",
    "\n",
    "    # -------------------- Build Model --------------------\n",
    "    model = build_cnn_lstm_multiclass(X_train.shape[1], num_classes=NUM_CLASSES)\n",
    "    \n",
    "    if fold_no == 0:  # Show summary only for first fold\n",
    "        model.summary()\n",
    "\n",
    "    # -------------------- Class Weights --------------------\n",
    "    unique_classes = np.unique(y_train)\n",
    "    cw = compute_class_weight('balanced', classes=unique_classes, y=y_train)\n",
    "    class_weights = {cls: weight for cls, weight in zip(unique_classes, cw)}\n",
    "    print(f\"\\nâš–ï¸ Class weights: {class_weights}\")\n",
    "\n",
    "    # -------------------- Train Model --------------------\n",
    "    print(f\"\\nğŸš€ Training Fold {fold_no + 1}...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train_onehot,\n",
    "        epochs=40,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val_onehot),\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # -------------------- Evaluate --------------------\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=0)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    print(f\"\\nâœ… Fold {fold_no + 1} - Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # -------------------- Save Weights --------------------\n",
    "    weight_path = os.path.join(RESULTS_DIR, f\"cnn_lstm_fold{fold_no + 1}.weights.h5\")\n",
    "    model.save_weights(weight_path)\n",
    "    print(f\"ğŸ’¾ Weights saved to {weight_path}\")\n",
    "\n",
    "    # -------------------- Confusion Matrix --------------------\n",
    "    y_pred_probs = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['NORMAL', 'INTERICTAL', 'ICTAL'],\n",
    "                yticklabels=['NORMAL', 'INTERICTAL', 'ICTAL'])\n",
    "    plt.title(f\"Fold {fold_no + 1} Confusion Matrix\\nAccuracy: {test_acc:.4f}\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f\"cnn_lstm_conf_fold{fold_no + 1}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # -------------------- Classification Report --------------------\n",
    "    print(f\"\\nğŸ“‹ Classification Report - Fold {fold_no + 1}:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=['NORMAL', 'INTERICTAL', 'ICTAL'],\n",
    "                                digits=4))\n",
    "\n",
    "# -------------------- Overall Results --------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Accuracy across all folds: {np.mean(acc_per_fold):.4f} Â± {np.std(acc_per_fold):.4f}\")\n",
    "print(f\"Accuracy per fold: {[f'{acc:.4f}' for acc in acc_per_fold]}\")\n",
    "\n",
    "# -------------------- Overall Confusion Matrix --------------------\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(total_cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['NORMAL', 'INTERICTAL', 'ICTAL'],\n",
    "            yticklabels=['NORMAL', 'INTERICTAL', 'ICTAL'])\n",
    "plt.title(\"Overall Confusion Matrix (All Folds)\")\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"cnn_lstm_conf_overall.png\"))\n",
    "plt.show()\n",
    "\n",
    "# -------------------- Calculate Overall Metrics --------------------\n",
    "overall_accuracy = np.trace(total_cm) / np.sum(total_cm)\n",
    "\n",
    "# Per-class metrics\n",
    "per_class_metrics = []\n",
    "class_names = ['NORMAL', 'INTERICTAL', 'ICTAL']\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    tp = total_cm[i, i]\n",
    "    fp = total_cm[:, i].sum() - tp\n",
    "    fn = total_cm[i, :].sum() - tp\n",
    "    tn = total_cm.sum() - tp - fp - fn\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    per_class_metrics.append({\n",
    "        'class': class_names[i],\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "print(\"\\nğŸ“Š Overall Metrics (All Folds Combined):\")\n",
    "print(f\"  Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for metrics in per_class_metrics:\n",
    "    print(f\"\\n  {metrics['class']}:\")\n",
    "    print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"    Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"    F1-score:  {metrics['f1']:.4f}\")\n",
    "\n",
    "# Macro-averaged metrics\n",
    "macro_precision = np.mean([m['precision'] for m in per_class_metrics])\n",
    "macro_recall = np.mean([m['recall'] for m in per_class_metrics])\n",
    "macro_f1 = np.mean([m['f1'] for m in per_class_metrics])\n",
    "\n",
    "print(f\"\\n  Macro-Averaged:\")\n",
    "print(f\"    Precision: {macro_precision:.4f}\")\n",
    "print(f\"    Recall:    {macro_recall:.4f}\")\n",
    "print(f\"    F1-score:  {macro_f1:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… CNN+LSTM 3-Class Training Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
