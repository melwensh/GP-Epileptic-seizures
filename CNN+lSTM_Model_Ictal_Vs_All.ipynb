{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b5b6d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 1/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([ 640, 2560]))\n",
      "Binary labels - Test: (array([0, 1]), array([160, 640]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">862</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">431</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">431</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">427</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">427</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">213</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">213</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">211</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">211</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m862\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m431\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m431\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m427\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚        \u001b[38;5;34m10,304\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m427\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m213\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m213\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m211\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m24,704\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m211\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m49,408\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m4,160\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m65\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,793</span> (350.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m89,793\u001b[0m (350.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,345</span> (349.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m89,345\u001b[0m (349.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {0: np.float64(2.5), 1: np.float64(0.625)}\n",
      "\n",
      "ğŸš€ Training Fold 1...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 126ms/step - accuracy: 0.5036 - loss: 0.3901 - val_accuracy: 0.7894 - val_loss: 0.3432\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.4805 - loss: 0.3731 - val_accuracy: 0.7664 - val_loss: 0.3277\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.5731 - loss: 0.3521 - val_accuracy: 0.7876 - val_loss: 0.2955\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.6987 - loss: 0.3014 - val_accuracy: 0.9027 - val_loss: 0.2145\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.8406 - loss: 0.2016 - val_accuracy: 0.9398 - val_loss: 0.1368\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9482 - loss: 0.1072 - val_accuracy: 0.9451 - val_loss: 0.1219\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9723 - loss: 0.0735 - val_accuracy: 0.9593 - val_loss: 0.0944\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9677 - loss: 0.0707 - val_accuracy: 0.9487 - val_loss: 0.1050\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9594 - loss: 0.0746 - val_accuracy: 0.9381 - val_loss: 0.1422\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 110ms/step - accuracy: 0.9780 - loss: 0.0533 - val_accuracy: 0.9487 - val_loss: 0.1276\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9824 - loss: 0.0477 - val_accuracy: 0.9416 - val_loss: 0.1602\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 128ms/step - accuracy: 0.9799 - loss: 0.0469 - val_accuracy: 0.9469 - val_loss: 0.1434\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 104ms/step - accuracy: 0.9833 - loss: 0.0385 - val_accuracy: 0.9522 - val_loss: 0.1233\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.9784 - loss: 0.0417 - val_accuracy: 0.9416 - val_loss: 0.1779\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9858 - loss: 0.0374 - val_accuracy: 0.9487 - val_loss: 0.1386\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9865 - loss: 0.0306 - val_accuracy: 0.9593 - val_loss: 0.1253\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 157ms/step - accuracy: 0.9787 - loss: 0.0341 - val_accuracy: 0.9558 - val_loss: 0.1405\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 170ms/step - accuracy: 0.9852 - loss: 0.0302 - val_accuracy: 0.9416 - val_loss: 0.1554\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 171ms/step - accuracy: 0.9841 - loss: 0.0283 - val_accuracy: 0.9540 - val_loss: 0.1629\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 141ms/step - accuracy: 0.9770 - loss: 0.0374 - val_accuracy: 0.9558 - val_loss: 0.1175\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9882 - loss: 0.0293 - val_accuracy: 0.9522 - val_loss: 0.1406\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9880 - loss: 0.0239 - val_accuracy: 0.9504 - val_loss: 0.1733\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9854 - loss: 0.0253 - val_accuracy: 0.9628 - val_loss: 0.1276\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 0.9896 - loss: 0.0281 - val_accuracy: 0.9611 - val_loss: 0.1268\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9886 - loss: 0.0276 - val_accuracy: 0.9628 - val_loss: 0.1065\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9860 - loss: 0.0260 - val_accuracy: 0.9575 - val_loss: 0.1310\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9920 - loss: 0.0168 - val_accuracy: 0.9699 - val_loss: 0.0902\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9860 - loss: 0.0238 - val_accuracy: 0.9717 - val_loss: 0.0831\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.9892 - loss: 0.0190 - val_accuracy: 0.9628 - val_loss: 0.1350\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9913 - loss: 0.0188 - val_accuracy: 0.9558 - val_loss: 0.1394\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9926 - loss: 0.0144 - val_accuracy: 0.9646 - val_loss: 0.1229\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9903 - loss: 0.0177 - val_accuracy: 0.9664 - val_loss: 0.1180\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9873 - loss: 0.0242 - val_accuracy: 0.9752 - val_loss: 0.0814\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.9892 - loss: 0.0219 - val_accuracy: 0.9558 - val_loss: 0.1421\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 129ms/step - accuracy: 0.9901 - loss: 0.0185 - val_accuracy: 0.9699 - val_loss: 0.0855\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 99ms/step - accuracy: 0.9884 - loss: 0.0208 - val_accuracy: 0.9558 - val_loss: 0.1577\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 112ms/step - accuracy: 0.9922 - loss: 0.0140 - val_accuracy: 0.9628 - val_loss: 0.1192\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.9915 - loss: 0.0142 - val_accuracy: 0.9681 - val_loss: 0.0862\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9926 - loss: 0.0151 - val_accuracy: 0.9575 - val_loss: 0.1601\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 138ms/step - accuracy: 0.9932 - loss: 0.0136 - val_accuracy: 0.9522 - val_loss: 0.1705\n",
      "\n",
      "âœ… Fold 1 - Test Accuracy: 0.9550\n",
      "ğŸ’¾ Weights saved to results(Ictal)\\cnn_lstm_fold1.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 2/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([ 640, 2560]))\n",
      "Binary labels - Test: (array([0, 1]), array([160, 640]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {0: np.float64(2.5), 1: np.float64(0.625)}\n",
      "\n",
      "ğŸš€ Training Fold 2...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 145ms/step - accuracy: 0.4459 - loss: 0.3916 - val_accuracy: 0.2088 - val_loss: 0.3929\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 92ms/step - accuracy: 0.4581 - loss: 0.3755 - val_accuracy: 0.4354 - val_loss: 0.3791\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.5176 - loss: 0.3533 - val_accuracy: 0.7363 - val_loss: 0.3323\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 118ms/step - accuracy: 0.6514 - loss: 0.3160 - val_accuracy: 0.8973 - val_loss: 0.2539\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.8457 - loss: 0.2026 - val_accuracy: 0.9540 - val_loss: 0.1338\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9546 - loss: 0.1069 - val_accuracy: 0.9628 - val_loss: 0.0943\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9698 - loss: 0.0841 - val_accuracy: 0.9593 - val_loss: 0.0881\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9700 - loss: 0.0721 - val_accuracy: 0.9434 - val_loss: 0.1080\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9562 - loss: 0.0801 - val_accuracy: 0.9628 - val_loss: 0.0882\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.9727 - loss: 0.0602 - val_accuracy: 0.9575 - val_loss: 0.0900\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9677 - loss: 0.0662 - val_accuracy: 0.9664 - val_loss: 0.0769\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 114ms/step - accuracy: 0.9689 - loss: 0.0610 - val_accuracy: 0.9646 - val_loss: 0.0783\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - accuracy: 0.9835 - loss: 0.0463 - val_accuracy: 0.9575 - val_loss: 0.0881\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 145ms/step - accuracy: 0.9841 - loss: 0.0462 - val_accuracy: 0.9593 - val_loss: 0.0845\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9822 - loss: 0.0427 - val_accuracy: 0.9593 - val_loss: 0.0870\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9812 - loss: 0.0464 - val_accuracy: 0.9681 - val_loss: 0.0667\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 122ms/step - accuracy: 0.9854 - loss: 0.0355 - val_accuracy: 0.9558 - val_loss: 0.0964\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9835 - loss: 0.0419 - val_accuracy: 0.9522 - val_loss: 0.1056\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9873 - loss: 0.0311 - val_accuracy: 0.9540 - val_loss: 0.1089\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9812 - loss: 0.0436 - val_accuracy: 0.9522 - val_loss: 0.1234\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 118ms/step - accuracy: 0.9861 - loss: 0.0342 - val_accuracy: 0.9611 - val_loss: 0.1031\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9871 - loss: 0.0292 - val_accuracy: 0.9628 - val_loss: 0.0820\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 129ms/step - accuracy: 0.9852 - loss: 0.0327 - val_accuracy: 0.9593 - val_loss: 0.0825\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 136ms/step - accuracy: 0.9899 - loss: 0.0236 - val_accuracy: 0.9664 - val_loss: 0.0737\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 140ms/step - accuracy: 0.9898 - loss: 0.0232 - val_accuracy: 0.9628 - val_loss: 0.0876\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 143ms/step - accuracy: 0.9867 - loss: 0.0297 - val_accuracy: 0.9646 - val_loss: 0.0886\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 136ms/step - accuracy: 0.9879 - loss: 0.0270 - val_accuracy: 0.9628 - val_loss: 0.0807\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 101ms/step - accuracy: 0.9843 - loss: 0.0349 - val_accuracy: 0.9699 - val_loss: 0.0694\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 135ms/step - accuracy: 0.9920 - loss: 0.0188 - val_accuracy: 0.9664 - val_loss: 0.0763\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 143ms/step - accuracy: 0.9882 - loss: 0.0238 - val_accuracy: 0.9628 - val_loss: 0.0743\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 129ms/step - accuracy: 0.9911 - loss: 0.0213 - val_accuracy: 0.9628 - val_loss: 0.0900\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 145ms/step - accuracy: 0.9935 - loss: 0.0144 - val_accuracy: 0.9593 - val_loss: 0.0954\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 135ms/step - accuracy: 0.9894 - loss: 0.0245 - val_accuracy: 0.9735 - val_loss: 0.0577\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.9884 - loss: 0.0246 - val_accuracy: 0.9717 - val_loss: 0.0659\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 0.9901 - loss: 0.0218 - val_accuracy: 0.9735 - val_loss: 0.0622\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 114ms/step - accuracy: 0.9928 - loss: 0.0165 - val_accuracy: 0.9894 - val_loss: 0.0340\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 110ms/step - accuracy: 0.9939 - loss: 0.0161 - val_accuracy: 0.9752 - val_loss: 0.0563\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 0.9880 - loss: 0.0300 - val_accuracy: 0.9805 - val_loss: 0.0560\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9920 - loss: 0.0186 - val_accuracy: 0.9805 - val_loss: 0.0476\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.9947 - loss: 0.0125 - val_accuracy: 0.9788 - val_loss: 0.0504\n",
      "\n",
      "âœ… Fold 2 - Test Accuracy: 0.9675\n",
      "ğŸ’¾ Weights saved to results(Ictal)\\cnn_lstm_fold2.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 3/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([ 640, 2560]))\n",
      "Binary labels - Test: (array([0, 1]), array([160, 640]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {0: np.float64(2.5), 1: np.float64(0.625)}\n",
      "\n",
      "ğŸš€ Training Fold 3...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 128ms/step - accuracy: 0.4744 - loss: 0.3902 - val_accuracy: 0.2000 - val_loss: 0.4226\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.4953 - loss: 0.3666 - val_accuracy: 0.2920 - val_loss: 0.4146\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 81ms/step - accuracy: 0.5769 - loss: 0.3385 - val_accuracy: 0.6814 - val_loss: 0.3432\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.7258 - loss: 0.2813 - val_accuracy: 0.8796 - val_loss: 0.2241\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.9112 - loss: 0.1483 - val_accuracy: 0.9681 - val_loss: 0.0877\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 106ms/step - accuracy: 0.9670 - loss: 0.0859 - val_accuracy: 0.9681 - val_loss: 0.0769\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.9723 - loss: 0.0642 - val_accuracy: 0.9699 - val_loss: 0.0655\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.9672 - loss: 0.0666 - val_accuracy: 0.9717 - val_loss: 0.0622\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 94ms/step - accuracy: 0.9753 - loss: 0.0612 - val_accuracy: 0.9717 - val_loss: 0.0595\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9769 - loss: 0.0535 - val_accuracy: 0.9699 - val_loss: 0.0622\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9770 - loss: 0.0445 - val_accuracy: 0.9912 - val_loss: 0.0333\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 134ms/step - accuracy: 0.9791 - loss: 0.0531 - val_accuracy: 0.9805 - val_loss: 0.0456\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9827 - loss: 0.0412 - val_accuracy: 0.9823 - val_loss: 0.0498\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9854 - loss: 0.0376 - val_accuracy: 0.9805 - val_loss: 0.0471\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9844 - loss: 0.0332 - val_accuracy: 0.9752 - val_loss: 0.0699\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 134ms/step - accuracy: 0.9852 - loss: 0.0379 - val_accuracy: 0.9841 - val_loss: 0.0453\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 104ms/step - accuracy: 0.9833 - loss: 0.0340 - val_accuracy: 0.9752 - val_loss: 0.0560\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.9873 - loss: 0.0250 - val_accuracy: 0.9876 - val_loss: 0.0320\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.9871 - loss: 0.0303 - val_accuracy: 0.9876 - val_loss: 0.0380\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 129ms/step - accuracy: 0.9856 - loss: 0.0325 - val_accuracy: 0.9823 - val_loss: 0.0441\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 128ms/step - accuracy: 0.9905 - loss: 0.0252 - val_accuracy: 0.9841 - val_loss: 0.0359\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 122ms/step - accuracy: 0.9835 - loss: 0.0373 - val_accuracy: 0.9858 - val_loss: 0.0314\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.9863 - loss: 0.0303 - val_accuracy: 0.9823 - val_loss: 0.0382\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.9899 - loss: 0.0237 - val_accuracy: 0.9858 - val_loss: 0.0455\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9920 - loss: 0.0256 - val_accuracy: 0.9894 - val_loss: 0.0364\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9890 - loss: 0.0220 - val_accuracy: 0.9858 - val_loss: 0.0361\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9879 - loss: 0.0261 - val_accuracy: 0.9876 - val_loss: 0.0356\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9856 - loss: 0.0272 - val_accuracy: 0.9894 - val_loss: 0.0335\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9905 - loss: 0.0189 - val_accuracy: 0.9858 - val_loss: 0.0301\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 131ms/step - accuracy: 0.9924 - loss: 0.0145 - val_accuracy: 0.9841 - val_loss: 0.0411\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9926 - loss: 0.0144 - val_accuracy: 0.9894 - val_loss: 0.0354\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9877 - loss: 0.0229 - val_accuracy: 0.9894 - val_loss: 0.0318\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9951 - loss: 0.0131 - val_accuracy: 0.9876 - val_loss: 0.0413\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9943 - loss: 0.0160 - val_accuracy: 0.9894 - val_loss: 0.0330\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9932 - loss: 0.0150 - val_accuracy: 0.9876 - val_loss: 0.0277\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9915 - loss: 0.0147 - val_accuracy: 0.9929 - val_loss: 0.0226\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9905 - loss: 0.0182 - val_accuracy: 0.9912 - val_loss: 0.0252\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9943 - loss: 0.0160 - val_accuracy: 0.9894 - val_loss: 0.0263\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 112ms/step - accuracy: 0.9901 - loss: 0.0181 - val_accuracy: 0.9912 - val_loss: 0.0264\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9968 - loss: 0.0077 - val_accuracy: 0.9929 - val_loss: 0.0165\n",
      "\n",
      "âœ… Fold 3 - Test Accuracy: 0.9837\n",
      "ğŸ’¾ Weights saved to results(Ictal)\\cnn_lstm_fold3.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 4/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([ 640, 2560]))\n",
      "Binary labels - Test: (array([0, 1]), array([160, 640]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n",
      "\n",
      "âš–ï¸ Class weights: {0: np.float64(2.5), 1: np.float64(0.625)}\n",
      "\n",
      "ğŸš€ Training Fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 137ms/step - accuracy: 0.4514 - loss: 0.3878 - val_accuracy: 0.2000 - val_loss: 0.4285\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.4514 - loss: 0.3700 - val_accuracy: 0.2531 - val_loss: 0.4094\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.5767 - loss: 0.3447 - val_accuracy: 0.5876 - val_loss: 0.3631\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 0.7072 - loss: 0.2839 - val_accuracy: 0.8442 - val_loss: 0.2508\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9157 - loss: 0.1426 - val_accuracy: 0.9770 - val_loss: 0.1092\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9666 - loss: 0.0866 - val_accuracy: 0.9770 - val_loss: 0.0755\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9715 - loss: 0.0673 - val_accuracy: 0.9735 - val_loss: 0.0753\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9721 - loss: 0.0582 - val_accuracy: 0.9788 - val_loss: 0.0576\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9676 - loss: 0.0721 - val_accuracy: 0.9858 - val_loss: 0.0469\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 117ms/step - accuracy: 0.9774 - loss: 0.0531 - val_accuracy: 0.9788 - val_loss: 0.0572\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9806 - loss: 0.0409 - val_accuracy: 0.9912 - val_loss: 0.0373\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 128ms/step - accuracy: 0.9808 - loss: 0.0394 - val_accuracy: 0.9876 - val_loss: 0.0369\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 104ms/step - accuracy: 0.9786 - loss: 0.0459 - val_accuracy: 0.9876 - val_loss: 0.0496\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 84ms/step - accuracy: 0.9818 - loss: 0.0326 - val_accuracy: 0.9894 - val_loss: 0.0455\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9822 - loss: 0.0380 - val_accuracy: 0.9876 - val_loss: 0.0371\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9844 - loss: 0.0374 - val_accuracy: 0.9858 - val_loss: 0.0345\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9863 - loss: 0.0315 - val_accuracy: 0.9858 - val_loss: 0.0431\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9844 - loss: 0.0333 - val_accuracy: 0.9858 - val_loss: 0.0339\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.9837 - loss: 0.0328 - val_accuracy: 0.9876 - val_loss: 0.0384\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9882 - loss: 0.0294 - val_accuracy: 0.9876 - val_loss: 0.0338\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 92ms/step - accuracy: 0.9865 - loss: 0.0246 - val_accuracy: 0.9858 - val_loss: 0.0437\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9911 - loss: 0.0262 - val_accuracy: 0.9841 - val_loss: 0.0446\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9875 - loss: 0.0228 - val_accuracy: 0.9876 - val_loss: 0.0392\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9877 - loss: 0.0230 - val_accuracy: 0.9858 - val_loss: 0.0379\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.9888 - loss: 0.0244 - val_accuracy: 0.9823 - val_loss: 0.0350\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9903 - loss: 0.0231 - val_accuracy: 0.9876 - val_loss: 0.0425\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 102ms/step - accuracy: 0.9903 - loss: 0.0213 - val_accuracy: 0.9876 - val_loss: 0.0468\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9911 - loss: 0.0183 - val_accuracy: 0.9894 - val_loss: 0.0418\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9926 - loss: 0.0165 - val_accuracy: 0.9894 - val_loss: 0.0349\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9894 - loss: 0.0224 - val_accuracy: 0.9912 - val_loss: 0.0234\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9924 - loss: 0.0146 - val_accuracy: 0.9929 - val_loss: 0.0246\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9911 - loss: 0.0173 - val_accuracy: 0.9894 - val_loss: 0.0260\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9941 - loss: 0.0149 - val_accuracy: 0.9894 - val_loss: 0.0397\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9972 - loss: 0.0071 - val_accuracy: 0.9912 - val_loss: 0.0310\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9954 - loss: 0.0086 - val_accuracy: 0.9929 - val_loss: 0.0393\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9913 - loss: 0.0135 - val_accuracy: 0.9912 - val_loss: 0.0488\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9960 - loss: 0.0116 - val_accuracy: 0.9929 - val_loss: 0.0357\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9903 - loss: 0.0168 - val_accuracy: 0.9894 - val_loss: 0.0416\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 120ms/step - accuracy: 0.9861 - loss: 0.0216 - val_accuracy: 0.9947 - val_loss: 0.0209\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9951 - loss: 0.0090 - val_accuracy: 0.9929 - val_loss: 0.0290\n",
      "\n",
      "âœ… Fold 4 - Test Accuracy: 0.9900\n",
      "ğŸ’¾ Weights saved to results(Ictal)\\cnn_lstm_fold4.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ”¹ Fold 5/5\n",
      "============================================================\n",
      "Original labels - Train: (array([0, 1, 2], dtype=int32), array([1280, 1280,  640]))\n",
      "Binary labels - Train: (array([0, 1]), array([ 640, 2560]))\n",
      "Binary labels - Test: (array([0, 1]), array([160, 640]))\n",
      "\n",
      "ğŸ“Š Data shapes:\n",
      "  Train: (5270, 868, 1)\n",
      "  Val:   (565, 868, 1)\n",
      "  Test:  (800, 868, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Class weights: {0: np.float64(2.5), 1: np.float64(0.625)}\n",
      "\n",
      "ğŸš€ Training Fold 5...\n",
      "Epoch 1/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 134ms/step - accuracy: 0.5357 - loss: 0.3939 - val_accuracy: 0.2584 - val_loss: 0.3859\n",
      "Epoch 2/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 117ms/step - accuracy: 0.4837 - loss: 0.3806 - val_accuracy: 0.4212 - val_loss: 0.3843\n",
      "Epoch 3/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 119ms/step - accuracy: 0.5072 - loss: 0.3628 - val_accuracy: 0.5628 - val_loss: 0.3649\n",
      "Epoch 4/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.6368 - loss: 0.3268 - val_accuracy: 0.7044 - val_loss: 0.3152\n",
      "Epoch 5/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.8156 - loss: 0.2274 - val_accuracy: 0.8248 - val_loss: 0.2655\n",
      "Epoch 6/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9571 - loss: 0.1050 - val_accuracy: 0.9681 - val_loss: 0.0863\n",
      "Epoch 7/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9668 - loss: 0.0785 - val_accuracy: 0.9540 - val_loss: 0.0989\n",
      "Epoch 8/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step - accuracy: 0.9797 - loss: 0.0559 - val_accuracy: 0.9628 - val_loss: 0.0818\n",
      "Epoch 9/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 104ms/step - accuracy: 0.9814 - loss: 0.0522 - val_accuracy: 0.9646 - val_loss: 0.0787\n",
      "Epoch 10/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.9867 - loss: 0.0429 - val_accuracy: 0.9752 - val_loss: 0.0613\n",
      "Epoch 11/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9797 - loss: 0.0466 - val_accuracy: 0.9752 - val_loss: 0.0639\n",
      "Epoch 12/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 120ms/step - accuracy: 0.9824 - loss: 0.0405 - val_accuracy: 0.9628 - val_loss: 0.0705\n",
      "Epoch 13/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9875 - loss: 0.0413 - val_accuracy: 0.9735 - val_loss: 0.0571\n",
      "Epoch 14/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9861 - loss: 0.0341 - val_accuracy: 0.9823 - val_loss: 0.0452\n",
      "Epoch 15/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9867 - loss: 0.0310 - val_accuracy: 0.9788 - val_loss: 0.0438\n",
      "Epoch 16/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9863 - loss: 0.0310 - val_accuracy: 0.9788 - val_loss: 0.0485\n",
      "Epoch 17/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9907 - loss: 0.0254 - val_accuracy: 0.9788 - val_loss: 0.0506\n",
      "Epoch 18/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.9786 - loss: 0.0432 - val_accuracy: 0.9752 - val_loss: 0.0591\n",
      "Epoch 19/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9884 - loss: 0.0254 - val_accuracy: 0.9699 - val_loss: 0.0669\n",
      "Epoch 20/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9865 - loss: 0.0265 - val_accuracy: 0.9699 - val_loss: 0.0587\n",
      "Epoch 21/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.9886 - loss: 0.0236 - val_accuracy: 0.9681 - val_loss: 0.0638\n",
      "Epoch 22/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9920 - loss: 0.0212 - val_accuracy: 0.9788 - val_loss: 0.0488\n",
      "Epoch 23/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9744 - loss: 0.0423 - val_accuracy: 0.9699 - val_loss: 0.0568\n",
      "Epoch 24/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9920 - loss: 0.0173 - val_accuracy: 0.9699 - val_loss: 0.0685\n",
      "Epoch 25/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9935 - loss: 0.0181 - val_accuracy: 0.9823 - val_loss: 0.0441\n",
      "Epoch 26/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9920 - loss: 0.0187 - val_accuracy: 0.9841 - val_loss: 0.0439\n",
      "Epoch 27/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9935 - loss: 0.0145 - val_accuracy: 0.9770 - val_loss: 0.0575\n",
      "Epoch 28/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 121ms/step - accuracy: 0.9934 - loss: 0.0171 - val_accuracy: 0.9805 - val_loss: 0.0468\n",
      "Epoch 29/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.9918 - loss: 0.0147 - val_accuracy: 0.9823 - val_loss: 0.0402\n",
      "Epoch 30/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9932 - loss: 0.0175 - val_accuracy: 0.9717 - val_loss: 0.0612\n",
      "Epoch 31/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.9930 - loss: 0.0125 - val_accuracy: 0.9841 - val_loss: 0.0361\n",
      "Epoch 32/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9816 - loss: 0.0316 - val_accuracy: 0.9735 - val_loss: 0.0581\n",
      "Epoch 33/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9875 - loss: 0.0208 - val_accuracy: 0.9398 - val_loss: 0.1398\n",
      "Epoch 34/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 123ms/step - accuracy: 0.9943 - loss: 0.0128 - val_accuracy: 0.9823 - val_loss: 0.0420\n",
      "Epoch 35/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - accuracy: 0.9962 - loss: 0.0105 - val_accuracy: 0.9646 - val_loss: 0.0682\n",
      "Epoch 36/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.9932 - loss: 0.0136 - val_accuracy: 0.9770 - val_loss: 0.0481\n",
      "Epoch 37/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.9913 - loss: 0.0208 - val_accuracy: 0.9717 - val_loss: 0.0574\n",
      "Epoch 38/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 98ms/step - accuracy: 0.9945 - loss: 0.0112 - val_accuracy: 0.9752 - val_loss: 0.0543\n",
      "Epoch 39/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 130ms/step - accuracy: 0.9941 - loss: 0.0161 - val_accuracy: 0.9664 - val_loss: 0.0717\n",
      "Epoch 40/40\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.9935 - loss: 0.0132 - val_accuracy: 0.9894 - val_loss: 0.0351\n",
      "\n",
      "âœ… Fold 5 - Test Accuracy: 0.9800\n",
      "ğŸ’¾ Weights saved to results(Ictal)\\cnn_lstm_fold5.weights.h5\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š FINAL RESULTS\n",
      "============================================================\n",
      "Mean Accuracy across all folds: 0.9752 Â± 0.0125\n",
      "Accuracy per fold: ['0.9550', '0.9675', '0.9837', '0.9900', '0.9800']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Overall Metrics (All Folds Combined):\n",
      "  Accuracy : 0.9752\n",
      "  Precision: 0.9743\n",
      "  Recall   : 0.9953\n",
      "  F1-score : 0.9847\n",
      "\n",
      "âœ… CNN+LSTM Training Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "PREPROCESSING_DIR = r\"Preprocessing_Updated_Kfold\"  # Path to your preprocessing output\n",
    "N_FOLDS = 5\n",
    "RESULTS_DIR = \"results(Ictal)\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- Hybrid Focal Loss --------------------\n",
    "def hybrid_focal_loss(alpha=0.25, gamma=2.0, bce_weight=0.5):\n",
    "    \"\"\"\n",
    "    Hybrid = BCE * bce_weight + FocalLoss * (1 - bce_weight)\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        # --- Binary Cross Entropy ---\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # --- Focal Loss ---\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        focal = -alpha * (1 - y_pred) ** gamma * y_true * K.log(y_pred) \\\n",
    "                - (1 - alpha) * y_pred ** gamma * (1 - y_true) * K.log(1 - y_pred)\n",
    "\n",
    "        focal = K.mean(focal, axis=-1)\n",
    "\n",
    "        # --- Combine Both ---\n",
    "        return bce_weight * bce + (1 - bce_weight) * focal\n",
    "\n",
    "    return loss\n",
    "\n",
    "# -------------------- CNN + LSTM Model --------------------\n",
    "def build_cnn_lstm(input_length):\n",
    "    model = Sequential([\n",
    "        # --- CNN Layers ---\n",
    "        Conv1D(32, kernel_size=7, activation='relu', input_shape=(input_length, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # --- LSTM ---\n",
    "        LSTM(64, return_sequences=False),\n",
    "\n",
    "        # --- Dense Layers ---\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(1e-4),\n",
    "        loss=hybrid_focal_loss(alpha=0.25, gamma=2.0, bce_weight=0.5),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -------------------- Data Augmentation --------------------\n",
    "def augment_signal(signal):\n",
    "    # 1) Very Light Noise\n",
    "    noise = np.random.normal(0, 0.005, signal.shape)\n",
    "    signal_noisy = signal + noise\n",
    "\n",
    "    # 2) Small Time Shift\n",
    "    shift = np.random.randint(-5, 5)\n",
    "    signal_shifted = np.roll(signal_noisy, shift)\n",
    "\n",
    "    # 3) Gentle Scaling\n",
    "    scale = np.random.uniform(0.97, 1.03)\n",
    "    signal_scaled = signal_shifted * scale\n",
    "\n",
    "    return signal_scaled\n",
    "\n",
    "def augment_batch(X, y):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        X_aug.append(X[i])\n",
    "        y_aug.append(y[i])\n",
    "\n",
    "        # Generate 1 weakly augmented version\n",
    "        X_aug.append(augment_signal(X[i]))\n",
    "        y_aug.append(y[i])\n",
    "\n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "# -------------------- Training Loop --------------------\n",
    "acc_per_fold = []\n",
    "conf_matrices = []\n",
    "\n",
    "for fold_no in range(N_FOLDS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ”¹ Fold {fold_no + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # -------------------- Load Preprocessed Data --------------------\n",
    "    X_train_full = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_X_train.npy\"), allow_pickle=True)\n",
    "    y_train_full = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_y_train.npy\"), allow_pickle=True)\n",
    "    X_test = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_X_test.npy\"), allow_pickle=True)\n",
    "    y_test = np.load(os.path.join(PREPROCESSING_DIR, f\"fold_{fold_no}_y_test.npy\"), allow_pickle=True)\n",
    "    \n",
    "    # Convert to proper numpy arrays if they're object dtype\n",
    "    if X_train_full.dtype == object:\n",
    "        X_train_full = np.vstack(X_train_full).astype(np.float32)\n",
    "    if X_test.dtype == object:\n",
    "        X_test = np.vstack(X_test).astype(np.float32)\n",
    "    \n",
    "    X_train_full = X_train_full.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_train_full = y_train_full.astype(np.int32)\n",
    "    y_test = y_test.astype(np.int32)\n",
    "\n",
    "    # -------------------- Convert to Binary Classification --------------------\n",
    "    # ICTAL = 0 (class 2 in preprocessing), ALL OTHERS = 1 (classes 0 and 1)\n",
    "    y_train_full_binary = np.where(y_train_full == 2, 0, 1)\n",
    "    y_test_binary = np.where(y_test == 2, 0, 1)\n",
    "\n",
    "    print(f\"Original labels - Train: {np.unique(y_train_full, return_counts=True)}\")\n",
    "    print(f\"Binary labels - Train: {np.unique(y_train_full_binary, return_counts=True)}\")\n",
    "    print(f\"Binary labels - Test: {np.unique(y_test_binary, return_counts=True)}\")\n",
    "\n",
    "    # -------------------- Split Train into Train/Val --------------------\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full_binary, \n",
    "        test_size=0.1765, \n",
    "        stratify=y_train_full_binary, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # -------------------- Apply Data Augmentation --------------------\n",
    "    X_train, y_train = augment_batch(X_train, y_train)\n",
    "\n",
    "    # -------------------- Reshape for CNN --------------------\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    print(f\"\\nğŸ“Š Data shapes:\")\n",
    "    print(f\"  Train: {X_train.shape}\")\n",
    "    print(f\"  Val:   {X_val.shape}\")\n",
    "    print(f\"  Test:  {X_test.shape}\")\n",
    "\n",
    "    # -------------------- Build Model --------------------\n",
    "    model = build_cnn_lstm(X_train.shape[1])\n",
    "    \n",
    "    if fold_no == 0:  # Show summary only for first fold\n",
    "        model.summary()\n",
    "\n",
    "    # -------------------- Class Weights --------------------\n",
    "    cw = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = {0: cw[0], 1: cw[1]}\n",
    "    print(f\"\\nâš–ï¸ Class weights: {class_weights}\")\n",
    "\n",
    "    # -------------------- Train Model --------------------\n",
    "    print(f\"\\nğŸš€ Training Fold {fold_no + 1}...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=40,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # -------------------- Evaluate --------------------\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_binary, verbose=0)\n",
    "    acc_per_fold.append(test_acc)\n",
    "    print(f\"\\nâœ… Fold {fold_no + 1} - Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # -------------------- Save Weights --------------------\n",
    "    weight_path = os.path.join(RESULTS_DIR, f\"cnn_lstm_fold{fold_no + 1}.weights.h5\")\n",
    "    model.save_weights(weight_path)\n",
    "    print(f\"ğŸ’¾ Weights saved to {weight_path}\")\n",
    "\n",
    "    # -------------------- Confusion Matrix --------------------\n",
    "    y_pred = (model.predict(X_test, verbose=0) > 0.5).astype(\"int32\").flatten()\n",
    "    cm = confusion_matrix(y_test_binary, y_pred)\n",
    "    conf_matrices.append(cm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=['ICTAL', 'NON-ICTAL'], \n",
    "        yticklabels=['ICTAL', 'NON-ICTAL']\n",
    "    )\n",
    "    plt.title(f\"Fold {fold_no + 1} Confusion Matrix\\nAccuracy: {test_acc:.4f}\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f\"cnn_lstm_conf_fold{fold_no + 1}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# -------------------- Overall Results --------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Accuracy across all folds: {np.mean(acc_per_fold):.4f} Â± {np.std(acc_per_fold):.4f}\")\n",
    "print(f\"Accuracy per fold: {[f'{acc:.4f}' for acc in acc_per_fold]}\")\n",
    "\n",
    "# -------------------- Overall Confusion Matrix --------------------\n",
    "total_cm = np.sum(conf_matrices, axis=0)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    total_cm, annot=True, fmt='d', cmap='Greens',\n",
    "    xticklabels=['ICTAL', 'NON-ICTAL'], \n",
    "    yticklabels=['ICTAL', 'NON-ICTAL']\n",
    ")\n",
    "plt.title(\"Overall Confusion Matrix (All Folds)\")\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"cnn_lstm_conf_overall.png\"))\n",
    "plt.show()\n",
    "\n",
    "# -------------------- Calculate Overall Metrics --------------------\n",
    "tn, fp, fn, tp = total_cm.ravel()\n",
    "\n",
    "overall_accuracy = (tp + tn) / np.sum(total_cm)\n",
    "overall_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "overall_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "\n",
    "print(\"\\nğŸ“Š Overall Metrics (All Folds Combined):\")\n",
    "print(f\"  Accuracy : {overall_accuracy:.4f}\")\n",
    "print(f\"  Precision: {overall_precision:.4f}\")\n",
    "print(f\"  Recall   : {overall_recall:.4f}\")\n",
    "print(f\"  F1-score : {overall_f1:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… CNN+LSTM Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5199e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
